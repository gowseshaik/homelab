{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Knowledge Base","text":""},{"location":"#ai-engineer","title":"Ai engineer","text":"<ul> <li>Agents</li> <li>Fine-tuning</li> <li>Github Repos</li> <li>Multi-Agents</li> <li>RAG-based DevOps assistant using an LLM</li> <li>RAG</li> <li>Reasoning Models</li> <li>Vibe Checking</li> </ul>"},{"location":"#api-resources","title":"Api-resources","text":"<ul> <li>k8s-api-resources-details</li> </ul>"},{"location":"#apm","title":"Apm","text":"<ul> <li>Add Elastic repo and install filebeat on app nodes</li> <li>Docker APM images and setup steps</li> <li>Docker Compose - setup for Elasticsearch-Kibana-Elastic APM Server</li> <li>ELK Stack - APM Setup in Multipass VM</li> <li>ELK installation and setup</li> <li>Setup ELK with filebeat</li> <li>Steps to parse custom logs with Filebeat</li> <li>Why you need Filebeat and Logstash and when to use which</li> </ul>"},{"location":"#alertmanager","title":"Alertmanager","text":"<ul> <li>how to configure Alertmanager on K3d</li> </ul>"},{"location":"#argocd","title":"Argocd","text":"<ul> <li>ArgoCD Architecture</li> <li>ArgoCD installation steps</li> <li>ArgoCD with HTTPS using cert-manager</li> <li>Argocd CLI</li> <li>Common misunderstanding in ArgoCD Usage</li> <li>How ArgoCD and OpenShift</li> <li>To configure two K3s clusters in ArgoCD</li> </ul>"},{"location":"#ci-cd-tools","title":"Ci-cd tools","text":"<ul> <li>List of online open-source CI-CD tools</li> </ul>"},{"location":"#crds","title":"Crds","text":"<ul> <li>Controllers and CRDs</li> <li>Create your own Custom Resource Definition (CRD)</li> <li>Custom Resource Definition</li> <li>Deep Dive into CRDs</li> <li>To know which namespaces or manifests your CRDs are being used</li> </ul>"},{"location":"#cert-manager","title":"Cert-manager","text":"<ul> <li>install-cert-manager</li> </ul>"},{"location":"#choosing-kubernetes-by-usecases","title":"Choosing kubernetes by usecases","text":"<ul> <li>For homelab setup with</li> <li>K8s HA clusters</li> <li>Kind Vs K3d</li> <li>Kubernetes cluster types on your usecases</li> </ul>"},{"location":"#cluster-setup-ha","title":"Cluster setup ha","text":"<ul> <li>Deploy HA Kubernetes Cluster on Linux using Rancher RKE2</li> </ul>"},{"location":"#clusteraccess","title":"Clusteraccess","text":"<ul> <li>Configuring Keycloak for Kubernetes authentication</li> <li>Create a kubeconfig file for the external users</li> <li>merge two kubeconfig files</li> </ul>"},{"location":"#configmaps","title":"Configmaps","text":"<ul> <li>ConfigMap is a Kubernetes object used to store non-confidential data</li> </ul>"},{"location":"#cronjob","title":"Cronjob","text":"<ul> <li>CronJobs in Kubernetes</li> <li>Real-World CronJob Example- Automated Database Backup n Cleanup</li> <li>k8s Jobs</li> </ul>"},{"location":"#daemonsets","title":"Daemonsets","text":"<ul> <li>CronJobs Vs DaemonSets</li> <li>DaemonSets in Kubernetes</li> </ul>"},{"location":"#dailytasks","title":"Dailytasks","text":"<ul> <li>Critical Thinking</li> <li>Failures with solution</li> <li>Important resources to learn k8s</li> <li>K8s Cmds Cheatsheet</li> <li>Keywords</li> <li>Usecases for ConfigMaps and Secrets</li> <li>k8s error codes</li> <li>pod exec and extract info from pods</li> </ul>"},{"location":"#deployments","title":"Deployments","text":"<ul> <li>Deployments Strategies</li> <li>Deployments in kubernetes</li> </ul>"},{"location":"#docker-podman-images","title":"Docker-podman-images","text":"<ul> <li>Build Docker images directly from Git repos</li> <li>Docker-compose to K8s</li> <li>Enterprise-grade images as safer</li> <li>Limitations of Containers</li> <li>Run a container with all resource limits</li> <li>Tag images to push</li> <li>To find the memory usage of containers</li> <li>podman build --squash  commit and ImageStream cleanup</li> <li>scp images in tar format to remote hosts - offline</li> </ul>"},{"location":"#helm","title":"Helm","text":"<ul> <li>1. Check Helm Release(deployed) Information</li> <li>Checking Traefik Helm Configuration</li> <li>Helm Commands - Cheatsheet</li> <li>Helm charts for deployments</li> </ul>"},{"location":"#ingress","title":"Ingress","text":"<ul> <li>1. Host based ingress routing</li> <li>2. Path based ingress routing</li> <li>3. subdomain based ingress routing</li> <li>Access Traefik Dashboard</li> <li>Check Traefik Configuration logs for config issues</li> <li>Diff between Host-path-subdomain based ingress routing</li> <li>How to confirm Ingress resources, routes, and active requests</li> <li>How to confirm the exact Ingress Controller used in your Kubernetes cluster</li> <li>Types of Ingress Controllers with their pros, cons, and pricing details</li> <li>create ingress with traefik</li> </ul>"},{"location":"#k3d","title":"K3d","text":"<ul> <li>Complete Traefik Setup Guide for k3d</li> <li>K3d cluster setup with HA</li> <li>K3d cluster setup</li> <li>k3d and kind commands</li> <li>k3d cluster with traefik dashboard</li> <li>k3d with traefik dashboard setup</li> </ul>"},{"location":"#keycloak","title":"Keycloak","text":"<ul> <li>Efficient way to to Set Up Keycloak (with Helm)</li> </ul>"},{"location":"#keywords","title":"Keywords","text":"<ul> <li>Must know keywords</li> </ul>"},{"location":"#kubernetes","title":"Kubernetes","text":"<ul> <li>Full Sequence to Implement Kubernetes with kubeadm</li> <li>Installing metric server</li> <li>K8s - manifest files structure</li> <li>K8s Pod Architecture &amp; Lifecycle</li> <li>Kube API server Architechture - deep dive</li> <li>Kuberentes usecases-deepdive flow</li> <li>Kubernetes history</li> </ul>"},{"location":"#loadbalancers-types","title":"Loadbalancers types","text":"<ul> <li>Loadbalancer Types</li> <li>Reverse proxy means</li> </ul>"},{"location":"#longhorn","title":"Longhorn","text":"<ul> <li>Can You Use Longhorn on top of NFS ?</li> <li>Create a PV and PVC using longhorn-static</li> <li>Hands-On - Deploying Longhorn on Kubernetes</li> <li>Longhorn Access Modes &amp; Volume Resizing Guide</li> <li>Longhorn for stateful applications</li> <li>Longhorn in Kubernetes</li> <li>Longhorn with loopback file on Kind</li> <li>Nginx pod with a volume backed by Longhorn</li> <li>Steps to Install Longhorn on K3d</li> <li>Use Longhorn for PV and mount to MinIO</li> <li>Using Longhorn Storage with Kind Cluster</li> </ul>"},{"location":"#minio","title":"Minio","text":"<ul> <li>MinIO cli with MinIO and velero</li> <li>MinIO external for Velero backups</li> </ul>"},{"location":"#namespace","title":"Namespace","text":"<ul> <li>Default namespaces</li> <li>Delete all resources from namespace</li> </ul>"},{"location":"#nexus","title":"Nexus","text":"<ul> <li>Create a Docker image registry in Nexus</li> <li>Nexus Repository Manager OSS and integrate it with your Kubernetes</li> <li>Nexus oss setup - On-Prem</li> <li>Steps to uninstall Nexus it cleanly</li> <li>nexus3-cli-deprecated not working</li> </ul>"},{"location":"#nodejs","title":"Nodejs","text":"<ul> <li>Installing Node.js with Apt from the Default Repositories</li> </ul>"},{"location":"#opentofu","title":"Opentofu","text":"<ul> <li>About OpenTofu</li> </ul>"},{"location":"#pv-n-pvcs","title":"Pv n pvcs","text":"<ul> <li>PV's and PVC's in Kubernetes</li> </ul>"},{"location":"#pods","title":"Pods","text":"<ul> <li>Debug pod (ephermal container)</li> <li>Kubernetes Pod Architecture &amp; Lifecycle</li> <li>List all pods with their init containers and sidecar containers info</li> <li>Types of Containers in Kubernetes</li> <li>Types of Pods</li> </ul>"},{"location":"#prometheus-grafana","title":"Prometheus + grafana","text":"<ul> <li>Expose Prometheus and Grafana using Ingress</li> <li>Set up Prometheus + Grafana for your Kubernetes K3d cluster</li> </ul>"},{"location":"#rbac","title":"Rbac","text":"<ul> <li>Kubernetes RBAC</li> <li>htpasswd - based access with basic auth for k3s cluster</li> <li>htpasswd based authentication</li> </ul>"},{"location":"#sealed-secrets","title":"Sealed-secrets","text":"<ul> <li>Step By Step- Using Sealed Secrets with GitOps</li> <li>Steps to push sealed secrets for an app</li> </ul>"},{"location":"#secrets","title":"Secrets","text":"<ul> <li>Deep Dive into securing secrets in specific envs</li> <li>Secrets management for sensitive information</li> <li>Some real-world examples of secrets mismanagement</li> </ul>"},{"location":"#serviceaccounts","title":"Serviceaccounts","text":"<ul> <li>ServiceAccounts in Kubernetes</li> </ul>"},{"location":"#services","title":"Services","text":"<ul> <li>Headless Services in Kubernetes</li> <li>Types of Kubernetes Services</li> </ul>"},{"location":"#software","title":"Software","text":"<ul> <li>Best practice to follow - software versioning</li> </ul>"},{"location":"#statefulsets","title":"Statefulsets","text":"<ul> <li>Updating a MongoDB StatefulSet in Kubernetes</li> <li>W3H-Understanding StatefulSets in Kubernetes</li> </ul>"},{"location":"#storage","title":"Storage","text":"<ul> <li>About StorageClass fields</li> <li>Common Kubernetes storage types</li> <li>Setup NFS on Ubuntu Local Machine</li> <li>Storage Types</li> </ul>"},{"location":"#taint-tolerations","title":"Taint-tolerations","text":"<ul> <li>Understanding Tolerations in Kubernetes</li> <li>What is taint-toleration</li> </ul>"},{"location":"#traefik","title":"Traefik","text":"<ul> <li>Securing Traefik Dashboard with Keycloak (OIDC or OAuth2)</li> <li>Traefik Dashboard only for Visibility not for Administration</li> </ul>"},{"location":"#ubuntu","title":"Ubuntu","text":"<ul> <li>Check memory slots available</li> <li>list of good tools to extract rar part files</li> </ul>"},{"location":"#annotations","title":"Annotations","text":"<ul> <li>About annotations in k8s</li> </ul>"},{"location":"#etcdctl","title":"Etcdctl","text":"<ul> <li>Install etcdctl tool with supported versions</li> <li>To take backup of etcd db with etcdctl tool</li> <li>To take backup of etcd with minIO</li> <li>check etcd is running</li> <li>etcdctl commands for get, put, snapshot save and restore</li> <li>set up Kind with external etcd and Longhorn</li> </ul>"},{"location":"#fluentd","title":"Fluentd","text":"<ul> <li>Fluentd DaemonSet deployment YAML for Kubernetes</li> <li>Fluentd can run on standalone VMs</li> <li>Fluentd in Kubernetes</li> <li>Fluentd install and config steps for a Linux VM</li> </ul>"},{"location":"#git","title":"Git","text":"<ul> <li>Git file attributes</li> <li>How to Set Up Git Aliases</li> <li>View your configured for Git</li> <li>git commits - hacks</li> <li>git errors</li> </ul>"},{"location":"#images","title":"Images","text":""},{"location":"#mailhog","title":"Mailhog","text":"<ul> <li>Steps to deploy MailHog with Ingress</li> <li>Usecase of mailhog</li> </ul>"},{"location":"#mkdocs","title":"Mkdocs","text":"<ul> <li>mkdocs to github projects</li> <li>plugins list used with MkDocs-material</li> </ul>"},{"location":"#multipass","title":"Multipass","text":"<ul> <li>1. k8s hardway</li> <li>Setup access your cluster from the host machine</li> <li>To access ports from the host machine running multipass</li> <li>k3s with HA in multipass</li> <li>k8s with hard-way - overview</li> </ul>"},{"location":"#nginx","title":"Nginx","text":"<ul> <li>Configuring Multiple Services on Common Ports</li> <li>Installing and Configuring Nginx on Ubuntu</li> </ul>"},{"location":"#python","title":"Python","text":"<ul> <li>Microservice for Amazon affiliate marketing data scraping</li> <li>Microservices - Project</li> <li>mkdocs to github pages</li> <li>python pyautogui</li> </ul>"},{"location":"#templates","title":"Templates","text":"<ul> <li>frontmatter</li> </ul>"},{"location":"#velero-for-migration","title":"Velero - for migration","text":"<ul> <li>Schedule backups of NFS volumes using Velero</li> <li>Velero backup + restore setup with Longhorn</li> <li>Velero backup and restore command examples</li> <li>Velero with kind</li> </ul>"},{"location":"about/","title":"About Me","text":""},{"location":"about/#introduction","title":"\ud83d\udc4b Introduction","text":"<p>Hello! I'm Gouse Shaik, a DevOps Engineer and Homelab enthusiast passionate about building robust infrastructure and automation solutions.</p>"},{"location":"about/#technical-expertise","title":"\ud83d\udee0\ufe0f Technical Expertise","text":"<ul> <li>Cloud Platforms: AWS, GCP, Azure  </li> <li>Containers &amp; Orchestration: Docker, Podman, Kubernetes (k3d)  </li> <li>Infrastructure as Code: Terraform, Ansible  </li> <li>Monitoring Stack: Prometheus, Grafana, Alertmanager  </li> <li>CI/CD: GitHub Actions, ArgoCD  </li> <li>Scripting: Python, Bash  </li> </ul>"},{"location":"about/#my-homelab","title":"\ud83c\udfe0 My Homelab","text":""},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/","title":"RAG based DevOps assistant using an LLM","text":"<p>This is a simple local setup for a RAG-based DevOps assistant using an LLM on your machine with 8 GB RAM and 600 GB storage (CPU-only, no GPU).</p>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#what-youll-get","title":"\u2705 What You\u2019ll Get","text":"<p>A fully working local RAG system to:</p> <ul> <li> <p>Ask questions about CI/CD, Bash, YAML</p> </li> <li> <p>Search your own markdown, PDF, scripts</p> </li> <li> <p>Answer using a lightweight local LLM</p> </li> </ul>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#your-machine-constraints","title":"\u2699\ufe0f Your Machine Constraints","text":"Spec Status 8 GB RAM \u2705 Fine for 4-bit models (<code>Q4_K_M</code>) 600 GB Disk \u2705 More than enough CPU-only \u2705 Will use <code>llama-cpp-python</code>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#tools-used","title":"\ud83e\udde9 Tools Used","text":"Component Tool LLM <code>mistral-7b-instruct.Q4_K_M.gguf</code> Embeddings <code>all-MiniLM-L6-v2</code> (HuggingFace) Vector DB <code>Chroma</code> Language Layer <code>LangChain</code> Inference <code>llama-cpp-python</code> Interface CLI"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#setup-instructions-one-by-one","title":"\ud83d\udee0 Setup Instructions (One-by-One)","text":""},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#1-create-a-clean-python-env","title":"1. \ud83e\uddea Create a clean Python env","text":"<pre><code>python3 -m venv rag-devops\nsource rag-devops/bin/activate\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#2-install-dependencies","title":"2. \ud83d\udce6 Install dependencies","text":"<pre><code>pip install llama-cpp-python langchain chromadb sentence-transformers pypdf\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#3-create-directory-structure","title":"3. \ud83d\udcc1 Create directory structure","text":"<pre><code>mkdir -p devops-rag/{data,embeddings,model}\ncd devops-rag\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#4-download-gguf-model","title":"4. \u2b07\ufe0f Download GGUF model","text":"<p>Download the model from: \ud83d\udc49 https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF</p> <p>Pick: \ud83d\udfe9 <code>mistral-7b-instruct-v0.1.Q4_K_M.gguf</code></p> <p>Put it in:</p> <pre><code>devops-rag/model/mistral-7b-instruct.Q4_K_M.gguf\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#5-paste-this-into-load_and_indexpy","title":"5. \ud83e\udde0 Paste this into <code>load_and_index.py</code>","text":"<pre><code>from langchain.document_loaders import TextLoader, PDFMinerLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom sentence_transformers import SentenceTransformer\nimport os\n\nEMBEDDING_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\nDATA_DIR = \"data\"\n\ndef load_docs():\n    docs = []\n    for root, _, files in os.walk(DATA_DIR):\n        for file in files:\n            path = os.path.join(root, file)\n            if file.endswith(\".pdf\"):\n                docs.extend(PDFMinerLoader(path).load())\n            else:\n                docs.extend(TextLoader(path).load())\n    return docs\n\ndef index_documents():\n    docs = load_docs()\n    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    chunks = splitter.split_documents(docs)\n\n    texts = [chunk.page_content for chunk in chunks]\n    metadatas = [chunk.metadata for chunk in chunks]\n    embeddings = EMBEDDING_MODEL.encode(texts).tolist()\n\n    db = Chroma.from_embeddings(texts=texts, embedding=embeddings, metadatas=metadatas, persist_directory=\"embeddings\")\n    db.persist()\n\nif __name__ == \"__main__\":\n    index_documents()\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#6-paste-this-into-querypy","title":"6. \ud83e\udde0 Paste this into <code>query.py</code>","text":"<pre><code>from langchain.vectorstores import Chroma\nfrom langchain.llms import LlamaCpp\nfrom sentence_transformers import SentenceTransformer\n\nDB_DIR = \"embeddings\"\nMODEL_PATH = \"model/mistral-7b-instruct.Q4_K_M.gguf\"\nEMBEDDING_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef get_relevant_chunks(query, k=4):\n    vectordb = Chroma(persist_directory=DB_DIR)\n    embedded = EMBEDDING_MODEL.encode([query]).tolist()[0]\n    return vectordb.similarity_search_by_vector(embedded, k=k)\n\ndef ask_llm(context, question):\n    context_text = \"\\n\".join([doc.page_content for doc in context])\n    prompt = f\"\"\"### Instruction:\nYou are a DevOps assistant. Based on the context below, answer the question.\n\n### Context:\n{context_text}\n\n### Question:\n{question}\n\n### Answer:\"\"\"\n\n    llm = LlamaCpp(\n        model_path=MODEL_PATH,\n        n_ctx=2048,\n        temperature=0.3,\n        max_tokens=512,\n        n_threads=4,\n        verbose=False\n    )\n    return llm(prompt)\n\nif __name__ == \"__main__\":\n    while True:\n        q = input(\"\\nAsk your DevOps assistant: \")\n        if q.lower() == \"exit\": break\n        docs = get_relevant_chunks(q)\n        answer = ask_llm(docs, q)\n        print(\"\\n\ud83d\udd27 Answer:\\n\", answer)\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#7-add-files-to-data-folder","title":"7. \ud83d\ude80 Add files to <code>data/</code> folder","text":"<p>Put your <code>.md</code>, <code>.sh</code>, <code>Jenkinsfile</code>, <code>.yaml</code>, <code>.pdf</code> in <code>devops-rag/data/</code></p>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#8-build-the-vector-db","title":"8. \ud83e\udde0 Build the vector DB","text":"<pre><code>python load_and_index.py\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#9-ask-questions","title":"9. \ud83e\udd16 Ask questions!","text":"<pre><code>python query.py\n</code></pre> <p>Then ask:</p> <pre><code>How does this Jenkinsfile deploy to staging?\nWhat does this bash script do in step 3?\nExplain how to rollback a Helm chart using this doc\n</code></pre>"},{"location":"AI%20Engineer/RAG-based%20DevOps%20assistant%20using%20an%20LLM/#optional-add-ons","title":"\ud83e\udde9 Optional Add-ons","text":"Need Suggestion Web UI Add <code>streamlit</code> interface Dockerized Wrap in container + mount volumes PDF OCR Use <code>unstructured</code> + Tesseract <p>Let me know if you want:</p> <ul> <li> <p>A <code>streamlit</code> version of this</p> </li> <li> <p>Docker setup</p> </li> <li> <p>Autoloader that watches a Git repo</p> </li> </ul> <p>I\u2019ll give that in one shot too.</p>"},{"location":"AI%20Engineer/RAG/","title":"RAG","text":"<p>RAG (Retrieval-Augmented Generation) is a hybrid AI framework that combines retrieval-based systems with generative models to improve the accuracy, relevance, and factual grounding of responses. Let\u2019s break this down from a first-principles and critical thinking perspective.</p>"},{"location":"AI%20Engineer/RAG/#basic-principles-breakdown","title":"\ud83e\udde0 Basic Principles Breakdown","text":""},{"location":"AI%20Engineer/RAG/#1-what-problem-are-we-solving","title":"1. What problem are we solving?","text":"<p>Foundation models hallucinate \u2014 they generate plausible-sounding but incorrect or outdated answers. Also, they have limited memory of newer or domain-specific knowledge.</p>"},{"location":"AI%20Engineer/RAG/#2-what-do-llms-do-well","title":"2. What do LLMs do well?","text":"<ul> <li> <p>Language fluency</p> </li> <li> <p>Pattern recognition</p> </li> <li> <p>Synthesizing information</p> </li> <li> <p>Code, reasoning, Q&amp;A </p> </li> </ul> <p>But LLMs are static and frozen after training \u2014 they don\u2019t know recent data, private documents, or specific company policies.</p>"},{"location":"AI%20Engineer/RAG/#3-what-is-the-simplest-way-to-fix-this","title":"3. What is the simplest way to fix this?","text":"<p>Feed them the relevant context (from an external source) at runtime.</p> <p>Hence, RAG was born.</p>"},{"location":"AI%20Engineer/RAG/#what-is-rag-mechanism","title":"\u2699\ufe0f What is RAG? (Mechanism)","text":"Component Role Retriever Search system (e.g., FAISS, Elasticsearch) to find relevant text/doc Augmenter Adds the retrieved content as context into the LLM prompt Generator (LLM) Uses the augmented prompt to generate grounded answers"},{"location":"AI%20Engineer/RAG/#workflow","title":"\ud83d\udccd Workflow:","text":"<pre><code>User Query \u2192 Retrieve docs \u2192 Combine with query \u2192 Feed to LLM \u2192 Generate answer\n</code></pre>"},{"location":"AI%20Engineer/RAG/#components-in-practice","title":"\ud83e\uddf1 Components in Practice:","text":"Layer Example Tools Retriever FAISS, Weaviate, Elasticsearch Vector DB Chroma, Pinecone, Qdrant Embeddings OpenAI, HuggingFace, Cohere Generator Mistral, LLaMA, GPT, Claude, etc."},{"location":"AI%20Engineer/RAG/#evaluation","title":"\u2705 Evaluation","text":"Principle Evaluation Why not fine-tune? Expensive, static, inflexible. RAG gives dynamic memory without retraining. Is this scalable? Yes. You can plug in millions of docs, update data frequently, and it's fast. Does it solve hallucination? Reduces it, but doesn't eliminate it. Still depends on retrieval quality. How to measure success? Factual accuracy, source attribution, latency, cost-efficiency."},{"location":"AI%20Engineer/RAG/#what-is-video-checking-in-rag-context","title":"\ud83d\udcfc What is \u201cVideo Checking\u201d in RAG context?","text":"<p>If you meant \u201cvideo checking\u201d in the sense of:</p>"},{"location":"AI%20Engineer/RAG/#1-rag-applied-to-video-content","title":"\u2705 1. RAG applied to video content:","text":"<ul> <li> <p>Goal: Search and generate answers from videos.</p> </li> <li> <p>Method:</p> <ol> <li>Transcribe video using Whisper or other ASR tools.</li> <li>Chunk the transcript.</li> <li>Embed and store in vector DB.</li> <li>Apply RAG like on documents.</li> </ol> </li> </ul>"},{"location":"AI%20Engineer/RAG/#2-validating-the-source-fact-checking","title":"\u2705 2. Validating the source (fact-checking):","text":"<ul> <li> <p>RAG can cite where it pulled content from, so you can verify facts manually or programmatically from the source document (or transcript).</p> </li> <li> <p>Tools like: LangChain\u2019s <code>source_documents</code>, Guardrails.ai for checking hallucination.</p> </li> </ul>"},{"location":"AI%20Engineer/RAG/#example-use-case-rag-video-transcripts","title":"\ud83d\udd0d Example Use Case: RAG + Video Transcripts","text":"<pre><code>YouTube Video \u2192 Transcription \u2192 Chunk \u2192 Embed \u2192 Store in Pinecone \u2192\n\u2192 User Asks: \"What did the speaker say about AI in medicine?\" \u2192\n\u2192 Top chunks retrieved \u2192 Passed to LLM \u2192 Answer + Source timestamps\n</code></pre>"},{"location":"AI%20Engineer/RAG/#key-benefits-of-rag","title":"\ud83d\udd11 Key Benefits of RAG","text":"Benefit Why It Matters Dynamic Knowledge Pulls latest info from your docs/DB/web Reduced Hallucination Because it relies on grounded retrieved data Privacy/Control You can control what data the model uses Scalability Plug in any number of documents or formats Input Needed Example Options / Notes \ud83d\udcc2 Type of data <code>PDFs</code>, <code>Office Docs</code>, <code>HTML pages</code>, <code>YouTube Videos</code>, <code>Slack messages</code>, <code>Codebases</code> \ud83d\udccd Storage Location <code>Local disk</code>, <code>S3</code>, <code>MinIO</code>, <code>PostgreSQL</code>, <code>Git repo</code>, <code>Shared folder</code> \ud83d\udd0d Query Type <code>Ask Q&amp;A</code>, <code>Summarize</code>, <code>Generate Code</code>, <code>Search policy</code>, <code>Compliance check</code>, etc. \ud83c\udfaf User audience <code>DevOps</code>, <code>Developers</code>, <code>Customers</code>, <code>Support Team</code>, etc. \u2699\ufe0f Runtime <code>CPU-only</code>, <code>GPU</code>, <code>On-prem</code>, <code>Cloud</code>, <code>Docker</code>, <code>K8s</code> \ud83d\udd10 Data sensitivity <code>Internal only</code>, <code>Public</code>, <code>PII</code>, <code>Financial</code>, <code>Logs</code>, etc. \ud83d\udce4 Interface needed <code>CLI</code>, <code>Web UI</code>, <code>API</code>, <code>Slack bot</code>, <code>VSCode plugin</code> <p>Perfect. Here's a custom RAG architecture designed for your use-case: \u201cDevOps Assistant RAG for CI/CD Docs + Bash Scripts\u201d, running fully on-prem (CPU only).</p>"},{"location":"AI%20Engineer/RAG/#use-case-summary","title":"\ud83e\udde0 Use-Case Summary","text":"Feature Details Goal Answer questions about CI/CD processes, Jenkinsfiles, bash scripts Data Sources <code>.md</code>, <code>.sh</code>, <code>Jenkinsfile</code>, <code>.yaml</code>, <code>.pdf</code> Storage Local filesystem + Chroma DB Runtime CPU-only (no GPU), llama.cpp + GGUF models Interface CLI or Streamlit UI LLM Model <code>mistral-7b-instruct.Q4_K_M.gguf</code> via <code>llama-cpp-python</code> Security Fully offline/on-prem, private CI/CD content"},{"location":"AI%20Engineer/RAG/#rag-architecture-step-by-step","title":"\ud83d\udd27 RAG Architecture (Step-by-step)","text":"<pre><code># Directory structure example\ndevops_rag/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 scripts/           # bash, jenkinsfiles\n\u2502   \u251c\u2500\u2500 docs/              # markdown, pdf, yaml\n\u251c\u2500\u2500 embeddings/\n\u251c\u2500\u2500 app.py                 # main RAG app\n\u251c\u2500\u2500 load_and_index.py      # data loader and index builder\n\u251c\u2500\u2500 query.py               # query interface\n\u251c\u2500\u2500 model/                 # gguf models\n</code></pre>"},{"location":"AI%20Engineer/RAG/#tools-used","title":"\ud83d\udee0 Tools Used","text":"Component Tool Embedder <code>all-MiniLM-L6-v2</code> via <code>sentence-transformers</code> Vector DB <code>Chroma</code> (local, fast) Retriever <code>SimilaritySearch</code> from Chroma LLM <code>llama-cpp-python</code> with Mistral-7B GGUF Loader <code>Langchain</code>'s <code>TextLoader</code>, <code>PDFLoader</code> Splitter <code>RecursiveCharacterTextSplitter</code>"},{"location":"AI%20Engineer/RAG/#step-by-step-implementation","title":"\ud83e\uddea Step-by-Step Implementation","text":""},{"location":"AI%20Engineer/RAG/#1-install-requirements","title":"1. Install Requirements","text":"<pre><code>pip install chromadb langchain llama-cpp-python sentence-transformers pypdf\n</code></pre>"},{"location":"AI%20Engineer/RAG/#2-load_and_indexpy","title":"2. <code>load_and_index.py</code>","text":"<pre><code>from langchain.document_loaders import TextLoader, PDFMinerLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom sentence_transformers import SentenceTransformer\nimport os\n\nDATA_DIR = \"data\"\nEMBEDDING_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef load_docs():\n    docs = []\n    for root, _, files in os.walk(DATA_DIR):\n        for file in files:\n            path = os.path.join(root, file)\n            if file.endswith(\".pdf\"):\n                docs.extend(PDFMinerLoader(path).load())\n            else:\n                docs.extend(TextLoader(path).load())\n    return docs\n\ndef index_documents():\n    docs = load_docs()\n    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    chunks = splitter.split_documents(docs)\n\n    texts = [chunk.page_content for chunk in chunks]\n    metadatas = [chunk.metadata for chunk in chunks]\n    embeddings = EMBEDDING_MODEL.encode(texts).tolist()\n\n    db = Chroma.from_embeddings(texts=texts, embedding=embeddings, metadatas=metadatas, persist_directory=\"embeddings\")\n    db.persist()\n\nif __name__ == \"__main__\":\n    index_documents()\n</code></pre>"},{"location":"AI%20Engineer/RAG/#3-querypy","title":"3. <code>query.py</code>","text":"<pre><code>from langchain.vectorstores import Chroma\nfrom langchain.llms import LlamaCpp\nfrom sentence_transformers import SentenceTransformer\n\nDB_DIR = \"embeddings\"\nMODEL_PATH = \"model/mistral-7b-instruct.Q4_K_M.gguf\"\nEMBEDDING_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ndef get_relevant_chunks(query, k=4):\n    vectordb = Chroma(persist_directory=DB_DIR)\n    embedded = EMBEDDING_MODEL.encode([query]).tolist()[0]\n    return vectordb.similarity_search_by_vector(embedded, k=k)\n\ndef ask_llm(context, question):\n    context_text = \"\\n\".join([doc.page_content for doc in context])\n    prompt = f\"\"\"### Instruction:\nYou are a DevOps assistant. Based on the context below, answer the question.\n\n### Context:\n{context_text}\n\n### Question:\n{question}\n\n### Answer:\"\"\"\n\n    llm = LlamaCpp(model_path=MODEL_PATH, n_ctx=2048, temperature=0.3, max_tokens=512, verbose=False)\n    return llm(prompt)\n\nif __name__ == \"__main__\":\n    while True:\n        q = input(\"\\nAsk your DevOps assistant: \")\n        if q.lower() == \"exit\": break\n        docs = get_relevant_chunks(q)\n        answer = ask_llm(docs, q)\n        print(\"\\n\ud83d\udd27 Answer:\\n\", answer)\n</code></pre>"},{"location":"AI%20Engineer/RAG/#usage","title":"\ud83d\ude80 Usage","text":"<pre><code># Step 1: Index the documents\npython load_and_index.py\n\n# Step 2: Start querying\npython query.py\n</code></pre>"},{"location":"AI%20Engineer/RAG/#enhancements-optional","title":"\ud83d\udee1\ufe0f Enhancements (Optional)","text":"Feature How to Add Streamlit UI <code>streamlit</code>, add dropdowns and code display Source citation Print <code>doc.metadata[\"source\"]</code> with answers Auto-ingest Git files Use <code>GitPython</code> to pull and parse Git repos Schedule re-indexing Use <code>cron</code> or <code>watchdog</code> - might be you could have a a Streamlit web UI Or a dockerized setup with config for local run?"},{"location":"AI%20Engineer/Reasoning%20Models/","title":"Reasoning Models","text":"<p>Here\u2019s a list of powerful reasoning-focused open-source LLMs you can run locally (offline) \u2014 especially suitable for DevOps, coding, and logic-heavy RAG use cases like yours.</p>"},{"location":"AI%20Engineer/Reasoning%20Models/#top-reasoning-capable-llms-open-local","title":"\ud83e\udde0 Top Reasoning-Capable LLMs (Open, Local)","text":"Model Name Size GGUF Ready Reasoning Strength \ud83d\udd0d Notes Mistral-7B-Instruct 7B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Fast, clean instruction following Mixtral-8x7B-Instruct 12.9B (MoE) \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Best open MoE model, solid reasoning Nous Hermes 2 - Mistral 7B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Refined for long reasoning, RAG-friendly OpenHermes 2.5 (Mistral) 7B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Best general-purpose reasoning mix MythoMax-L2 (LLaMA2) 13B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Great for philosophical/logical depth Deepseek-Coder-Instruct 7B, 33B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Best for DevOps, Bash, and pipelines WizardLM 2 - Mistral 7B 7B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 Clean logic and coding reasoning LLaMA 3 - Instruct 8B, 70B \u2705 8B only \ud83e\udde0\ud83e\udde0\ud83e\udde0\ud83e\udde0 70B has best reasoning, 8B okay on CPU Phi-3-mini (Microsoft) 3.8B \u2705 Yes \ud83e\udde0\ud83e\udde0\ud83e\udde0 Great at math, code, low footprint"},{"location":"AI%20Engineer/Reasoning%20Models/#recommended-model-for-you-cpu-on-prem-devops-focus","title":"\ud83d\udee0 Recommended Model for You (CPU On-Prem, DevOps Focus)","text":"Setup Priority Recommended Model \u2705 Best reasoning + fast Mixtral-8x7B-Instruct.Q4_K_M.gguf \ud83e\udde9 Bash + CI/CD scripts Deepseek-Coder-7B-Instruct.Q4_K_M.gguf \ud83e\udde0 Light + precise RAG Nous Hermes 2 - Mistral.Q4_K_M.gguf"},{"location":"AI%20Engineer/Reasoning%20Models/#where-to-download-gguf","title":"\ud83e\udde9 Where to Download (GGUF)","text":"<p>Use these sites to download <code>.gguf</code> models:</p> Site URL HuggingFace huggingface.co/TheBloke LM Studio models lmstudio.ai/models Ollama models ollama.com/library <p>Search for models like: <code>\"TheBloke/Mixtral-8x7B-Instruct-GGUF\"</code> or <code>\"NousResearch/Nous-Hermes-2-Mistral-GGUF\"</code></p>"},{"location":"AI%20Engineer/Reasoning%20Models/#1-what-is-size","title":"\ud83e\uddf1 1. What is Size?","text":""},{"location":"AI%20Engineer/Reasoning%20Models/#definition","title":"\u2705 Definition:","text":"<p>\u201cSize\u201d refers to the number of parameters in the model \u2014 the neurons or weights that it uses to understand and generate language.</p> Size Meaning Performance Resource Need 3B 3 Billion parameters Small, fast Very light 7B 7 Billion parameters Good reasoning + fast Medium 13B 13 Billion parameters Strong reasoning, slower Heavy on CPU 33B+ 33 Billion or more (MoE) Top-tier, needs good hardware Heavy 70B 70 Billion (LLaMA 3 etc.) Very strong, GPU usually needed Not for CPU <p>\ud83e\udde0 Rule of thumb: Bigger = more knowledge and better logic, but slower and heavier.</p>"},{"location":"AI%20Engineer/Reasoning%20Models/#2-what-is-gguf","title":"\ud83d\udce6 2. What is GGUF?","text":""},{"location":"AI%20Engineer/Reasoning%20Models/#definition_1","title":"\u2705 Definition:","text":"<p>GGUF (GPT-Generated Unified Format) is a compressed format for LLMs used by <code>llama.cpp</code>, optimized for offline/CPU use.</p> Feature Why It Matters <code>.gguf</code> file You download this for the model Works with <code>llama-cpp-python</code>, <code>text-generation-webui</code>, etc. Quantized levels <code>Q2</code>, <code>Q4</code>, <code>Q5</code>, <code>Q6</code>, <code>Q8</code> Lower Q \u2192 Faster <code>Q4_K_M</code> is a balance of size + accuracy <p>\ud83e\udde0 Use <code>.gguf</code> if you're running LLMs locally without GPU (e.g., CPU-only servers).</p>"},{"location":"AI%20Engineer/Reasoning%20Models/#3-what-is-reasoning","title":"\ud83d\udd0d 3. What is Reasoning?","text":""},{"location":"AI%20Engineer/Reasoning%20Models/#definition_2","title":"\u2705 Definition:","text":"<p>Reasoning means how well the model can analyze, connect concepts, solve problems, explain logic, or understand steps in CI/CD, DevOps, or coding pipelines.</p> Task Needs Reasoning? \u201cWhat is this bash script doing?\u201d \u2705 Yes \u201cExplain how to fix Jenkins error\u201d \u2705 Yes \u201cWhat does <code>kubectl rollout</code> do?\u201d \u2705 Yes \u201cTranslate English to French\u201d \u274c Not much <p>\ud83e\udde0 Higher reasoning = better at complex DevOps queries, logic steps, troubleshooting.</p>"},{"location":"AI%20Engineer/Reasoning%20Models/#summary-table","title":"\ud83c\udfaf Summary Table","text":"Term Simple Meaning Why It Matters to You Size Brain size of the model (e.g., 7B) Bigger = smarter but slower GGUF Local CPU-friendly file format Lets you run LLMs offline easily Reasoning Model's ability to solve, explain, debug Needed for CI/CD, Bash, logic use <p>If you want, I can give you:</p> <ul> <li> <p>A table of top GGUF models by reasoning power</p> </li> <li> <p>Command to run the model using <code>llama-cpp-python</code> on your machine</p> </li> <li> <p>Test prompts to compare reasoning output</p> </li> </ul> <p>Just say the word.</p>"},{"location":"AI%20Engineer/Vibe%20Checking/","title":"Vibe Checking","text":""},{"location":"AI%20Engineer/Vibe%20Checking/#what-is-vibe-checking","title":"\ud83d\udd0d What is Vibe Checking?","text":"<p>Vibe checking is an informal, often subjective way to assess the emotional atmosphere of a person, group, situation, or even content. It's not about facts \u2014 it's about how something feels.</p>"},{"location":"AI%20Engineer/Vibe%20Checking/#first-principles-thinking","title":"\ud83e\udde0 First Principles Thinking","text":""},{"location":"AI%20Engineer/Vibe%20Checking/#1-what-is-a-vibe","title":"1. What is a \u201cvibe\u201d?","text":"<p>A \"vibe\" is an emotional signal or felt impression \u2014 like an invisible energy that you interpret.</p> <ul> <li>Someone walks in a room \u2014 you \u201cfeel\u201d they\u2019re confident or nervous.</li> <li>You read a text \u2014 it \u201cfeels\u201d passive-aggressive or supportive.</li> <li>You join a team call \u2014 it \u201cfeels\u201d tense or relaxed.</li> </ul>"},{"location":"AI%20Engineer/Vibe%20Checking/#2-why-do-we-check-it","title":"2. Why do we \u201ccheck\u201d it?","text":"<p>Because emotions shape decisions, reactions, and outcomes. You check vibes to:</p> <ul> <li>Avoid conflict</li> <li>Fit in better</li> <li>Choose the right tone</li> <li>Protect your energy</li> </ul>"},{"location":"AI%20Engineer/Vibe%20Checking/#3-how-do-humans-do-it","title":"3. How do humans do it?","text":"<ul> <li>Body language</li> <li>Facial expressions</li> <li>Tone of voice</li> <li>Word choice</li> <li>Timing, silence</li> <li>Social context</li> </ul> <p>You pattern match using past experiences and cultural norms.</p>"},{"location":"AI%20Engineer/Vibe%20Checking/#basic-view","title":"\u2705 Basic View","text":"Aspect Insight Is it reliable? Not always \u2014 vibes are subjective and prone to bias. Can it be trained? Yes, through emotional intelligence and active observation. Can machines do it? Partially. NLP models can analyze sentiment, tone, and emotion, but lack full human empathy or context. Why is it important in leadership or tech? Team trust, communication tone, feedback quality \u2014 all depend on \u201creading the room\u201d well."},{"location":"AI%20Engineer/Vibe%20Checking/#vibe-check-in-different-contexts","title":"\ud83e\udde0 Vibe Check in Different Contexts","text":"Context What You\u2019re Assessing Text Message Tone: friendly vs dry vs angry Meeting Room Group energy: collaborative vs defensive Code Review Tone of comments: constructive vs nitpicky Social Media Post Authenticity, negativity, sarcasm, trolling Workplace Slack Burnout signals, mood shifts in message tone"},{"location":"AI%20Engineer/Vibe%20Checking/#can-you-build-a-vibe-checker","title":"\ud83d\udee0 Can You Build a Vibe Checker?","text":"<p>Yes \u2014 using NLP + sentiment/tone detection:</p> <ol> <li> <p>Input text</p> </li> <li> <p>Use models like:</p> <ul> <li><code>transformers</code> (HuggingFace): e.g., <code>cardiffnlp/twitter-roberta-base-sentiment</code></li> <li>OpenAI's GPT + function calling</li> <li><code>textblob</code>, <code>VADER</code> (for quick setups)</li> </ul> </li> <li> <p>Output:</p> <ul> <li>Sentiment: Positive / Neutral / Negative</li> <li>Emotion: Joy / Anger / Sadness / Fear / Surprise</li> <li>Tone: Formal / Friendly / Aggressive</li> </ul> </li> </ol>"},{"location":"AI%20Engineer/Vibe%20Checking/#quick-example","title":"\ud83e\uddea Quick Example","text":"<p>Text: \u201cSure, do whatever you want \ud83d\ude44\u201d</p> <ul> <li>Sentiment: Negative</li> <li>Emotion: Sarcasm / Resentment</li> <li>Tone: Passive-aggressive</li> <li>\u2192 Vibe Check: Bad mood / conflict likely</li> </ul>"},{"location":"AI%20Engineer/Vibe%20Checking/#python-script-text-vibe-checker","title":"\u2705 Python Script \u2013 Text Vibe Checker","text":"<pre><code>from transformers import pipeline\n\n# Load sentiment and emotion models\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\nemotion_pipeline = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=2)\n\ndef vibe_check(text):\n    print(f\"\\n\ud83d\udcdd Input: {text}\\n\" + \"-\" * 40)\n\n    # Sentiment check\n    sentiment = sentiment_pipeline(text)[0]\n    print(f\"\ud83d\udd0d Sentiment: {sentiment['label']} ({sentiment['score']:.2f})\")\n\n    # Emotion check\n    emotions = emotion_pipeline(text)\n    print(\"\ud83c\udfad Emotions:\")\n    for em in emotions:\n        print(f\"  - {em['label']}: {em['score']:.2f}\")\n\n    # Vibe judgment\n    if sentiment[\"label\"] == \"NEGATIVE\" or any(e[\"label\"] in [\"anger\", \"disgust\", \"fear\"] for e in emotions):\n        print(\"\ud83d\udea8 Vibe Check: \u26a0\ufe0f Something feels off.\")\n    elif sentiment[\"label\"] == \"POSITIVE\" and any(e[\"label\"] in [\"joy\", \"love\"] for e in emotions):\n        print(\"\u2705 Vibe Check: \ud83d\udc4d All good, positive energy.\")\n    else:\n        print(\"\u2696\ufe0f Vibe Check: \ud83d\ude10 Neutral or mixed signals.\")\n\nif __name__ == \"__main__\":\n    while True:\n        text = input(\"\\nEnter text to vibe check (or type 'exit'): \")\n        if text.lower() == \"exit\":\n            break\n        vibe_check(text)\n</code></pre>"},{"location":"AI%20Engineer/Vibe%20Checking/#setup-instructions","title":"\ud83e\udde9 Setup Instructions","text":"<pre><code>pip install transformers torch\n</code></pre>"},{"location":"AI%20Engineer/Vibe%20Checking/#example","title":"\ud83d\udee0 Example","text":"<pre><code>Enter text to vibe check: Sure, do whatever you want \ud83d\ude44\n\n\ud83d\udcdd Input: Sure, do whatever you want \ud83d\ude44\n----------------------------------------\n\ud83d\udd0d Sentiment: NEGATIVE (0.99)\n\ud83c\udfad Emotions:\n  - anger: 0.75\n  - disgust: 0.21\n\ud83d\udea8 Vibe Check: \u26a0\ufe0f Something feels off.\n</code></pre>"},{"location":"API-Resources/k8s-api-resources-details/","title":"K8s api resources details","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To discover Kubernetes API resources and their available fields/values for YAML files, you can use the following commands:</p>"},{"location":"API-Resources/k8s-api-resources-details/#1-list-all-api-resources","title":"1. List All API Resources","text":"<p><pre><code>kubectl api-resources\n</code></pre> Output Example: <pre><code>NAME          SHORTNAMES   APIVERSION     NAMESPACED   KIND\npods          po           v1             true         Pod\ndeployments   deploy       apps/v1        true         Deployment\nservices      svc          v1             true         Service\ningresses     ing          networking.k8s.io/v1   true  Ingress\nclusterissuers             cert-manager.io/v1     false   ClusterIssuer\n</code></pre></p> <p>The\u00a0<code>false</code>\u00a0value means that\u00a0<code>ClusterIssuer</code>\u00a0is a cluster-scoped resource, not a namespaced resource. Here's what this means:</p> <ol> <li> <p><code>false</code>\u00a0(cluster-scoped):</p> <ul> <li>The resource exists at the cluster level</li> <li>Not tied to any specific namespace</li> <li>Can be referenced from any namespace</li> <li>Typically includes resources that affect the entire cluster</li> </ul> </li> <li> <p><code>true</code>\u00a0(namespaced)</p> <ul> <li>The resource exists within a specific namespace</li> <li>Must be created in a namespace</li> <li>Only accessible within that namespace</li> </ul> </li> </ol>"},{"location":"API-Resources/k8s-api-resources-details/#2-get-api-version-for-a-resource","title":"2. Get API Version for a Resource","text":"<p><pre><code>kubectl explain &lt;resource&gt; | head -n 2\n</code></pre> Example: <pre><code>kubectl explain deployment | head -n 2\n</code></pre> Output: <pre><code>KIND:     Deployment\nVERSION:  apps/v1\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#3-view-all-available-fields-in-a-resource","title":"3. View All Available Fields in a Resource","text":"<p><pre><code>kubectl explain &lt;resource&gt; --recursive\n</code></pre> Example (Deployment): <pre><code>kubectl explain deployment --recursive\n</code></pre> Output (Partial): <pre><code>KIND:     Deployment\nVERSION:  apps/v1\nDESCRIPTION:\n     Deployment enables declarative updates for Pods and ReplicaSets.\nFIELDS:\n   apiVersion   &lt;string&gt;\n   kind &lt;string&gt;\n   metadata     &lt;Object&gt;\n      annotations       &lt;map[string]string&gt;\n      creationTimestamp &lt;string&gt;\n      name      &lt;string&gt;\n      namespace &lt;string&gt;\n   spec &lt;Object&gt;\n      replicas  &lt;integer&gt;\n      selector  &lt;Object&gt;\n         matchLabels    &lt;map[string]string&gt;\n      template  &lt;Object&gt;\n         metadata       &lt;Object&gt;\n            labels      &lt;map[string]string&gt;\n         spec   &lt;Object&gt;\n            containers  &lt;[]Object&gt;\n               name     &lt;string&gt;\n               image    &lt;string&gt;\n               ports    &lt;[]Object&gt;\n                  containerPort    &lt;integer&gt;\n...\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#4-check-specific-field-details","title":"4. Check Specific Field Details","text":"<p><pre><code>kubectl explain &lt;resource&gt;.&lt;field&gt;\n</code></pre> Examples: <pre><code># Check Deployment's `spec.strategy`\nkubectl explain deployment.spec.strategy\n\n# Check Pod's `spec.containers`\nkubectl explain pod.spec.containers\n\n# Check Ingress `spec.rules`\nkubectl explain ingress.spec.rules\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#5-list-all-possible-values-for-enums-eg-restartpolicy","title":"5. List All Possible Values for Enums (e.g., <code>restartPolicy</code>)","text":"<p><pre><code>kubectl explain pod.spec.restartPolicy\n</code></pre> Output: <pre><code>KIND:     Pod\nVERSION:  v1\nFIELD:    restartPolicy &lt;string&gt;\nDESCRIPTION:\n     Restart policy for all containers within the pod.\n     Possible enum values:\n     - `\"Always\"`\n     - `\"OnFailure\"`\n     - `\"Never\"`\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#6-view-openapiswagger-docs-full-schema","title":"6. View OpenAPI/Swagger Docs (Full Schema)","text":"<p><pre><code>kubectl get --raw /openapi/v2 | jq . | less\n</code></pre> (Use <code>jq</code> for filtering, e.g., <code>kubectl get --raw /openapi/v2 | jq '.definitions[\"io.k8s.api.apps.v1.Deployment\"]'</code>)</p>"},{"location":"API-Resources/k8s-api-resources-details/#7-check-crd-custom-resource-schema","title":"7. Check CRD (Custom Resource) Schema","text":"<p><pre><code>kubectl get crd &lt;crd-name&gt; -o yaml\n</code></pre> Example: <pre><code>kubectl get crd traefikservices.traefik.io -o yaml\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#8-shortcut-for-common-resources","title":"8. Shortcut for Common Resources","text":"Resource Command to View Schema Pod <code>kubectl explain pod</code> Deployment <code>kubectl explain deployment</code> Service <code>kubectl explain service</code> Ingress <code>kubectl explain ingress</code> ConfigMap <code>kubectl explain configmap</code> Secret <code>kubectl explain secret</code>"},{"location":"API-Resources/k8s-api-resources-details/#summary-cheatsheet","title":"Summary Cheatsheet","text":"Command Purpose <code>kubectl api-resources</code> List all available resources <code>kubectl explain &lt;resource&gt;</code> Show YAML structure of a resource <code>kubectl explain &lt;resource&gt; --recursive</code> Show all nested fields <code>kubectl explain &lt;resource&gt;.&lt;field&gt;</code> Check a specific field <code>kubectl get --raw /openapi/v2</code> View full OpenAPI schema <p>These commands help you discover valid fields and values when writing Kubernetes YAML files. \ud83d\ude80</p>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/","title":"Add Elastic repo and install filebeat on app nodes","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#step-1-install-filebeat-on-your-localhost-where-nginx-runs","title":"Step 1: Install Filebeat on your localhost (where NGINX runs):","text":"<pre><code># Install prerequisite packages\nsudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https wget\n\n# Download and install Elastic GPG key\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n\n# Add Elastic repo to your sources list (for Ubuntu 24.04 'noble')\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n\n# Update and install filebeat\nsudo apt-get update &amp;&amp; sudo apt-get install filebeat -y\n</code></pre>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#step-2-enable-and-configure-the-nginx-module-in-filebeat","title":"Step 2: Enable and configure the NGINX module in Filebeat:","text":"<pre><code>sudo filebeat modules enable nginx\n</code></pre>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#step-3-edit-filebeat-config-file","title":"Step 3: Edit Filebeat config file","text":"<p>Edit Filebeat config <code>/etc/filebeat/filebeat.yml</code> to point to your ELK (Elasticsearch) inside Multipass:</p> <pre><code>Add or edit output Elasticsearch section:\n\n# vi /etc/filebeat/filebeat.yml\n\noutput.elasticsearch:\n  hosts: [\"&lt;multipass-ip&gt;:9200\"]\n  username: \"elastic\"\n  password: \"&lt;your-elastic-password&gt;\"\n\n\noutput.elasticsearch:\n  hosts: [\"&lt;multipass-ip&gt;:9200\"]\n  username: \"elastic\"\n  password: \"&lt;your-elastic-password&gt;\"\n</code></pre>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#step-4-set-up-filebeat-to-read-nginx-logs","title":"Step 4: Set up Filebeat to read NGINX logs","text":"<p>Set up Filebeat to read NGINX logs (usually <code>/var/log/nginx/access.log</code> and <code>/var/log/nginx/error.log</code>):</p> <p>The NGINX module automatically configures this, so no extra manual input is needed if enabled.</p> <ol> <li>*Start and enable Filebeat: <pre><code>gouse@gouse:~/DevOps/multipass_scripts$ sudo filebeat modules\nManage configured modules\n\nUsage:\n  filebeat modules [command]\n\nAvailable Commands:\n  disable     Disable one or more given modules\n  enable      Enable one or more given modules\n  list        List modules\n\n\nsudo systemctl enable filebeat sudo systemctl start filebeat\n</code></pre></li> </ol> <p>Note: On ELK (Multipass VM), make sure ports 9200 (Elasticsearch) and 5601 (Kibana) are accessible from your localhost (via port forwarding or firewall rules).</p>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#now-enable-logs-values-in-nginx-inside-filebeat-dir","title":"now enable logs values in nginx inside filebeat dir","text":"<p>cd /etc/filebeat/modules.d take backup of existing conf <code>nginx.yml</code> vi `nginx.yml' <pre><code>- module: nginx\n  access:\n    enabled: true\n    var.paths: [\"/var/log/nginx/access.log\"]\n\n  error:\n    enabled: true\n    var.paths: [\"/var/log/nginx/error.log\"]\n\n  ingress_controller:\n    enabled: false\n</code></pre></p> <p>\ud83d\udd04 Then reload and restart Filebeat: <pre><code>sudo filebeat modules enable nginx     # Optional if already done\nsudo filebeat setup                    # Optional: sets up dashboards\nsudo systemctl restart filebeat\n</code></pre></p>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#to-confirm-it-works","title":"\ud83e\uddea To confirm it works:","text":"<ul> <li>Check in Kibana \u2192 Discover \u2192 <code>event.module : \"nginx\"</code></li> <li>Or check CLI:</li> </ul> <pre><code>curl -u elastic:admin123 http://localhost:9200/filebeat-*/_search?q=event.module:nginx&amp;pretty\n</code></pre> <pre><code>Test the configuration.\nsudo filebeat test config\ngouse@gouse:/etc/filebeat/modules.d$ sudo filebeat test config\n[sudo] password for gouse:\nConfig OK\n</code></pre> <pre><code>Apply Filebeat setup changes.\nsudo filebeat setup\n</code></pre> <pre><code>gouse@gouse:/etc/filebeat/modules.d$ sudo filebeat setup\nOverwriting lifecycle policy is disabled. Set `setup.ilm.overwrite: true` to overwrite.\nSDK 2025/06/29 12:32:01 WARN falling back to IMDSv1: operation error ec2imds: getToken, http response error StatusCode: 404, request to EC2 IMDS failed\nIndex setup finished.\nLoading dashboards (Kibana must be running and reachable)\nExiting: error connecting to Kibana: fail to get the Kibana version: HTTP GET request to http://localhost:5601/api/status fails: status=503. Response: {\"status\":{\"overall\":{\"level\":\"critical\"}}}\n</code></pre> <pre><code>ubuntu@elk-vm:~$ curl http://localhost:5601/api/status\n{\"status\":{\"overall\":{\"level\":\"unavailable\"}}}ubuntu@elk-vm:~$\n\n# update as below vlaues in kibana config\n$ sudo vi /etc/kibana/kibana.yml\nelasticsearch.hosts: [\"https://localhost:9200\"]\nelasticsearch.ssl.verificationMode: \"none\"\n\n\n\n# resovled the issue by disabling the \n# server.port: 5601\n\n$ sudo systemctl status kibana\n$ sudo systemctl start kibana\n$ sudo systemctl status kibana --no-page\n</code></pre> <pre><code>$ sudo journalctl -u kibana -e\n\n[ERROR][elasticsearch-service] Unable to retrieve version information from Elasticsearch nodes. write EPROTO ...\n\nfrom:\nelasticsearch.hosts: ['https://localhost:9200']\nTo:\nelasticsearch.hosts: ['http://localhost:9200']\nsudo systemctl restart kibana\n\nso , now issue resolved:\n$ sudo journalctl -u kibana -e\nJun 29 13:42:51 elk-vm systemd[1]: Started Kibana.\n....\nJun 29 13:43:12 elk-vm kibana[58924]: [2025-06-29T13:43:12.946+03:00][INFO ][http.server.Preboot] http server running at http://0.0.0.0:5601\nJun 29 13:43:13 elk-vm kibana[58924]: [2025-06-29T13:43:13.349+03:00][INFO ][plugins-system.preboot] Setting up [1] plugins: [interactiveSetup]\n</code></pre> <p>Able to access dashboard <pre><code>http://100.75.49.6:5601/app/home#/\n</code></pre></p> <p>Here's a simple explanation:</p>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#filebeatyml-filebeat-configuration-file","title":"\ud83d\udcc4 <code>filebeat.yml</code> (Filebeat Configuration File)","text":"<ul> <li> <p>This is the main configuration file for Filebeat, the log shipping agent.</p> </li> <li> <p>It tells Filebeat:</p> <ul> <li>What files to watch (log paths)</li> <li>How to process logs (parsers, multiline, etc.)</li> <li>Where to send logs (Elasticsearch, Logstash, etc.)</li> </ul> </li> </ul>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#example-filebeatyml","title":"\ud83e\uddfe Example: <code>filebeat.yml</code>","text":"<pre><code>filebeat.inputs:\n  - type: log\n    paths:\n      - /var/log/nginx/*.log\n\noutput.logstash:\n  hosts: [\"elk-logstash:5044\"]\n</code></pre> <pre><code>sudo filebeat test config\nsudo systemctl restart filebeat\n</code></pre>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#explanation","title":"\ud83d\udd0d Explanation:","text":"Section Purpose <code>filebeat.inputs</code> Defines which files Filebeat reads <code>type: log</code> Says the input is a regular log file <code>paths</code> Log file paths to monitor <code>output.logstash</code> Defines destination (Logstash in this case) <code>hosts</code> IP/hostname of Logstash server"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#nginxconf-or-nginxyml-nginx-config-file","title":"\ud83d\udcc4 <code>nginx.conf</code> or <code>nginx.yml</code> (NGINX Config File)","text":"<ul> <li> <p>This config defines how NGINX handles HTTP traffic.</p> </li> <li> <p>Controls:</p> <ul> <li>Listening ports (e.g., 80/443)</li> <li>Proxy rules</li> <li>SSL certs</li> <li>Load balancing</li> </ul> </li> </ul>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#example-nginx-proxy-config","title":"\ud83e\uddfe Example: NGINX Proxy Config","text":"<pre><code>server {\n    listen 80;\n\n    location / {\n        proxy_pass http://127.0.0.1:5601;\n    }\n}\n</code></pre>"},{"location":"APM/Add%20Elastic%20repo%20and%20install%20filebeat%20on%20app%20nodes/#explanation_1","title":"\ud83d\udd0d Explanation:","text":"Section Purpose <code>server</code> Defines an HTTP listener block <code>listen 80</code> NGINX listens on port 80 (HTTP) <code>location /</code> For all paths (<code>/</code>), apply the rule below <code>proxy_pass</code> Forward requests to Kibana on localhost ## \u2696\ufe0f Difference Summary Aspect <code>filebeat.yml</code> <code>nginx.conf</code> or <code>nginx.yml</code> Used by Filebeat (log shipper) NGINX (web/proxy server) Purpose Read and forward logs Handle and route HTTP traffic Format YAML NGINX syntax (not YAML) Key Focus Log source and destination Request routing and reverse proxying <p>Let me know if you want both configs customized for your ELK setup.</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/","title":"Docker APM images and setup steps","text":"<pre><code>sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:8.13.0\nsudo docker pull docker.elastic.co/kibana/kibana:8.13.0\nsudo docker pull docker.elastic.co/apm/apm-server:8.13.0\n\nsudo docker run -d --name elasticsearch --network=host \\\n  -e discovery.type=single-node \\\n  -e xpack.security.enabled=false \\\n  -e ES_JAVA_OPTS='-Xms1g -Xmx1g' \\\n  docker.elastic.co/elasticsearch/elasticsearch:8.13.0\n\nsudo docker run -d --name kibana --network=host \\\n  -e ELASTICSEARCH_HOSTS=http://localhost:9200 \\\n  docker.elastic.co/kibana/kibana:8.13.0\n\nsudo docker run -d --name apm-server --network=host \\\n  -e ELASTICSEARCH_HOSTS=http://localhost:9200 \\\n  -e KIBANA_HOST=http://localhost:5601 \\\n  docker.elastic.co/apm/apm-server:8.13.0\n\n\nhttp://10.189.65.114:9200/\nhttp://10.189.65.114:5601/\nhttp://10.189.65.114:8200/\n</code></pre> <p>Here is a simple Java Spring Boot app that sends traces to Elastic APM.</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#1-create-pomxml-maven-with-dependencies","title":"1. Create <code>pom.xml</code> (Maven) with dependencies:","text":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" \n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 \n                             http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n  &lt;groupId&gt;com.example&lt;/groupId&gt;\n  &lt;artifactId&gt;apm-demo&lt;/artifactId&gt;\n  &lt;version&gt;1.0.0&lt;/version&gt;\n  &lt;packaging&gt;jar&lt;/packaging&gt;\n\n  &lt;parent&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n    &lt;version&gt;2.7.0&lt;/version&gt;\n  &lt;/parent&gt;\n\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n      &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n&lt;build&gt;\n  &lt;plugins&gt;\n    &lt;plugin&gt;\n      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n      &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n      &lt;version&gt;2.7.0&lt;/version&gt;\n      &lt;executions&gt;\n        &lt;execution&gt;\n          &lt;goals&gt;\n            &lt;goal&gt;repackage&lt;/goal&gt;\n          &lt;/goals&gt;\n        &lt;/execution&gt;\n      &lt;/executions&gt;\n    &lt;/plugin&gt;\n  &lt;/plugins&gt;\n&lt;/build&gt;\n\n&lt;/project&gt;\n</code></pre>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#2-create-srcmainjavacomexampleapmdemoapmdemoapplicationjava","title":"2. Create <code>src/main/java/com/example/apmdemo/ApmDemoApplication.java</code>:","text":"<pre><code>package com.example.apmdemo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n@SpringBootApplication\npublic class ApmDemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(ApmDemoApplication.class, args);\n    }\n}\n\n@RestController\nclass HelloController {\n\n    @GetMapping(\"/hello\")\n    public String hello() throws InterruptedException {\n        // Simulate work\n        Thread.sleep(100);\n        return \"Hello from APM Demo!\";\n    }\n}\n</code></pre>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#3-run-the-app-with-elastic-apm-agent","title":"3. Run the app with Elastic APM Agent:","text":"<ul> <li>Download Elastic APM Java agent jar:</li> </ul> <pre><code>curl -L -o elastic-apm-agent.jar https://search.maven.org/remotecontent?filepath=co/elastic/apm/elastic-apm-agent/1.51.0/elastic-apm-agent-1.51.0.jar\n</code></pre> <ul> <li>Run Spring Boot app:</li> </ul> <pre><code>mvn clean package\n\njava -javaagent:/path/to/elastic-apm-agent.jar \\\n     -Delastic.apm.service_name=apm-demo-app \\\n     -Delastic.apm.server_urls=http://localhost:8200 \\\n     -Delastic.apm.application_packages=com.example.apmdemo \\\n     -jar target/apm-demo-1.0.0.jar\n\njava -javaagent:/home/ubuntu/SpringBootApp-demo/elastic-apm-agent.jar \\\n     -Delastic.apm.service_name=apm-demo-app \\\n     -Delastic.apm.server_urls=http://localhost:8200 \\\n     -Delastic.apm.application_packages=com.example.apmdemo \\\n     -jar target/apm-demo-1.0.0.jar\n</code></pre>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#4-test","title":"4. Test","text":"<pre><code>curl http://localhost:8080/hello\n</code></pre> <p>You should see traces in Kibana APM dashboard under service name <code>apm-demo-app</code>.</p> <p>Your app ran correctly (no errors shown) and Java version is fine (OpenJDK 11).</p> <p>Now open a new terminal and test:</p> <pre><code>curl http://localhost:8080/hello\n</code></pre> <p>If still no response, check:</p> <pre><code>ss -tuln | grep 8080\n</code></pre> <p>to verify if app is listening.</p> <p>If still not accessible, try adding this to your <code>application.properties</code> (create in <code>src/main/resources</code>):</p> <pre><code>server.address=0.0.0.0\nserver.port=8080\n</code></pre> <p>Rebuild and rerun the app. This forces Spring Boot to listen on all interfaces.</p> <p>To confirm your Java app is sending data to Elastic APM:</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#1-check-apm-server-logs","title":"1. Check APM Server logs","text":"<p>Inside the VM running APM Server container, run:</p> <pre><code>sudo docker logs apm-server --tail 50\n</code></pre> <p>Look for lines showing incoming transactions or no connection errors.</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#2-query-apm-server-api-for-received-data-quick-test","title":"2. Query APM Server API for received data (quick test)","text":"<pre><code>curl -s http://localhost:8200/intake/v2/events | jq\n</code></pre> <p>(It usually returns empty unless you POST data, so mainly use logs.)</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#3-check-kibana-apm-dashboard","title":"3. Check Kibana APM Dashboard","text":"<ul> <li> <p>Open Kibana UI: <code>http://localhost:5601</code></p> </li> <li> <p>Go to Observability &gt; APM</p> </li> <li> <p>Look for your service name (e.g., <code>apm-demo-app</code>).</p> </li> <li> <p>You should see transaction traces, errors, and metrics from your app.</p> </li> </ul>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#4-enable-debug-logging-on-apm-agent-optional","title":"4. Enable DEBUG logging on APM agent (optional)","text":"<p>Add to your Java run command for detailed agent logs:</p> <pre><code>-Delastic.apm.log_level=DEBUG\n</code></pre> <p>Example:</p> <pre><code>java -javaagent:/path/to/elastic-apm-agent.jar \\\n     -Delastic.apm.service_name=apm-demo-app \\\n     -Delastic.apm.server_urls=http://localhost:8200 \\\n     -Delastic.apm.application_packages=com.example.apmdemo \\\n     -Delastic.apm.log_level=DEBUG \\\n     -jar target/apm-demo-1.0.0.jar\n</code></pre> <p>Check your app console for APM communication logs.</p>"},{"location":"APM/Docker%20APM%20images%20and%20setup%20steps/#summary","title":"Summary:","text":"<ul> <li> <p>Best way: Use Kibana APM UI to see live traces.</p> </li> <li> <p>Logs: Check APM Server and Java app logs for errors or sent data.</p> </li> <li> <p>Debug mode: Helps troubleshoot agent communication.</p> </li> </ul> <p>Let me know if you want a script to pull recent APM logs or export Kibana dashboards.</p>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/","title":"Docker Compose   setup for Elasticsearch Kibana Elastic APM Server","text":"<p>Use at least 16+ characters, random: <pre><code>openssl rand -base64 24\n# Example: apm-server.secret_token=Kf84sF9vA4vRzJsP8GzJ+0cR\n</code></pre></p>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#docker-composeyml","title":"<code>docker-compose.yml</code>","text":"<pre><code>version: '3.8'\n\nservices:\n\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n      - xpack.monitoring.collection.enabled=true\n      - bootstrap.memory_lock=true\n      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    ports:\n      - \"9200:9200\"\n    volumes:\n      - esdata:/usr/share/elasticsearch/data\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.13.4\n    ports:\n      - \"5601:5601\"\n    environment:\n      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n\n  apm-server:\n    image: docker.elastic.co/apm/apm-server:8.13.4\n    ports:\n      - \"8200:8200\"\n    environment:\n      - output.elasticsearch.hosts=[\"http://elasticsearch:9200\"]\n      - setup.kibana.host=http://kibana:5601\n      - apm-server.secret_token=my_secret_token\n      - apm-server.host=0.0.0.0:8200\n    depends_on:\n      - elasticsearch\n      - kibana\n\nvolumes:\n  esdata:\n</code></pre>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#commands-to-run","title":"\ud83d\udd27 Commands to run","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#after-startup","title":"\ud83d\udccc After startup","text":"<ul> <li>Elasticsearch: http://localhost:9200</li> <li>Kibana: http://localhost:5601</li> <li>APM Server: http://localhost:8200</li> </ul> <p>Use <code>my_secret_token</code> in your APM agent config.</p>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#now-apm-agent-configuration-to-connect-apm-server","title":"Now APM agent configuration to connect APM server","text":"<p>Here are sample APM agent configurations for different applications to connect with your Elastic APM Server at <code>http://localhost:8200</code> using <code>admin123</code> as the secret token:</p>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#1-java-app-spring-boot-any-jvm","title":"\u2705 1. Java App (Spring Boot / Any JVM)","text":"<p>Add this to your <code>JAVA_OPTS</code> or in systemd/launch script:</p> <pre><code>-javaagent:/path/to/elastic-apm-agent.jar \\\n-Delastic.apm.server_url=http://localhost:8200 \\\n-Delastic.apm.secret_token=admin123 \\\n-Delastic.apm.service_name=my-java-app \\\n-Delastic.apm.environment=dev\n</code></pre> <p>\ud83d\udd17 Download agent: https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent</p>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#2-python-app-flask-django-fastapi","title":"\u2705 2. Python App (Flask, Django, FastAPI)","text":"<p>Install agent:</p> <pre><code>mkdir flask-apm-app &amp;&amp; cd flask-apm-app\npython3 -m venv venv\nsource venv/bin/activate\npip install flask elastic-apm\npip install elastic-apm\n</code></pre> <p>Flask Example: Create a file <code>app.py</code>: <pre><code>from flask import Flask\nfrom elasticapm.contrib.flask import ElasticAPM\n\napp = Flask(__name__)\napp.config['ELASTIC_APM'] = {\n  'SERVICE_NAME': 'my-python-app',\n  'SECRET_TOKEN': 'admin123',\n  'SERVER_URL': 'http://localhost:8200',\n  'ENVIRONMENT': 'dev'\n}\napm = ElasticAPM(app)\n\n\n\n---\nfrom flask import Flask\nfrom elasticapm.contrib.flask import ElasticAPM\n\napp = Flask(__name__)\n\n# APM config\napp.config['ELASTIC_APM'] = {\n    'SERVICE_NAME': 'my-python-app',\n    'SECRET_TOKEN': 'eDSEsZcZeUnj5mFMhrCvOv80BdY5HCAY',  # use your token here\n    'SERVER_URL': 'http://localhost:8200',\n    'ENVIRONMENT': 'dev'\n}\n\n# Enable APM\napm = ElasticAPM(app)\n\n# Sample route\n@app.route(\"/\")\ndef hello():\n    return \"Hello from Flask + APM!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n</code></pre></p> <pre><code>python app.py\nThen visit: http://localhost:5000\n</code></pre>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#verify-in-apm-dashboard","title":"Verify in APM Dashboard","text":"<pre><code>Go to http://localhost:5601, then:\n- Observability \u2192 Services\n- You should see `my-python-app` listed.\n</code></pre>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#3-nodejs-app-express-etc","title":"\u2705 3. Node.js App (Express, etc.)","text":"<p>Install agent:</p> <pre><code>npm install elastic-apm-node\n</code></pre> <p>In your app.js:</p> <pre><code>require('elastic-apm-node').start({\n  serviceName: 'my-node-app',\n  secretToken: 'admin123',\n  serverUrl: 'http://localhost:8200',\n  environment: 'dev'\n})\n</code></pre>"},{"location":"APM/Docker%20Compose%20-%20setup%20for%20Elasticsearch-Kibana-Elastic%20APM%20Server/#4-go-app","title":"\u2705 4. Go App","text":"<p>Install:</p> <pre><code>go get go.elastic.co/apm\n</code></pre> <p>In code:</p> <pre><code>import \"go.elastic.co/apm/module/apmhttp\"\n\n// wrap your handler\nhttp.Handle(\"/\", apmhttp.Wrap(http.HandlerFunc(myHandler)))\n</code></pre> <p>Set env vars:</p> <pre><code>export ELASTIC_APM_SERVER_URL=http://localhost:8200\nexport ELASTIC_APM_SECRET_TOKEN=admin123\nexport ELASTIC_APM_SERVICE_NAME=my-go-app\nexport ELASTIC_APM_ENVIRONMENT=dev\n</code></pre> <p>Let me know which language you're targeting \u2014 I can give complete working demo code.</p>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/","title":"ELK Stack   APM Setup in Multipass VM","text":"<p>I'll help you set up Elasticsearch, Kibana, and APM Server in a Multipass VM. This is a comprehensive setup that requires several steps.This guide will help you set up a complete ELK stack with APM monitoring in a Multipass VM. Here are some key points to keep in mind:</p> <p>Resource Requirements:</p> <ul> <li>The VM is configured with 4 CPUs, 8GB RAM, and 20GB disk space, which should be sufficient for development</li> <li>Elasticsearch uses 2GB of heap memory by default - adjust if needed</li> </ul> <p>Important Notes:</p> <ol> <li>Security is disabled in this setup for easier development - enable it for production use</li> <li>Single-node configuration - suitable for development but not for production clusters</li> <li>Network access is configured to allow connections from your host machine</li> </ol> <p>Quick Start Sequence:</p> <ol> <li>Create the VM and SSH into it</li> <li>Run the installation commands in order</li> <li>Wait for all services to start (Elasticsearch takes the longest)</li> <li>Access Kibana at <code>http://VM_IP:5601</code> to verify everything is working</li> </ol> <p>Common Issues:</p> <ul> <li>If services fail to start, check the logs using the journalctl commands provided</li> <li>Memory issues can be resolved by increasing VM memory or reducing JVM heap size</li> <li>Network connectivity issues usually resolve by using the correct VM IP address</li> </ul>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-1-create-and-configure-multipass-vm","title":"Step 1: Create and Configure Multipass VM","text":"<pre><code># Create a VM with sufficient resources\nmultipass launch --name elk-stack --cpus 4 --memory 8G --disk 20G\n\n# Get shell access\nmultipass shell elk-stack\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-2-update-system-and-install-prerequisites","title":"Step 2: Update System and Install Prerequisites","text":"<pre><code># Update package list\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install Java (required for Elasticsearch)\nsudo apt install openjdk-11-jdk -y\n\n# Verify Java installation\njava -version\n\n# Install curl, wget, and gnupg\nsudo apt install curl wget gnupg2 -y\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-3-add-elastic-repository","title":"Step 3: Add Elastic Repository","text":"<pre><code># Import Elastic GPG key\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\n\n# Add Elastic repository\necho \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n\n# Update package list\nsudo apt update\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-4-install-elasticsearch","title":"Step 4: Install Elasticsearch","text":"<pre><code># Install Elasticsearch\nsudo apt install elasticsearch -y\n\n# Configure Elasticsearch\nsudo vi /etc/elasticsearch/elasticsearch.yml\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#elasticsearch-configuration-etcelasticsearchelasticsearchyml","title":"Elasticsearch Configuration (<code>/etc/elasticsearch/elasticsearch.yml</code>)","text":"<pre><code># Cluster name\ncluster.name: my-application\n\n# Node name\nnode.name: node-1\n\n# Network settings\nnetwork.host: 0.0.0.0\nhttp.port: 9200\n\n# Discovery settings for single node\ndiscovery.type: single-node\n\n# Security settings (disable for development)\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: false\nxpack.security.http.ssl.enabled: false\nxpack.security.transport.ssl.enabled: false\n\n# Memory settings\nbootstrap.memory_lock: true\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#configure-jvm-heap-size","title":"Configure JVM Heap Size","text":"<pre><code># Edit JVM options\nsudo vi /etc/elasticsearch/jvm.options.d/heap.options\n</code></pre> <p>Add the following content:</p> <pre><code>-Xms2g\n-Xmx2g\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code># Enable and start Elasticsearch\nsudo systemctl daemon-reload\nsudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n\n# Check status\nsudo systemctl status elasticsearch\n\n# Test Elasticsearch\ncurl -X GET \"localhost:9200/\"\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-5-install-kibana","title":"Step 5: Install Kibana","text":"<pre><code># Install Kibana\nsudo apt install kibana -y\n\n# Configure Kibana\nsudo nano /etc/kibana/kibana.yml\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#kibana-configuration-etckibanakibanayml","title":"Kibana Configuration (<code>/etc/kibana/kibana.yml</code>)","text":"<pre><code># Server settings\nserver.port: 5601\nserver.host: \"0.0.0.0\"\nserver.name: \"kibana-server\"\n\n# Elasticsearch settings\nelasticsearch.hosts: [\"http://localhost:9200\"]\n\n# Disable security for development\nxpack.security.enabled: false\nxpack.encryptedSavedObjects.encryptionKey: \"something_at_least_32_characters_long\"\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#start-kibana","title":"Start Kibana","text":"<pre><code># Enable and start Kibana\nsudo systemctl enable kibana\nsudo systemctl start kibana\n\n# Check status\nsudo systemctl status kibana\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-6-install-apm-server","title":"Step 6: Install APM Server","text":"<pre><code># Install APM Server\nsudo apt install apm-server -y\n\n# Configure APM Server\nsudo nano /etc/apm-server/apm-server.yml\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#apm-server-configuration-etcapm-serverapm-serveryml","title":"APM Server Configuration (<code>/etc/apm-server/apm-server.yml</code>)","text":"<pre><code># APM Server settings\napm-server:\n  host: \"0.0.0.0:8200\"\n  rum:\n    enabled: true\n    allow_origins: ['*']\n\n# Output to Elasticsearch\noutput.elasticsearch:\n  hosts: [\"localhost:9200\"]\n\n# Kibana settings\nsetup.kibana:\n  host: \"localhost:5601\"\n\n# Disable security\napm-server.auth.anonymous.enabled: true\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#setup-apm-server","title":"Setup APM Server","text":"<pre><code># Setup APM Server dashboards and templates\nsudo apm-server setup -e\n\n# Enable and start APM Server\nsudo systemctl enable apm-server\nsudo systemctl start apm-server\n\n# Check status\nsudo systemctl status apm-server\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-7-configure-vm-network-access","title":"Step 7: Configure VM Network Access","text":"<pre><code># Get VM IP address\nip addr show\n\n# Note the IP address (usually 192.168.x.x)\n</code></pre> <p>From your host machine:</p> <pre><code># Get VM info\nmultipass info elk-stack\n\n# The VM IP will be shown - use this to access services\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-8-access-services","title":"Step 8: Access Services","text":"<p>Open your web browser and navigate to:</p> <ul> <li>Kibana: <code>http://VM_IP:5601</code></li> <li>Elasticsearch: <code>http://VM_IP:9200</code></li> <li>APM Server: <code>http://VM_IP:8200</code> (for agent connections)</li> </ul>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-9-verify-installation","title":"Step 9: Verify Installation","text":""},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#test-elasticsearch","title":"Test Elasticsearch","text":"<pre><code>curl -X GET \"VM_IP:9200/_cluster/health?pretty\"\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#test-apm-server","title":"Test APM Server","text":"<pre><code>curl -X GET \"VM_IP:8200/\"\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#access-kibana","title":"Access Kibana","text":"<ol> <li>Open <code>http://VM_IP:5601</code> in your browser</li> <li>Go to \"APM\" section to verify APM integration</li> <li>Check \"Stack Management\" &gt; \"Index Management\" for APM indices</li> </ol>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#step-10-basic-apm-usage","title":"Step 10: Basic APM Usage","text":""},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#install-apm-agent-example-for-nodejs","title":"Install APM Agent (Example for Node.js)","text":"<pre><code># In your application directory\nnpm install elastic-apm-node --save\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#configure-apm-agent","title":"Configure APM Agent","text":"<pre><code>// At the top of your main application file\nconst apm = require('elastic-apm-node').start({\n  serviceName: 'my-service',\n  serverUrl: 'http://VM_IP:8200',\n  environment: 'development'\n})\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#troubleshooting","title":"Troubleshooting","text":""},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#check-logs","title":"Check Logs","text":"<pre><code># Elasticsearch logs\nsudo journalctl -u elasticsearch -f\n\n# Kibana logs\nsudo journalctl -u kibana -f\n\n# APM Server logs\nsudo journalctl -u apm-server -f\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#restart-services","title":"Restart Services","text":"<pre><code>sudo systemctl restart elasticsearch\nsudo systemctl restart kibana\nsudo systemctl restart apm-server\n</code></pre>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#memory-issues","title":"Memory Issues","text":"<p>If you encounter memory issues, you can:</p> <ol> <li>Increase VM memory: <code>multipass set local.elk-stack.memory=12G</code></li> <li>Reduce JVM heap size in <code>/etc/elasticsearch/jvm.options.d/heap.options</code></li> </ol>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#security-note","title":"Security Note","text":"<p>This setup disables security features for development purposes. For production use, enable security features and configure proper authentication and encryption.</p>"},{"location":"APM/ELK%20Stack%20-%20APM%20Setup%20in%20Multipass%20VM/#port-summary","title":"Port Summary","text":"<ul> <li>Elasticsearch: 9200</li> <li>Kibana: 5601</li> <li>APM Server: 8200</li> </ul> <p>Make sure these ports are accessible from your host machine to the VM.</p>"},{"location":"APM/ELK%20installation%20and%20setup/","title":"ELK installation and setup","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"APM/ELK%20installation%20and%20setup/#elk-running-ports","title":"ELK running ports","text":"<pre><code>kibana : port 5601\nElasticsearch: (port 9200)\n</code></pre>"},{"location":"APM/ELK%20installation%20and%20setup/#kibana-configuration","title":"kibana configuration","text":"<p>Open the Kibana configuration file for editing. <pre><code>sudo nano /etc/kibana/kibana.yml\n</code></pre></p> <p>Uncomment and adjust the following lines to bind Kibana to all IP addresses and connect it to Elasticsearch. <pre><code>#server.port: 5601\nserver.host: \"0.0.0.0\"\nelasticsearch.hosts: [\"http://localhost:9200\"]\n\n$ sudo systemctl restart kibana\n</code></pre></p> <pre><code>ubuntu@elk-vm:~$ sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana\neyJ2ZXIiOiI4LjE0LjAiLCJhZHIiOlsiMTAuOTQuMjI2LjE0OTo5MjAwIl0sImZnciI6IjllMDM3M2VjMzBhMzFkYmE4ZDUzODdjNzc3ZjQ4ZTA4NWFkNDkxMzk0MTNiNWYzMDUwYTAyMGY4NjgyODRiMTkiLCJrZXkiOiJrVUY0dXBjQnk1eElqNS1OSzVvMjpRN0pTNWNOazA3TWJtMEdad0RjUkxRIn0=\n</code></pre>"},{"location":"APM/ELK%20installation%20and%20setup/#kibana-token","title":"kibana token","text":"<pre><code>ubuntu@elk-vm:~$ sudo /usr/share/kibana/bin/kibana-verification-code\nYour verification code is:  080 708\n</code></pre>"},{"location":"APM/ELK%20installation%20and%20setup/#default-superuser-login-elasticsearch-kibana-8x","title":"\u2705 Default Superuser Login (Elasticsearch / Kibana 8.x+)","text":"<p>When Elasticsearch was first started, it generated a password for the <code>elastic</code> user.</p>"},{"location":"APM/ELK%20installation%20and%20setup/#to-retrieve-it-on-the-elasticsearch-vm","title":"\ud83d\udccd To retrieve it (on the Elasticsearch VM):","text":"<pre><code># check the elasticsearch service\nsudo systemctl status elasticsearch\nsudo systemctl start elasticsearch --no-page\n\nsudo cat /etc/elasticsearch/elasticsearch.keystore | grep elastic\nBut more reliably, if you missed the initial message, reset the password:\n</code></pre> <pre><code>ubuntu@elk-vm:~$ sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic\nThis tool will reset the password of the [elastic] user to an autogenerated value.\nThe password will be printed in the console.\nPlease confirm that you would like to continue [y/N]y\n\n\nPassword for the [elastic] user successfully reset.\nNew value: ZQhiI9Ppr9Z8pTibJ7hJ\nubuntu@elk-vm:~$ sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -i\nThis tool will reset the password of the [elastic] user.\nYou will be prompted to enter the password.\nPlease confirm that you would like to continue [y/N]y\n\n\nEnter password for [elastic]:\nRe-enter password for [elastic]:\nPassword for the [elastic] user successfully reset.\n</code></pre>"},{"location":"APM/ELK%20installation%20and%20setup/#then-login-to-kibana-with","title":"Then login to Kibana with:","text":"<ul> <li>Username: <code>elastic</code> </li> <li>Password: the new password you just reset</li> </ul> <p>Reset the <code>elastic</code> user password: <pre><code>sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic\nThis will prompt you to enter a new password (or generate one for you).\n</code></pre></p> <p>Edit Elasticsearch config: if you are not getting the json success output  curl http://10.94.226.149:9200 then check and do the configurations for elasticsearch <pre><code>sudo nano /etc/elasticsearch/elasticsearch.yml\nnetwork.host: 0.0.0.0\nhttp.port: 9200\nsudo systemctl restart elasticsearch\n\ncurl http://10.94.226.149:9200\n</code></pre></p> <p>Modify Elasticsearch configuration for remote access.</p> <pre><code>sudo nano /etc/elasticsearch/elasticsearch.yml\n\nset the below values as \n\nnetwork.host: 0.0.0.0\ndiscovery.seed_hosts: []\n</code></pre> <p>For a basic setup (not recommended for production), disable security features.</p> <pre><code>xpack.security.enabled: false\n</code></pre> <pre><code>sudo systemctl restart elasticsearch\n</code></pre> <pre><code>ubuntu@elk-vm:~$ curl -X GET \"localhost:9200\"\n{\n  \"name\" : \"elk-vm\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"t5b5MBJiTCS5E68E6Su5Ww\",\n  \"version\" : {\n    \"number\" : \"8.18.3\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"deb\",\n    \"build_hash\" : \"28fc77664903e7de48ba5632e5d8bfeb5e3ed39c\",\n    \"build_date\" : \"2025-06-18T22:08:41.171261054Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.12.1\",\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre> <pre><code>ubuntu@elk-vm:~$ curl http://localhost:9200\n{\n  \"name\" : \"elk-vm\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"t5b5MBJiTCS5E68E6Su5Ww\",\n  \"version\" : {\n    \"number\" : \"8.18.3\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"deb\",\n    \"build_hash\" : \"28fc77664903e7de48ba5632e5d8bfeb5e3ed39c\",\n    \"build_date\" : \"2025-06-18T22:08:41.171261054Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.12.1\",\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/","title":"Setup ELK with filebeat","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#best-practice-setup-4-vms","title":"Best Practice Setup (4 VMs):","text":"<ol> <li><code>elk-es</code> \u2013 4 CPU, 4\u20138 GB RAM, 20+ GB Disk</li> <li><code>elk-kibana</code> \u2013 2 CPU, 2 GB RAM</li> <li><code>elk-logstash</code> \u2013 2\u20134 CPU, 4 GB RAM</li> <li><code>elk-agent-app1</code> \u2013 Filebeat (with your app) \u2013 1 CPU, 1 GB RAM</li> </ol>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#step-1-create-vms-with-resources","title":"\ud83d\udd27 Step 1: Create VMs with resources","text":"<pre><code># Create Elasticsearch VM\nmultipass launch --name elk-es --cpus 4 --mem 8G --disk 20G\n\n# Create Kibana VM\nmultipass launch --name elk-kibana --cpus 2 --mem 2G --disk 10G\n\n# Create Logstash VM\nmultipass launch --name elk-logstash --cpus 4 --mem 4G --disk 15G\n\n# Create Filebeat Agent VM (can simulate app logs)\nmultipass launch --name elk-agent-app1 --cpus 1 --mem 1G --disk 5G\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#step-2-install-elasticsearch-on-elk-es","title":"\ud83d\udd27 Step 2: Install Elasticsearch on <code>elk-es</code>","text":"<pre><code>multipass exec elk-es -- bash -c '\nsudo apt update\nsudo apt install -y wget apt-transport-https openjdk-17-jdk\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\nsudo apt update\nsudo apt install -y elasticsearch\nsudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n'\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#step-3-install-kibana-on-elk-kibana","title":"\ud83d\udd27 Step 3: Install Kibana on <code>elk-kibana</code>","text":"<pre><code>multipass exec elk-kibana -- bash -c '\nsudo apt update\nsudo apt install -y wget apt-transport-https openjdk-17-jdk\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\nsudo apt update\nsudo apt install -y kibana\nsudo systemctl enable kibana\nsudo systemctl start kibana\n'\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#step-4-install-logstash-on-elk-logstash","title":"\ud83d\udd27 Step 4: Install Logstash on <code>elk-logstash</code>","text":"<pre><code>multipass exec elk-logstash -- bash -c '\nsudo apt update\nsudo apt install -y wget apt-transport-https openjdk-17-jdk\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\nsudo apt update\nsudo apt install -y logstash\nsudo systemctl enable logstash\nsudo systemctl start logstash\n'\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#step-5-install-filebeat-on-elk-agent-app1","title":"\ud83d\udd27 Step 5: Install Filebeat on <code>elk-agent-app1</code>","text":"<pre><code>multipass exec elk-agent-app1 -- bash -c '\nsudo apt update\nsudo apt install -y wget apt-transport-https\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\nsudo apt update\nsudo apt install -y filebeat\nsudo systemctl enable filebeat\nsudo systemctl start filebeat\n'\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#result","title":"\u2705 Result","text":"VM Name Component Status <code>elk-es</code> Elasticsearch \u2705 Installed <code>elk-kibana</code> Kibana \u2705 Installed <code>elk-logstash</code> Logstash \u2705 Installed <code>elk-agent-app1</code> Filebeat \u2705 Installed <p>Let me know if you want config files for <code>logstash</code>, <code>filebeat</code>, or port forwarding/Nginx setup.</p>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#below-is-the-single-automated-bash-script-to","title":"Below is the single automated Bash script to:","text":"<p>\u2705 Create 4 VMs using Multipass \u2705 Install Elasticsearch, Kibana, Logstash, Filebeat \u2705 Set required resources for each \u2705 Enable and start all services</p>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#setup-elksh","title":"\u2705 <code>setup-elk.sh</code>","text":"<pre><code>#!/bin/bash\n\n# Create VMs\necho \"Creating VMs...\"\nmultipass launch --name elk-es --cpus 4 --mem 8G --disk 20G\nmultipass launch --name elk-kibana --cpus 2 --mem 2G --disk 10G\nmultipass launch --name elk-logstash --cpus 4 --mem 4G --disk 15G\nmultipass launch --name elk-agent-app1 --cpus 1 --mem 1G --disk 5G\n\n# Common Elastic APT setup\nAPT_SETUP=$(cat &lt;&lt;'EOF'\nsudo apt update\nsudo apt install -y wget apt-transport-https gnupg openjdk-17-jdk\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\nsudo apt update\nEOF\n)\n\n# Install Elasticsearch\necho \"Installing Elasticsearch on elk-es...\"\nmultipass exec elk-es -- bash -c \"$APT_SETUP &amp;&amp; sudo apt install -y elasticsearch &amp;&amp; sudo systemctl enable --now elasticsearch\"\n\n# Install Kibana\necho \"Installing Kibana on elk-kibana...\"\nmultipass exec elk-kibana -- bash -c \"$APT_SETUP &amp;&amp; sudo apt install -y kibana &amp;&amp; sudo systemctl enable --now kibana\"\n\n# Install Logstash\necho \"Installing Logstash on elk-logstash...\"\nmultipass exec elk-logstash -- bash -c \"$APT_SETUP &amp;&amp; sudo apt install -y logstash &amp;&amp; sudo systemctl enable --now logstash\"\n\n# Install Filebeat\necho \"Installing Filebeat on elk-agent-app1...\"\nmultipass exec elk-agent-app1 -- bash -c \"$APT_SETUP &amp;&amp; sudo apt install -y filebeat &amp;&amp; sudo systemctl enable --now filebeat\"\n\necho -e \"\\n\u2705 ELK Stack setup completed on Multipass VMs!\"\nmultipass list\n</code></pre>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#usage","title":"\ud83d\udccc Usage","text":"<ol> <li>Save as <code>setup-elk.sh</code></li> <li>Run:</li> </ol> <pre><code>chmod +x setup-elk.sh\n./setup-elk.sh\n</code></pre> <p>Let me know if you want:</p> <ul> <li>Config files (<code>filebeat.yml</code>, <code>logstash.conf</code>)</li> <li>External access via port forwarding or Nginx</li> <li>Sample logs to ingest</li> </ul>"},{"location":"APM/Setup%20ELK%20with%20filebeat/#nginx-to-configure","title":"nginx to configure","text":"<pre><code>sudo nginx -t &amp;&amp; sudo systemctl reload nginx\n</code></pre>"},{"location":"APM/Steps%20to%20parse%20custom%20logs%20with%20Filebeat/","title":"Steps to parse custom logs with Filebeat","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>For custom logs, you don\u2019t use a module \u2014 instead, you configure a custom Filebeat input and optionally use Elasticsearch ingest pipelines to parse the logs.</p>"},{"location":"APM/Steps%20to%20parse%20custom%20logs%20with%20Filebeat/#steps-to-parse-custom-logs-with-filebeat","title":"\u2705 Steps to parse custom logs with Filebeat:","text":"<ol> <li>Edit Filebeat config:</li> </ol> <pre><code>sudo vi /etc/filebeat/filebeat.yml\n\n\n# ======================= Filebeat inputs ===============================\n\nfilebeat.inputs:\n\n# Each - is an input. Most options can be set at the input level, so\n# you can use different inputs for various configurations.\n# Below are the input-specific configurations.\n\n# filestream is an input for collecting log messages from files.\n- type: filestream\n\n  # Unique ID among all inputs, an ID is required.\n  id: my-filestream-id\n\n  # Change to true to enable this input configuration.\n  enabled: false\n\n  # Paths that should be crawled and fetched. Glob based paths.\n  paths:\n    - /var/log/*.log\n\n\n# ---------------------------- Elasticsearch Output ----------------------------\noutput.elasticsearch:\n  # Array of hosts to connect to.\n  hosts: [\"http://localhost:9200\"]\n\n  # Performance preset - one of \"balanced\", \"throughput\", \"scale\",\n  # \"latency\", or \"custom\".\n  preset: balanced\n\n  # Protocol - either `http` (default) or `https`.\n  #protocol: \"https\"\n\n  # Authentication credentials - either API key or username/password.\n  #api_key: \"id:api_key\"\n  username: \"elastic\"\n  password: \"admin123\"\n</code></pre> <ol> <li>Add a custom input:</li> </ol> <pre><code>filebeat.inputs:\n  - type: log\n    enabled: true\n    paths:\n      - /var/log/myapp/*.log    # Change to your log path\n    fields:\n      log_type: custom          # Optional tag\n    multiline.pattern: '^\\['    # If logs span multiple lines, example: starts with [\n    multiline.negate: true\n    multiline.match: after\n</code></pre> <ol> <li>(Optional) Define parsing via Elasticsearch pipeline or Logstash (if used)</li> </ol> <p>If logs are in custom format, you can create a pipeline in Elasticsearch and apply it using:</p> <pre><code>output.elasticsearch:\n  hosts: [\"http://&lt;elk-ip&gt;:9200\"]\n  pipeline: \"my-custom-pipeline\"\n</code></pre>"},{"location":"APM/Steps%20to%20parse%20custom%20logs%20with%20Filebeat/#optional-test-config-and-start-filebeat","title":"\ud83d\udd0d Optional: Test config and start filebeat","text":"<pre><code>sudo filebeat test config\nsudo systemctl restart filebeat\n</code></pre> <p>Let me know if you want help writing a custom grok pattern or pipeline for your log format (just paste a few sample log lines).</p>"},{"location":"APM/Why%20you%20need%20Filebeat%20and%20Logstash%20and%20when%20to%20use%20which/","title":"Why you need Filebeat and Logstash and when to use which","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here's a simple breakdown of why you need Filebeat and Logstash, and when to use which</p> Tool Purpose When to Use Pros Filebeat Lightweight log shipper (agent) On app VMs to forward logs Low CPU/RAM, reliable, easy to use Logstash Centralized log processor &amp; parser On central VM to parse/enrich Handles complex parsing, filtering"},{"location":"APM/Why%20you%20need%20Filebeat%20and%20Logstash%20and%20when%20to%20use%20which/#why-filebeat","title":"\ud83d\udd39 Why Filebeat?","text":"<ul> <li>Reads logs from files (<code>/var/log/*.log</code>, app logs).</li> <li>Ships logs to Elasticsearch or Logstash.</li> <li>Lightweight, low memory usage.</li> <li>Ideal for deployment on many machines.</li> </ul> <p>\u2705 Use Filebeat where logs are generated (e.g., app servers, web servers).</p>"},{"location":"APM/Why%20you%20need%20Filebeat%20and%20Logstash%20and%20when%20to%20use%20which/#why-logstash","title":"\ud83d\udd39 Why Logstash?","text":"<ul> <li>Used for parsing, transforming, and enriching logs.</li> <li>Can handle complex data (JSON, multiline, grok patterns).</li> <li>Used to preprocess logs before sending to Elasticsearch.</li> </ul> <p>\u2705 Use Logstash centrally, when logs need parsing before indexing.</p>"},{"location":"APM/Why%20you%20need%20Filebeat%20and%20Logstash%20and%20when%20to%20use%20which/#summary-use-case","title":"\ud83d\udd27 Summary Use-Case:","text":"<pre><code>[App Server] \u2192 Filebeat \u2192 [Logstash] \u2192 [Elasticsearch] \u2192 [Kibana]\n</code></pre> <p>Or if logs are already clean:</p> <pre><code>[App Server] \u2192 Filebeat \u2192 [Elasticsearch] \u2192 [Kibana]\n</code></pre> <p>Let me know your use case (plain logs or complex?) and I\u2019ll suggest the best combo.</p>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/","title":"how to configure Alertmanager on K3d","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#1-install-k3d-cluster-if-not-already","title":"\u2705 1. Install K3d cluster (if not already)","text":"<pre><code>k3d cluster create alert-cluster --agents 2\n</code></pre>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#2-install-prometheus-alertmanager-with-helm","title":"\u2705 2. Install Prometheus &amp; Alertmanager with Helm","text":"<pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring --create-namespace\n</code></pre> <p>This deploys:</p> <ul> <li>Prometheus</li> <li>Alertmanager</li> <li>Grafana</li> <li>Node Exporters</li> </ul>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#3-edit-alertmanager-config","title":"\u2705 3. Edit Alertmanager config","text":"<p>Create a file <code>alertmanager.yaml</code> with Gmail SMTP:</p> <pre><code>global:\n  smtp_smarthost: 'smtp.gmail.com:587'\n  smtp_from: 'your.email@gmail.com'\n  smtp_auth_username: 'your.email@gmail.com'\n  smtp_auth_password: 'your_app_password'\n  smtp_require_tls: true\n\nroute:\n  group_by: ['alertname']\n  receiver: 'gmail-alert'\n\nreceivers:\n  - name: 'gmail-alert'\n    email_configs:\n      - to: 'destination@email.com'\n        send_resolved: true\n</code></pre>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#4-create-a-kubernetes-secret-for-alertmanager-config","title":"\u2705 4. Create a Kubernetes Secret for Alertmanager config","text":"<pre><code>kubectl -n monitoring create secret generic alertmanager-prometheus-alertmanager \\\n  --from-file=alertmanager.yaml\n</code></pre>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#5-restart-alertmanager-pod","title":"\u2705 5. Restart Alertmanager pod","text":"<pre><code>kubectl -n monitoring delete pod -l app.kubernetes.io/name=alertmanager\n</code></pre>"},{"location":"Alertmanager/how%20to%20configure%20Alertmanager%20on%20K3d/#6-verify-alertmanager-is-using-the-config","title":"\u2705 6. Verify Alertmanager is using the config","text":"<p>Check via:</p> <pre><code>kubectl -n monitoring port-forward svc/prometheus-kube-prometheus-alertmanager 9093\n</code></pre> <p>Then open: http://localhost:9093</p> <p>if you want, trigger test alerts also to verify emails.</p>"},{"location":"ArgoCD/ArgoCD%20Architecture/","title":"ArgoCD Architecture","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>![[Argocd-Archtiecture.png]]</p>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/","title":"ArgoCD installation steps","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#ingress-access","title":"ingress access","text":"<pre><code># argocd-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-ingress\n  namespace: argocd\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: argocd.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:\n                  number: 80\n</code></pre> <pre><code>k get ingress -n argocd\n</code></pre> <p>update /etc/hosts file <pre><code>sudo sh -c \"echo '&lt;multipass-any-node-ip&gt; argocd.local' &gt;&gt; /etc/hosts\"\n</code></pre></p> <p>deployment patch for <code>--insecure</code> access for argocd url http://argocd.local <pre><code>kubectl -n argocd patch deployment argocd-server \\\n  --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/args\", \"value\": [\"/usr/local/bin/argocd-server\", \"--insecure\"]}]'\n</code></pre></p>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#get-admin-password","title":"get admin password","text":"<pre><code>$ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d &amp;&amp; echo\n\n$ FNLoqZlqBZry3Gcc\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#install-argo-cd-cli-linux","title":"\u2705 Install Argo CD CLI (Linux)","text":"<pre><code>VERSION=$(curl -s https://api.github.com/repos/argoproj/argo-cd/releases/latest | grep tag_name | cut -d '\"' -f 4)\n\ncurl -sSL -o argocd \"https://github.com/argoproj/argo-cd/releases/download/${VERSION}/argocd-linux-amd64\"\n\nchmod +x argocd\nsudo mv argocd /usr/local/bin/\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#verify","title":"\ud83d\udd0d Verify","text":"<pre><code>argocd version\n</code></pre> <p>Should output CLI and server versions (CLI only if not logged in yet).</p>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#login-to-argo-cd","title":"\u2705 Login to Argo CD","text":"<pre><code>$ argocd login argocd.local --username admin --password &lt;current-password&gt; --insecure\n\n$ argocd login argocd.local --username admin --password FNLoqZlqBZry3Gcc --insecure\n\n\ngouse@gouse:~/DevOps/multipass_scripts$ argocd login argocd.local --username admin --password FNLoqZlqBZry3Gcc --insecure\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-06-30T22:28:33+03:00\"}\n'admin:login' logged in successfully\nContext 'argocd.local' updated\n</code></pre> <p><code>--insecure</code> is required if you're using plain HTTP or self-signed TLS.</p>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#change-password","title":"\ud83d\udd10 Change Password","text":"<pre><code>argocd account update-password --current-password &lt;current-password&gt; --new-password &lt;new-password&gt;\n\nargocd account update-password --current-password FNLoqZlqBZry3Gcc --new-password admin123\n\ngouse@gouse:~/DevOps/multipass_scripts$ argocd account update-password --current-password FNLoqZlqBZry3Gcc --new-password admin123\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-06-30T22:29:33+03:00\"}\nPassword updated\nContext 'argocd.local' updated\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20installation%20steps/#creating-apps-via-cli","title":"Creating Apps Via CLI[","text":"<p>First we need to set the current namespace to argocd running the following command: <pre><code>kubectl config set-context --current --namespace=argocd\n</code></pre></p> <p>Create the example guestbook application with the following command: <pre><code>argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default\n</code></pre></p> <pre><code>$ argocd app sync guestbook\n$ argocd app get guestbook\ngouse@gouse:~/DevOps/multipass_scripts$ argocd app get guestbook\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-06-30T22:47:38+03:00\"}\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-06-30T22:47:38+03:00\"}\nName:               argocd/guestbook\nProject:            default\nServer:             https://kubernetes.default.svc\nNamespace:          default\nURL:                https://argocd.local/applications/guestbook\nSource:\n- Repo:             http://192.168.100.5:3000/gouse/argocd-example-apps.git\n  Target:           HEAD\n  Path:             guestbook\nSyncWindow:         Sync Allowed\nSync Policy:        Manual\nSync Status:        Synced to HEAD (f58c7ed)\nHealth Status:      Healthy\n\nGROUP  KIND        NAMESPACE  NAME          STATUS  HEALTH   HOOK  MESSAGE\n       Service     default    guestbook-ui  Synced  Healthy        service/guestbook-ui created\napps   Deployment  default    guestbook-ui  Synced  Healthy        deployment.apps/guestbook-ui created\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/","title":"ArgoCD with HTTPS using cert manager","text":"<p>Created: 2025-06-30 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Great! Let\u2019s configure ArgoCD with HTTPS using <code>cert-manager</code> for automatic TLS certificates (via Let\u2019s Encrypt) in k3s (Traefik).  </p>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-1-install-cert-manager","title":"Step 1: Install Cert-Manager","text":"<p><pre><code># Install cert-manager (adjust version if needed)\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml\n</code></pre> Verify installation: <pre><code>kubectl get pods -n cert-manager\n\ngouse@gouse:~$ kubectl get pods -n cert-manager -w\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-776494b6cf-xb6dc              1/1     Running   0          20s\ncert-manager-cainjector-6cf76fc759-9267r   1/1     Running   0          20s\ncert-manager-webhook-7bfbfdc97c-g4btd      1/1     Running   0          19s\n</code></pre></p>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-2-configure-lets-encrypt-issuer","title":"Step 2: Configure Let\u2019s Encrypt Issuer","text":"<p>Create a <code>ClusterIssuer</code> for Let\u2019s Encrypt (replace <code>your-email@example.com</code>): File: <code>letsencrypt-issuer.yaml</code></p> <pre><code>gouse@gouse:~$ k api-resources | grep ClusterIssuer\nclusterissuers      cert-manager.io/v1    false     ClusterIssuer\n</code></pre> <p><pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: your-email@example.com\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod-account-key\n    solvers:\n    - http01:\n        ingress:\n          class: traefik  # k3s uses Traefik by default\n</code></pre> Apply: <pre><code>kubectl apply -f letsencrypt-issuer.yaml\n\ngouse@gouse:~/DevOps/multipass_scripts/argocd$ kubectl apply -f letsencrypt-issuer.yaml\nclusterissuer.cert-manager.io/letsencrypt-prod created\n\ngouse@gouse:~/DevOps/multipass_scripts/argocd$ k get clusterissuer.cert-manager.io/letsencrypt-prod\nNAME               READY   AGE\nletsencrypt-prod   False   23s\n</code></pre></p>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-3-deploy-argocd-skip-if-already-done","title":"Step 3: Deploy ArgoCD (Skip if Already Done)","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-4-create-ingress-with-tls-automation","title":"Step 4: Create Ingress with TLS Automation","text":"<p>File: <code>argocd-ingress-tls.yaml</code> Replace <code>argocd.yourdomain.com</code> with your actual domain (must resolve to your k3s IP). <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-ingress\n  namespace: argocd\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod  # Refers to ClusterIssuer\n    traefik.ingress.kubernetes.io/router.entrypoints: websecure\n    traefik.ingress.kubernetes.io/router.tls: \"true\"\nspec:\n  tls:\n  - hosts:\n    - argocd.local.com   # Your domain here\n    secretName: argocd-tls    # cert-manager stores cert here\n  rules:\n  - host: argocd.local.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              number: 443     # ArgoCD's HTTPS port\n</code></pre> Apply: <pre><code>kubectl apply -f argocd-ingress-tls.yaml\n\ngouse@gouse:~/DevOps/multipass_scripts/argocd$ kubectl apply -f argocd-ingress-tls.yaml\ningress.networking.k8s.io/argocd-ingress created\n\ngouse@gouse:~/DevOps/multipass_scripts/argocd$ k get ingress.networking.k8s.io/argocd-ingress -n argocd\nNAME             CLASS     HOSTS              ADDRESS                                                  PORTS     AGE\nargocd-ingress   traefik   argocd.local.com   10.94.226.136,10.94.226.145,10.94.226.164,10.94.226.18   80, 443   32s\n</code></pre></p>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-5-verify-tls-certificate","title":"Step 5: Verify TLS Certificate","text":"<p>Check certificate status: <pre><code>kubectl get certificate -n argocd\n\ngouse@gouse:~/DevOps/multipass_scripts/argocd$ kubectl get certificate -n argocd\nNAME         READY   SECRET       AGE\nargocd-tls   False   argocd-tls   65s\n</code></pre> Once ready (status <code>True</code>), access ArgoCD securely: <pre><code>https://argocd.yourdomain.com\n\nhttps://argocd.local.com\n</code></pre></p>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#step-6-get-admin-password","title":"Step 6: Get Admin Password","text":"<pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#key-notes","title":"Key Notes","text":"<ol> <li>Domain Requirements:  </li> <li>Your domain (<code>argocd.yourdomain.com</code>) must point to the public IP of your k3s cluster.  </li> <li> <p>For local testing, use a DNS service like <code>nip.io</code> (e.g., <code>argocd.192-168-1-100.nip.io</code>).  </p> </li> <li> <p>Traefik + cert-manager:  </p> </li> <li>cert-manager automatically creates a challenge (<code>HTTP01</code>) to verify domain ownership.  </li> <li> <p>Traefik routes the challenge to cert-manager for validation.  </p> </li> <li> <p>Production Security:  </p> </li> <li>Use <code>letsencrypt-prod</code> (not staging) for trusted certs.  </li> <li>Ensure ArgoCD\u2019s admin password is rotated after setup.  </li> </ol>"},{"location":"ArgoCD/ArgoCD%20with%20HTTPS%20using%20cert-manager/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Certificate not issued?   Check challenges: <pre><code>kubectl describe challenges -n argocd\n</code></pre></li> <li>Traefik not routing?   Check logs: <pre><code>kubectl logs -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre></li> </ul> <p>Let me know if you need help debugging! \ud83d\udee0\ufe0f</p>"},{"location":"ArgoCD/Argocd%20CLI/","title":"Argocd CLI","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"ArgoCD/Argocd%20CLI/#1-authentication","title":"\ud83d\udd10 1. Authentication","text":"Intent Command Login to ArgoCD <code>argocd login &lt;ARGOCD_SERVER&gt; --username admin --password &lt;PWD&gt; --grpc-web</code> List current context <code>argocd context</code> Switch to another context <code>argocd login &lt;OTHER_ARGOCD_SERVER&gt;</code> Current context info argocd account get-user-info ### \ud83d\udd27 2. Cluster Management Intent Command List all known clusters <code>argocd cluster list --grpc-web</code> Add external cluster <code>argocd cluster add &lt;CONTEXT_NAME&gt; --grpc-web</code> Remove a cluster <code>argocd cluster rm &lt;SERVER_URL&gt;</code> <p>Use <code>kubectl config get-contexts</code> to find your kube context name.</p>"},{"location":"ArgoCD/Argocd%20CLI/#3-application-management","title":"\ud83d\udce6 3. Application Management","text":"Intent Command Create an application <code>argocd app create &lt;APPNAME&gt; --repo &lt;REPO_URL&gt; --path &lt;CHART_PATH&gt; --dest-server &lt;DEST_SERVER&gt; --dest-namespace &lt;NS&gt; --grpc-web</code> Delete an app <code>argocd app delete &lt;APPNAME&gt; --yes --grpc-web</code> List all apps <code>argocd app list --grpc-web</code> Show app status <code>argocd app get &lt;APPNAME&gt; --grpc-web</code> Sync (apply latest manifests) <code>argocd app sync &lt;APPNAME&gt; --grpc-web</code> Rollback to previous revision <code>argocd app rollback &lt;APPNAME&gt; &lt;REVISION&gt;</code> Manually refresh app state <code>argocd app refresh &lt;APPNAME&gt; --grpc-web</code> only logs argocd app logs  to get the manifests argocd app manifests  --grpc-web"},{"location":"ArgoCD/Argocd%20CLI/#to-list-argocd-apps-filtered-by-status","title":"To list ArgoCD apps filtered by status","text":"<pre><code># Get only Synced apps\nargocd app list --grpc-web -o wide | grep Synced\n\n# Get only OutOfSync apps\nargocd app list --grpc-web -o wide | grep OutOfSync\n\n# Get only Healthy apps\nargocd app list --grpc-web -o wide | grep Healthy\n\n# Get only Degraded apps\nargocd app list --grpc-web -o wide | grep Degraded\n</code></pre>"},{"location":"ArgoCD/Argocd%20CLI/#4-app-sync-policies","title":"\ud83e\udde0 4. App Sync Policies","text":"Intent Command Set auto sync <code>argocd app set &lt;APPNAME&gt; --sync-policy automated --grpc-web</code> Set manual sync <code>argocd app set &lt;APPNAME&gt; --sync-policy none --grpc-web</code> Enable self-heal (auto repair) <code>argocd app set &lt;APPNAME&gt; --self-heal --grpc-web</code> Disable pruning <code>argocd app set &lt;APPNAME&gt; --auto-prune=false --grpc-web</code> ### \ud83d\udcc1 5. Repositories Intent Command List repos <code>argocd repo list --grpc-web</code> Add a new Git repo <code>argocd repo add &lt;REPO_URL&gt; --username &lt;USER&gt; --password &lt;PWD&gt; --grpc-web</code> Remove a repo <code>argocd repo rm &lt;REPO_URL&gt;</code> ### \ud83d\udd0d 6. Debug / Logs Intent Command View app logs (kubectl way) <code>kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server</code> View sync history <code>argocd app history &lt;APPNAME&gt; --grpc-web</code> Show diff <code>argocd app diff &lt;APPNAME&gt; --grpc-web</code> Got it \u2014 let's go deeper with real-time, practical <code>argocd</code> CLI examples using <code>--grpc-web</code>, with real-world flows in DevOps."},{"location":"ArgoCD/Argocd%20CLI/#7-login-with-ingress-exposed-argocd","title":"\ud83d\udd10 7. Login with Ingress-Exposed ArgoCD","text":"<pre><code>argocd login argocd.yourdomain.com \\\n  --username admin \\\n  --password &lt;YOUR_PASSWORD&gt; \\\n  --insecure \\\n  --grpc-web\n</code></pre> <p>Accessing via <code>argocd.yourdomain.com</code> behind ingress that only supports HTTP/1.1</p>"},{"location":"ArgoCD/Argocd%20CLI/#8-add-external-k8s-cluster-to-argocd","title":"\ud83d\udd17 8. Add External K8s Cluster to ArgoCD","text":"<pre><code>argocd cluster add gouse-k3s-cluster \\\n  --name dev-k3s \\\n  --grpc-web\n</code></pre> <p><code>gouse-k3s-cluster</code> is your kube context name from <code>kubectl config get-contexts</code></p>"},{"location":"ArgoCD/Argocd%20CLI/#9-create-sync-application-from-git","title":"\ud83d\udce6 9. Create &amp; Sync Application from Git","text":"<p><pre><code>argocd app create gouse-guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default \\\n  --sync-policy automated \\\n  --grpc-web\n</code></pre> Then manually sync (if auto-sync is off): <pre><code>argocd app sync gouse-guestbook --grpc-web\n</code></pre></p>"},{"location":"ArgoCD/Argocd%20CLI/#10-update-sync-policy-to-self-heal-auto-prune","title":"\ud83d\udd04 10. Update Sync Policy to Self-Heal &amp; Auto-Prune","text":"<pre><code>argocd app set gouse-guestbook \\\n  --sync-policy automated \\\n  --self-heal \\\n  --auto-prune \\\n  --grpc-web\n</code></pre> <p>Ensures ArgoCD keeps the app in the desired state</p>"},{"location":"ArgoCD/Argocd%20CLI/#11-diff-between-live-and-desired","title":"\ud83d\udd0d 11. Diff Between Live and Desired","text":"<pre><code>argocd app diff gouse-guestbook --grpc-web\n</code></pre> <p>Shows what's different between cluster state and Git</p>"},{"location":"ArgoCD/Argocd%20CLI/#12-delete-an-app-including-k8s-resources","title":"\ud83e\uddf9 12. Delete an App (including K8s resources)","text":"<pre><code>argocd app delete gouse-guestbook --yes --grpc-web\n</code></pre>"},{"location":"ArgoCD/Argocd%20CLI/#13-sync-a-specific-revision-git-commit-sha","title":"\ud83e\uddea 13. Sync a Specific Revision (Git commit SHA)","text":"<pre><code>argocd app sync gouse-guestbook \\\n  --revision 5aeff4b7f93e1d4c9c... \\\n  --grpc-web\n</code></pre>"},{"location":"ArgoCD/Argocd%20CLI/#14-manage-git-repos","title":"\ud83e\uddf0 14. Manage Git Repos","text":"<pre><code>argocd repo add https://github.com/gouse-devops/demo.git \\\n  --username gouse \\\n  --password &lt;TOKEN_OR_PASS&gt; \\\n  --grpc-web\n</code></pre> <pre><code>argocd repo list --grpc-web\n</code></pre>"},{"location":"ArgoCD/Argocd%20CLI/#15-list-all-applications","title":"\ud83d\udcdc 15. List All Applications","text":"<pre><code>argocd app list --grpc-web\n</code></pre>"},{"location":"ArgoCD/Argocd%20CLI/#16-watch-real-time-app-status","title":"\ud83d\udcac 16. Watch Real-time App Status","text":"<pre><code>watch -n2 \"argocd app get gouse-guestbook --grpc-web\"\n</code></pre> <p>Real-time view like <code>kubectl get pods -w</code></p>"},{"location":"ArgoCD/Argocd%20CLI/#17-using-argocd-behind-ingress-nginx-example","title":"\ud83c\udf10 17. Using ArgoCD Behind Ingress (NGINX Example)","text":"<p>Your Ingress config must have: <pre><code>nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\nnginx.ingress.kubernetes.io/grpc-web: \"true\"\n</code></pre> Then <code>--grpc-web</code> is mandatory or you\u2019ll hit gRPC errors.</p>"},{"location":"ArgoCD/Common%20misunderstanding%20in%20ArgoCD%20Usage/","title":"Common misunderstanding in ArgoCD Usage","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Assumption (which is wrong) Description Impact Git = Single Source of Truth Assuming Git always has the correct desired state May lead to ignoring live drift or emergency changes outside Git \"Sync = Deploy\" Believing <code>argocd app sync</code> is equivalent to a full deployment Ignores that partial syncs or hooks may not trigger everything Manual Override Ignorance Not tracking manual <code>kubectl</code> changes because \"ArgoCD will sync it anyway\" Causes hidden drift and misalignment with Git UI-Only Comfort Relying only on ArgoCD UI instead of CLI or automation Slows down automation, less reproducible workflows Overtrusting Health Status Assuming \"Healthy\" status means the app works perfectly Health checks are often superficial and may not reflect runtime issues One Repo Fits All Keeping all apps and configs in a single monorepo Makes scaling, ownership, and access control harder Ignoring Namespaces Deploying all apps in <code>default</code> or one namespace Security, RBAC, and observability become messy No Environment Parity Assuming dev/stage/prod ArgoCD instances behave the same Misconfigurations due to mismatched secrets, resources, or RBAC Tooling Thinking ArgoCD replaces Helm/Kustomize completely Leads to neglecting proper templating and environment layering"},{"location":"ArgoCD/How%20ArgoCD%20and%20OpenShift/","title":"How ArgoCD and OpenShift","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"ArgoCD/How%20ArgoCD%20and%20OpenShift/#how-argocd-and-openshift-work-together","title":"\ud83d\udd01 How ArgoCD and OpenShift Work Together","text":"Tool Purpose Typical Use in DevOps ArgoCD GitOps continuous delivery for Kubernetes Sync Kubernetes manifests from Git repos automatically OpenShift Enterprise Kubernetes platform with enhanced security &amp; UI Hosts clusters, manages RBAC, provides developer-friendly UI ### \ud83d\udd27 Common Use Case <p>ArgoCD manages app deployments into an OpenShift (OCP) cluster, syncing Git-based manifests or Helm charts. OpenShift provides the platform, ArgoCD handles the delivery.</p>"},{"location":"ArgoCD/How%20ArgoCD%20and%20OpenShift/#devops-challenges-when-using-argocd-on-openshift","title":"\ud83e\udde8 DevOps Challenges (When Using ArgoCD on OpenShift)","text":"Problem Area Details Impact RBAC Conflict OpenShift's stricter SecurityContextConstraints (SCC) may block ArgoCD apps unless configured Pods won't start or get permission denied ServiceAccount Confusion ArgoCD needs its own ServiceAccount with correct roles; often users forget to bind <code>edit</code> or <code>admin</code> on target namespaces ArgoCD syncs fail with <code>permission denied</code> Ingress Differences OpenShift routes vs Kubernetes Ingress objects behave differently ArgoCD apps fail unless OpenShift Route is created properly ImagePull Secrets OpenShift requires secrets to be attached in a specific way ArgoCD deployments pull image errors Resource Quota Limits OCP enforces limits (CPU/Memory/Pod count) tightly ArgoCD sync gets stuck or partially applied apps Sync Failures SCCs or OCP admission controllers reject certain deployments (e.g., privileged containers, hostPaths, etc.) ArgoCD shows <code>Health: Degraded</code> TLS &amp; Route Conflicts OpenShift automatically handles routes; deploying ingress.yaml via ArgoCD can conflict ArgoCD-managed ingress won\u2019t reflect correctly in OCP UI ArgoCD Console Access In OpenShift Console, ArgoCD UI is external unless set up with OAuth proxy or Route Requires manual <code>oc expose svc/argocd-server</code> or Route CR ### \u2705 Best Practices Category Best Practice RBAC Bind <code>argocd</code> SA to <code>edit</code> or higher role on target namespaces SCC Use custom SCCs with <code>runAsNonRoot</code> if needed; avoid default <code>restricted</code> profile for ArgoCD apps Route Management Let OpenShift handle Routes; avoid managing Ingress via ArgoCD unless explicitly supported Image Management Use OpenShift internal registry or ensure imagePullSecrets are in the same namespace ArgoCD Namespace Control Use ArgoCD Projects to limit access to namespaces, resources, and source repos OAuth Integration Use <code>argocd-dex</code> with OpenShift OAuth provider for smoother login Resource Limits Ensure ArgoCD apps specify resource requests/limits to comply with OpenShift quotas ### \ud83d\udd17 Typical Real-World Command Flow <pre><code># Install ArgoCD in OpenShift (namespace: argocd)\noc new-project argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Expose ArgoCD UI on OpenShift\noc expose svc/argocd-server -n argocd --port=http\n\n# Login to ArgoCD CLI\nargocd login &lt;argo_route_url&gt; --username admin --password &lt;initial_password&gt; --grpc-web\n\n# Create app that deploys to OpenShift project 'dev'\nargocd app create my-app \\\n  --repo https://github.com/example/repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace dev \\\n  --sync-policy automated\n\n# Give ArgoCD service account access to 'dev' project\noc adm policy add-role-to-user edit system:serviceaccount:argocd:argocd-application-controller -n dev\n</code></pre>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/","title":"To configure two K3s clusters in ArgoCD","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To configure two K3s clusters in ArgoCD, follow these first-principle steps:</p>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#prerequisites-on-the-machine-where-argocd-cli-is-installed","title":"\ud83d\udd27 Prerequisites (on the machine where ArgoCD CLI is installed):","text":"<ol> <li>You must have <code>argocd</code> CLI.</li> <li>You must have access to both clusters via <code>~/.kube/config</code>.</li> <li>You must be logged into ArgoCD:</li> </ol> <pre><code>argocd login &lt;ARGOCD_SERVER&gt; --username admin --password &lt;PASSWORD&gt; --insecure\n$ argocd login argocd.local --username admin --password admin123 --insecure\n# Note: you should login without \"http://\"\n</code></pre>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#step-by-step-commands","title":"\ud83e\ude9b Step-by-step Commands:","text":""},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#1-add-first-cluster-to-argocd","title":"1\ufe0f\u20e3 Add first cluster to ArgoCD","text":"<pre><code>gouse@gouse:~/DevOps/multipass_scripts$ kubectl config get-contexts\nCURRENT   NAME          CLUSTER       AUTHINFO   NAMESPACE\n*         dev-context   devcluster    dev-user   \n          prd-context   prd-cluster   prd-user   \n\nargocd cluster add dev-context --name devcluster --insecure\n\nargocd cluster add k3s-user@k3s-cluster --name dev-cluster\n\n\ngouse@gouse:~/DevOps/multipass_scripts$ argocd cluster add dev-context --name devcluster --insecure\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `dev-context` with full cluster level privileges. Do you want to continue [y/N]? y\n{\"level\":\"info\",\"msg\":\"ServiceAccount \\\"argocd-manager\\\" created in namespace \\\"kube-system\\\"\",\"time\":\"2025-07-03T14:00:15+03:00\"}\n{\"level\":\"info\",\"msg\":\"ClusterRole \\\"argocd-manager-role\\\" created\",\"time\":\"2025-07-03T14:00:15+03:00\"}\n{\"level\":\"info\",\"msg\":\"ClusterRoleBinding \\\"argocd-manager-role-binding\\\" created\",\"time\":\"2025-07-03T14:00:15+03:00\"}\n{\"level\":\"info\",\"msg\":\"Created bearer token secret for ServiceAccount \\\"argocd-manager\\\"\",\"time\":\"2025-07-03T14:00:15+03:00\"}\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-07-03T14:00:16+03:00\"}\nCluster 'https://10.189.65.115:6443' added\n</code></pre> <p><code>k3s-user@k3s-cluster</code> matches the context name in your <code>~/.kube/config</code></p>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#2-add-second-cluster-to-argocd","title":"2\ufe0f\u20e3 Add second cluster to ArgoCD","text":"<pre><code>argocd cluster add k3s-user@k3s-prod-cluster --name prod-cluster\n\n\nargocd cluster add prd-context --name prd-cluster --insecure\ngouse@gouse:~/DevOps/multipass_scripts$ argocd cluster add prd-context --name prd-cluster --insecure\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `prd-context` with full cluster level privileges. Do you want to continue [y/N]? y\n{\"level\":\"info\",\"msg\":\"ServiceAccount \\\"argocd-manager\\\" created in namespace \\\"kube-system\\\"\",\"time\":\"2025-07-03T14:01:47+03:00\"}\n{\"level\":\"info\",\"msg\":\"ClusterRole \\\"argocd-manager-role\\\" created\",\"time\":\"2025-07-03T14:01:47+03:00\"}\n{\"level\":\"info\",\"msg\":\"ClusterRoleBinding \\\"argocd-manager-role-binding\\\" created\",\"time\":\"2025-07-03T14:01:47+03:00\"}\n{\"level\":\"info\",\"msg\":\"Created bearer token secret for ServiceAccount \\\"argocd-manager\\\"\",\"time\":\"2025-07-03T14:01:47+03:00\"}\n{\"level\":\"warning\",\"msg\":\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\",\"time\":\"2025-07-03T14:01:48+03:00\"}\nCluster 'https://10.189.65.110:6443' added\n</code></pre> <p>Make sure your kubeconfig has a second context for the prod cluster, like:</p> <pre><code>- name: k3s-prod-cluster\n  context:\n    cluster: k3s-prod\n    user: k3s-user\n</code></pre>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#validate","title":"\u2705 Validate","text":"<p>Check both clusters registered:</p> <pre><code>argocd cluster list\nargocd cluster list --grpc-web\n\n# 1. Check ArgoCD server URL you are talking to\nargocd context\n\n# 2. Add cluster explicitly with kubeconfig path and grpc-web flag\nargocd cluster add https://10.189.65.115:6443 --grpc-web\n\n# 3. Verify cluster is added\nargocd cluster list --grpc-web\n\ngouse@gouse:~/DevOps/multipass_scripts$ argocd cluster list --grpc-web\nSERVER                          NAME         VERSION  STATUS   MESSAGE                                                  PROJECT\nhttps://10.189.65.110:6443      prd-cluster           Unknown  Cluster has no applications and is not being monitored.  \nhttps://10.189.65.115:6443      devcluster            Unknown  Cluster has no applications and is not being monitored.  \nhttps://kubernetes.default.svc  in-cluster            Unknown  Cluster has no applications and is not being monitored.  \n\n\n# 4. Try to create app again\nargocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://10.189.65.115:6443 \\\n  --dest-namespace default \\\n  --sync-policy automated \\\n  --grpc-web\n\nargocd app create guestbook-prd \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://10.189.65.110:6443 \\\n  --dest-namespace default \\\n  --sync-policy automated \\\n  --grpc-web\n</code></pre>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#sample-kubeconfig-should-have-2-contexts","title":"\ud83d\udcdd Sample <code>~/.kube/config</code> should have 2 contexts:","text":"<pre><code>contexts:\n- name: k3s-cluster\n  context:\n    cluster: k3s-cluster\n    user: k3s-user\n- name: k3s-prod-cluster\n  context:\n    cluster: k3s-prod\n    user: k3s-user\n</code></pre>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#why-its-needed-grpc","title":"\ud83d\udca1 Why it's needed --grpc:","text":"<p>We use <code>--grpc-web</code> with the <code>argocd</code> CLI when the ArgoCD API server is exposed through a reverse proxy (like NGINX, Ingress, etc.) that only supports HTTP/1.1 \u2014 and not native gRPC (which needs HTTP/2).</p> <p>ArgoCD CLI by default uses gRPC over HTTP/2, but many Ingress controllers or proxies: - don\u2019t support raw gRPC, or - strip HTTP/2 headers, breaking gRPC calls.</p> <p>To bypass this, <code>--grpc-web</code> makes the CLI use gRPC-Web (gRPC over HTTP/1.1 + JSON encoding), which is more compatible with typical reverse proxies and ingress setups.</p>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#when-to-use","title":"\u2705 When to use:","text":"<p>Use <code>--grpc-web</code> if:</p> <ul> <li>You access ArgoCD behind an ingress controller (e.g. NGINX Ingress).</li> <li>You get errors like:<ul> <li><code>Failed to invoke grpc call</code></li> <li><code>rpc error: code = Unavailable desc = transport is closing</code></li> <li><code>connect: connection refused</code> for gRPC</li> </ul> </li> </ul>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#when-not-needed","title":"\u274c When not needed:","text":"<ul> <li>If ArgoCD is directly exposed on a LoadBalancer IP or NodePort without any ingress or proxy, and supports HTTP/2, then <code>--grpc-web</code> is not required.</li> </ul>"},{"location":"ArgoCD/To%20configure%20two%20K3s%20clusters%20in%20ArgoCD/#tip","title":"Tip:","text":"<p>You can set it as default to avoid adding every time: <pre><code>alias argocd='argocd --grpc-web'\n</code></pre> Or set context-wide config (manually) via env or wrapper.</p>"},{"location":"CI-CD%20Tools/List%20of%20online%20open-source%20CI-CD%20tools/","title":"List of online open source CI CD tools","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Tool Hosted Option Open Source SCM Support Notes GitHub Actions \u2705 (GitHub) \u274c GitHub only Free tier, integrated into GitHub GitLab CI/CD \u2705 (GitLab) \u2705 GitLab Full DevOps suite Drone CI \u2705 \u2705 GitHub, GitLab Lightweight, container-native Woodpecker CI \u2705 (self-host) \u2705 GitHub, GitLab Drone fork, simple &amp; fast CircleCI \u2705 \u274c GitHub, Bitbucket Free tier available Argo CD / Argo Workflows \u274c (self-host only) \u2705 GitOps-style Kubernetes-native workflows Jenkins X \u2705 (via GCP) \u2705 GitHub, GitLab Jenkins for Kubernetes with GitOps Tekton \u274c (self-host) \u2705 Any Kubernetes-native pipelines Buildkite \u2705 \u274c GitHub, GitLab Agents run on your infra Tool Hosting Type Open Source UI Available Best For Woodpecker CI \u2705 Self-host \u2705 \u2705 Web UI Lightweight pipelines in Docker Drone CI \u2705 Self-host \u2705 \u2705 Web UI Container-native CI/CD GitLab CI/CD \u2705 GitLab-hosted \u2705 \u2705 Full UI Full DevOps pipelines Jenkins X \u2705 Self-host \u2705 \u2705 (via UI) Kubernetes + GitOps Argo Workflows \u2705 Self-host \u2705 \u2705 Web UI Kubernetes-native workflows Tekton + Dashboard \u2705 Self-host \u2705 \u2705 Web UI K8s-native, customizable Concourse CI \u2705 Self-host \u2705 \u2705 Web UI Pipeline-as-code, secure ### \ud83d\udd27 For k3d/k3s, recommended: <ol> <li>Woodpecker CI \u2013 lightweight, easy to set up with minimal resources.</li> <li>Drone CI \u2013 native to containers, simple YAML pipelines.</li> <li>Argo Workflows \u2013 great for Kubernetes-native automation.</li> <li>GitLab CE \u2013 if you need an all-in-one platform.</li> </ol>"},{"location":"CRDs/Controllers%20and%20CRDs/","title":"Controllers and CRDs","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes, you can list all controllers running in your Kubernetes cluster by checking the pods in the <code>kube-system</code> namespace and other namespaces where controllers commonly run. <pre><code># List all pods in kube-system (common place for core controllers)\nkubectl get pods -n kube-system\n\n# List pods in all namespaces (to find controllers elsewhere)\nkubectl get pods --all-namespaces\n\n# Filter pods by name pattern (e.g., controller)\nkubectl get pods --all-namespaces | grep controller\n</code></pre></p> <p>If you want to see specifically which controllers are installed as CustomResourceDefinitions (CRDs) or operators: <pre><code># List all CRDs (often installed by controllers/operators)\nkubectl get crds\n\n# Check deployed operators via deployments\nkubectl get deployments --all-namespaces | grep operator\n</code></pre></p> <p>For Sealed Secrets controller, for example: <pre><code>kubectl get pods -n kube-system -l name=sealed-secrets-controller\n</code></pre></p> <p>This will tell you if the sealed-secrets controller is running.</p> <p>CRDs (CustomResourceDefinitions) are custom resource types you can add to Kubernetes to extend its API.</p> <ul> <li>When you install a CRD, you define a new kind of object your cluster understands.</li> <li>To make those CRDs useful, you usually run a controller (often called an operator) that watches and manages those custom resources.</li> </ul> <p>So:</p> Term Meaning Controller The logic/process that manages resources and enforces desired state (built-in or custom) CRD Custom resource type you add to Kubernetes <p>Controllers often come bundled with CRDs (like in Operators), but a controller is not the same as a CRD.</p> <p>Example:</p> <ul> <li>Sealed Secrets has a CRD <code>SealedSecret</code> and a controller that watches SealedSecret objects and creates Secrets.</li> <li>The CRD is just the schema.</li> <li>The controller does the work.</li> </ul>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/","title":"Create your own Custom Resource Definition (CRD)","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/#what-you-need-to-do","title":"\u2705 What You Need to Do","text":"<p>When you're extending Kubernetes with your own custom types \u2014 without touching the core Kubernetes code \u2014 you use: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\n</code></pre></p> Goal Use Use core Kubernetes objects <code>apiVersion: v1</code>, <code>apps/v1</code>, etc. Define your own custom resource type <code>apiVersion: apiextensions.k8s.io/v1</code> Interact with your new object type <code>apiVersion: yourdomain.com/v1</code>, etc. <p>\u2705 So yes \u2014 all CRD definitions must use <code>apiextensions.k8s.io/v1</code> to register a new type into the Kubernetes API server.</p>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/#step-1-define-the-crd-structure-and-schema","title":"\ud83d\udee0\ufe0f Step 1: Define the CRD (structure and schema)","text":"<p>crd.yaml</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: hellos.example.com    # plural.name.group\nspec:\n  group: example.com\n  names:\n    kind: Hello               # Singular kind\n    plural: hellos\n    singular: hello\n    shortNames:\n    - hi\n  scope: Namespaced\n  versions:\n  - name: v1\n    served: true\n    storage: true\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              message:\n                type: string\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f crd.yaml\n</code></pre>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/#step-2-create-a-custom-resource-using-your-crd","title":"\ud83d\udee0\ufe0f Step 2: Create a custom resource using your CRD","text":"<p>hello.yaml</p> <pre><code>apiVersion: example.com/v1\nkind: Hello\nmetadata:\n  name: greeting\nspec:\n  message: \"Hello, CRD World!\"\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f hello.yaml\n</code></pre>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/#step-3-optional-build-a-controller-to-act-on-your-crd","title":"\ud83d\udee0\ufe0f Step 3: (Optional) Build a controller to act on your CRD","text":"<p>Without a controller, CRDs are just data. To make them do something, write a controller in Go, Python, or use tools like:</p> <ul> <li>kubebuilder (Go-based)</li> <li>operator-sdk</li> <li>kopf (Python)</li> <li>Metacontroller</li> <li>Pulumi/Helm (for lightweight logic)</li> </ul>"},{"location":"CRDs/Create%20your%20own%20Custom%20Resource%20Definition%20%28CRD%29/#step-4-see-your-crd-in-action","title":"\ud83d\udd0d Step 4: See your CRD in action","text":"<pre><code>kubectl get hellos\nkubectl describe hello greeting\n</code></pre>"},{"location":"CRDs/Custom%20Resource%20Definition/","title":"Custom Resource Definition","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <ul> <li>Sealed Secrets has a CRD <code>SealedSecret</code> and a controller that watches SealedSecret objects and creates Secrets.</li> <li>The CRD is just the schema.</li> <li>The controller does the work. Note: Controllers often come bundled with CRDs (like in Operators), but a controller is not the same as a CRD.</li> </ul>"},{"location":"CRDs/Custom%20Resource%20Definition/#1-what-are-crds","title":"1. What are CRDs?","text":"<p>Definition: CRDs (Custom Resource Definitions) extend the Kubernetes API to allow users to create and manage custom resources (besides built-in resources like Pods, Deployments, etc.).  </p> <p>Key Concepts: - Custom Resource (CR): An instance of a CRD (e.g., <code>MySQLDatabase</code>, <code>RedisCluster</code>). - CRD: The schema/definition that describes the custom resource (like a blueprint).  </p> <p>Example: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: mysqlsamples.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema: {...}\n  scope: Namespaced\n  names:\n    plural: mysqlsamples\n    singular: mysqlsample\n    kind: MySQLSample\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#2-why-use-crds","title":"2. Why Use CRDs?","text":"<p>Purpose/Benefits: - Extend Kubernetes: Add domain-specific resources (e.g., <code>PostgresDB</code>, <code>TensorFlowJob</code>). - Declarative APIs: Manage applications/resources using <code>kubectl</code> like native objects. - Automation: Integrate with operators (e.g., <code>etcd-operator</code>) for lifecycle management. - Reusability: Share CRDs across teams/organizations.  </p> <p>Use Cases: - Databases, ML workloads, CI/CD pipelines, etc.  </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#3-when-to-use-crds","title":"3. When to Use CRDs?","text":"<p>Scenarios: - When Kubernetes lacks a built-in resource for your use case. - When you need a declarative API for your application. - When paired with a Kubernetes Operator for complex logic (e.g., backup, scaling).  </p> <p>Alternatives: - Use ConfigMaps/Secrets for simple configurations. - Use Helm charts for packaging (but Helm doesn\u2019t provide API extensions).  </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#4-how-to-use-crds","title":"4. How to Use CRDs?","text":"<p>Steps to Create/Use a CRD: </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#1-define-the-crd","title":"1. Define the CRD","text":"<p><pre><code># mysql-crd.yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: mysqlsamples.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                dbName:\n                  type: string\n                replicas:\n                  type: integer\n  scope: Namespaced\n  names:\n    plural: mysqlsamples\n    singular: mysqlsample\n    kind: MySQLSample\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-crd.yaml\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#2-create-a-custom-resource-cr","title":"2. Create a Custom Resource (CR)","text":"<p><pre><code># mysql-instance.yaml\napiVersion: example.com/v1\nkind: MySQLSample\nmetadata:\n  name: my-mysql\nspec:\n  dbName: \"mydb\"\n  replicas: 2\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-instance.yaml\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#3-verify","title":"3. Verify","text":"<pre><code>kubectl get crd                          # List CRDs\nkubectl get mysqlsamples                 # List custom resources\nkubectl describe mysqlsamples my-mysql   # Inspect a CR\n</code></pre>"},{"location":"CRDs/Custom%20Resource%20Definition/#4-optional-build-an-operator","title":"4. (Optional) Build an Operator","text":"<p>Use tools like: - Kubebuilder - Operator SDK to automate CRD management (e.g., reconcile loops).</p>"},{"location":"CRDs/Custom%20Resource%20Definition/#summary-w3h-table","title":"Summary (W3H Table)","text":"Aspect Description What Kubernetes API extension for custom resources. Why To add domain-specific resources declaratively. When When built-in resources are insufficient. How Define CRD \u2192 Create CR \u2192 (Optional) Use Operator."},{"location":"CRDs/Deep%20Dive%20into%20CRDs/","title":"Deep Dive into CRDs","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik Great! Let\u2019s dive deeper into Custom Resource Definitions (CRDs), covering advanced concepts, best practices, and real-world examples.  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#deep-dive-into-crds","title":"Deep Dive into CRDs","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#1-what-are-crds-extended","title":"1. What are CRDs? (Extended)","text":"<ul> <li>CRDs vs. Built-in Resources: </li> <li>Built-in resources (e.g., <code>Pods</code>, <code>Deployments</code>) are defined in the Kubernetes core code.  </li> <li> <p>CRDs let users define new resource types without modifying the Kubernetes source code.  </p> </li> <li> <p>CRD vs. Operator: </p> </li> <li>A CRD defines the schema (structure) of a custom resource.  </li> <li> <p>An Operator is a controller that manages CRs (e.g., handling creation, scaling, backup).  </p> </li> <li> <p>Example CRDs in the Wild: </p> </li> <li><code>CertManager</code> (<code>Certificate</code>, <code>Issuer</code>)  </li> <li><code>ArgoCD</code> (<code>Application</code>, <code>AppProject</code>)  </li> <li><code>Prometheus Operator</code> (<code>Prometheus</code>, <code>ServiceMonitor</code>)  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#2-why-use-crds-advanced-benefits","title":"2. Why Use CRDs? (Advanced Benefits)","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#a-declarative-apis","title":"a) Declarative APIs","text":"<ul> <li>Instead of imperative scripts (<code>kubectl run</code>, <code>helm install</code>), CRDs allow: <pre><code>apiVersion: databases.example.com/v1\nkind: PostgreSQL\nmetadata:\n  name: my-db\nspec:\n  replicas: 3\n  storage: 100Gi\n</code></pre>   Kubernetes automatically reconciles the desired state.  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#b-integration-with-kubernetes-tooling","title":"b) Integration with Kubernetes Tooling","text":"<ul> <li>CRs work with:  </li> <li><code>kubectl</code> (e.g., <code>kubectl get postgresql</code>)  </li> <li>RBAC (<code>Role</code>/<code>ClusterRole</code> for CR access)  </li> <li><code>kubectl explain postgresql.spec</code> (for self-documenting APIs)  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#c-operator-pattern","title":"c) Operator Pattern","text":"<ul> <li>CRDs + Controllers = Operators (e.g., <code>etcd-operator</code>, <code>redis-operator</code>).  </li> <li>Operators handle:  </li> <li>Provisioning  </li> <li>Scaling  </li> <li>Backups/restores  </li> <li>Upgrades  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#3-when-to-use-crds-decision-guide","title":"3. When to Use CRDs? (Decision Guide)","text":"Use Case CRD? Alternative Need a custom API object \u2705 Yes ConfigMap/Secret (if simple) Complex lifecycle logic \u2705 Yes Helm (if only templating) Reusable across clusters \u2705 Yes Ansible/scripts (imperative) Just configuration storage \u274c No ConfigMap <p>Example: - \u2705 Good for CRD: A <code>SparkJob</code> resource that needs autoscaling, monitoring. - \u274c Not needed: Storing a list of feature flags (use <code>ConfigMap</code>).  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#4-how-to-use-crds-advanced-topics","title":"4. How to Use CRDs? (Advanced Topics)","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#a-crd-schema-validation","title":"a) CRD Schema Validation","text":"<p>Define data types, required fields, and defaults in the CRD: <pre><code>schema:\n  openAPIV3Schema:\n    type: object\n    properties:\n      spec:\n        type: object\n        required: [\"dbName\"]  # Mandatory field\n        properties:\n          dbName:\n            type: string\n          replicas:\n            type: integer\n            default: 1        # Default value\n</code></pre></p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#b-versioning-conversion","title":"b) Versioning &amp; Conversion","text":"<ul> <li>Support multiple API versions (e.g., <code>v1alpha1</code>, <code>v1beta1</code>, <code>v1</code>).  </li> <li>Use conversion webhooks to migrate between versions.  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#c-finalizers-deletion-control","title":"c) Finalizers &amp; Deletion Control","text":"<p>Prevent accidental deletion: <pre><code>metadata:\n  finalizers:\n  - \"finalizer.database.example.com\"\n</code></pre> A controller must clear the finalizer before deletion.  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#d-subresources-statusscale","title":"d) Subresources (Status/Scale)","text":"<p>Enable <code>status</code> updates and horizontal scaling: <pre><code>subresources:\n  status: {}       # For .status field\n  scale:           # For `kubectl scale`\n    specReplicasPath: .spec.replicas\n    statusReplicasPath: .status.replicas\n</code></pre></p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#e-admission-webhooks","title":"e) Admission Webhooks","text":"<ul> <li>MutatingWebhook: Modify CRs before creation (e.g., inject defaults).  </li> <li>ValidatingWebhook: Reject invalid CRs (e.g., invalid <code>replicas</code> value).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#5-crd-best-practices","title":"5. CRD Best Practices","text":"<ol> <li>Naming Conventions </li> <li>Use <code>&lt;plural&gt;.&lt;group&gt;</code> (e.g., <code>postgresqls.databases.example.com</code>).  </li> <li> <p>Follow DNS subdomain rules (lowercase, no underscores).  </p> </li> <li> <p>Schema Design </p> </li> <li>Use <code>required</code> fields for critical specs.  </li> <li> <p>Avoid frequent schema changes (break compatibility).  </p> </li> <li> <p>Operator Integration </p> </li> <li> <p>Use controller-runtime (Kubebuilder/Operator SDK) for reliable reconciliation.  </p> </li> <li> <p>RBAC </p> </li> <li>Restrict access to CRDs with <code>ClusterRole</code>: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: postgresql-admin\nrules:\n- apiGroups: [\"databases.example.com\"]\n  resources: [\"postgresqls\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n</code></pre></li> </ol>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#6-example-full-crd-operator-flow","title":"6. Example: Full CRD + Operator Flow","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-1-define-a-crd-cassandraclusteryaml","title":"Step 1: Define a CRD (<code>cassandracluster.yaml</code>)","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: cassandraclusters.db.example.com\nspec:\n  group: db.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema: { ... }\n  scope: Namespaced\n  names:\n    plural: cassandraclusters\n    singular: cassandracluster\n    kind: CassandraCluster\n</code></pre>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-2-create-a-cr-my-cassandrayaml","title":"Step 2: Create a CR (<code>my-cassandra.yaml</code>)","text":"<pre><code>apiVersion: db.example.com/v1\nkind: CassandraCluster\nmetadata:\n  name: my-cassandra\nspec:\n  nodes: 3\n  storage: 50Gi\n</code></pre>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-3-deploy-an-operator","title":"Step 3: Deploy an Operator","text":"<ul> <li>The operator watches <code>CassandraCluster</code> CRs and:  </li> <li>Creates <code>StatefulSet</code> + <code>Services</code>.  </li> <li>Handles scaling (<code>kubectl scale cassandracluster my-cassandra --replicas=5</code>).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#7-tools-for-working-with-crds","title":"7. Tools for Working with CRDs","text":"Tool Purpose Kubebuilder Scaffold CRDs + Operators (Go) Operator SDK Build Operators (Go/Ansible/Helm) kustomize Manage CRD YAMLs kubeval Validate CRD schemas"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#8-common-pitfalls","title":"8. Common Pitfalls","text":"<ul> <li>Schema Changes Breaking Clusters: Always support backward compatibility.  </li> <li>Orphaned CRs: Ensure your operator handles CRD deletion gracefully.  </li> <li>Performance Issues: Avoid CRDs with high churn (e.g., frequent updates).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#summary","title":"Summary","text":"<ul> <li>CRDs = Custom API Objects in Kubernetes.  </li> <li>Operators = Controllers that manage CRs.  </li> <li>Use Cases: Databases, ML, CI/CD, etc.  </li> <li>Advanced Features: Webhooks, versioning, subresources.  </li> </ul> <p>Would you like a hands-on lab or a specific CRD example (e.g., <code>PostgresOperator</code>)? Let me know! \ud83d\ude80</p>"},{"location":"CRDs/To%20know%20which%20namespaces%20or%20manifests%20your%20CRDs%20are%20being%20used/","title":"To know which namespaces or manifests your CRDs are being used","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"CRDs/To%20know%20which%20namespaces%20or%20manifests%20your%20CRDs%20are%20being%20used/#step-1-list-all-resources-of-a-crd-kind","title":"\ud83d\udd0d Step 1: List all resources of a CRD kind","text":"<p>If your CRD is named <code>sealedsecrets.bitnami.com</code>, then the resource kind is <code>SealedSecret</code>.</p> <p>Run:</p> <pre><code>kubectl get sealedsecrets --all-namespaces\n</code></pre> <p>This shows all instances (usages) of <code>SealedSecret</code> in your cluster across all namespaces.</p> <p>Replace <code>sealedsecrets</code> with the plural resource name of your CRD.</p> <p>To find the kind and plural name from CRD:</p> <pre><code>kubectl get crd sealedsecrets.bitnami.com -o yaml | grep kind\n# kind: SealedSecret\n</code></pre>"},{"location":"CRDs/To%20know%20which%20namespaces%20or%20manifests%20your%20CRDs%20are%20being%20used/#step-2-search-for-crd-usage-in-yaml-manifests-gitops-or-local-repo","title":"\ud83d\udd0d Step 2: Search for CRD usage in YAML manifests (GitOps or local repo)","text":"<p>If you use Git for infra (e.g., ArgoCD), run:</p> <pre><code>grep -R \"kind: SealedSecret\" .\n</code></pre> <p>This shows all YAML files that define <code>SealedSecret</code> objects.</p>"},{"location":"CRDs/To%20know%20which%20namespaces%20or%20manifests%20your%20CRDs%20are%20being%20used/#step-3-get-full-yaml-of-existing-crd-objects","title":"\ud83d\udd0d Step 3: Get full YAML of existing CRD objects","text":"<p>To see how it's used (what fields, metadata, etc.):</p> <pre><code>kubectl get sealedsecrets -n default -o yaml\n</code></pre> <p>Replace <code>sealedsecrets</code> and <code>default</code> with your CRD and namespace.</p>"},{"location":"CRDs/To%20know%20which%20namespaces%20or%20manifests%20your%20CRDs%20are%20being%20used/#summary","title":"\ud83e\udde0 Summary","text":"Task Command Example List instances of a CRD <code>kubectl get &lt;crd-resource&gt; --all-namespaces</code> Check CRD usage in Git/YAML files <code>grep -R \"kind: &lt;CRDKind&gt;\" .</code> View full object definition <code>kubectl get &lt;crd-resource&gt; -o yaml</code>"},{"location":"Cert-manager/install-cert-manager/","title":"Install cert manager","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here are the full steps to deploy MailHog with HTTPS (TLS) using Traefik + cert-manager on a <code>k3d</code> cluster:</p> <p>updated: 2025-07-06T12ssZ author: Gouse Shaik</p>"},{"location":"Cert-manager/install-cert-manager/#step-1-k3d-cluster-with-port-80-443","title":"\u2705 Step 1: <code>k3d</code> Cluster with Port 80 &amp; 443","text":"<pre><code># k3d-mailhog.yaml\napiVersion: k3d.io/v1alpha4\nkind: Simple\nname: mailhog-tls\nports:\n  - port: 80:80     # HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 443:443   # HTTPS\n    nodeFilters:\n      - loadbalancer\n</code></pre> <pre><code>k3d cluster create --config k3d-mailhog.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-2-install-cert-manager","title":"\u2705 Step 2: Install <code>cert-manager</code>","text":"<pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml\n</code></pre> <p>Wait until all pods in <code>cert-manager</code> namespace are ready.</p>"},{"location":"Cert-manager/install-cert-manager/#check-tls-certificate-secret-created","title":"\u2705 Check TLS Certificate Secret Created?","text":"<pre><code>kubectl get certificate\nkubectl describe certificate mailhog-cert\nkubectl get secret mailhog-tls\n\n\nIf all looks fine and still not working, paste the output of:\nkubectl get ingress\nkubectl describe ingress mailhog\nkubectl get certificate\nkubectl describe certificate mailhog-cert\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-3-install-clusterissuer-self-signed-for-dev","title":"\u2705 Step 3: Install <code>ClusterIssuer</code> (Self-Signed for Dev)","text":"<pre><code># selfsigned-clusterissuer.yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: selfsigned-issuer\nspec:\n  selfSigned: {}\n</code></pre> <pre><code>kubectl apply -f selfsigned-clusterissuer.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-4-create-tls-certificate-for-mailhoglocal","title":"\u2705 Step 4: Create TLS Certificate for <code>mailhog.local</code>","text":"<pre><code># mailhog-cert.yaml\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: mailhog-cert\nspec:\n  secretName: mailhog-tls\n  dnsNames:\n    - mailhog.local\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  commonName: mailhog.local\n</code></pre> <pre><code>kubectl apply -f mailhog-cert.yaml\n\ngouse@gouse:~/DevOps/k3d$ k apply -f mailhog-cert.yaml \nWarning: spec.privateKey.rotationPolicy: In cert-manager &gt;= v1.18.0, the default value changed from `Never` to `Always`.\ncertificate.cert-manager.io/mailhog-cert created\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-5-deploy-mailhog-service","title":"\u2705 Step 5: Deploy MailHog + Service","text":"<p>Same as before (<code>mailhog-deployment.yaml</code>):</p> <pre><code>kubectl apply -f mailhog-deployment.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-6-create-https-ingress-with-tls","title":"\u2705 Step 6: Create HTTPS Ingress with TLS","text":"<pre><code># mailhog-ingress-tls.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mailhog\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: websecure\nspec:\n  tls:\n  - hosts:\n    - mailhog.local\n    secretName: mailhog-tls\n  rules:\n  - host: mailhog.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: mailhog\n            port:\n              number: 8025\n</code></pre> <pre><code>kubectl apply -f mailhog-ingress-tls.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-7-add-to-etchosts","title":"\u2705 Step 7: Add to <code>/etc/hosts</code>","text":"<pre><code>127.0.0.1 mailhog.local\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-8-open-in-browser","title":"\u2705 Step 8: Open in Browser","text":"<pre><code>https://mailhog.local\n</code></pre> <p>You may need to accept the self-signed cert warning if using Chrome/Firefox.</p> <p>Let me know if you want the same using Let\u2019s Encrypt instead of self-signed.</p>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/","title":"For homelab setup with","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>You can practice Kubernetes with HA (2 control planes + 2 workers) under ~15 GiB memory using the tools you listed:</p> Tool Supports HA Min RAM Use ETCD Option Lightweight Recommended Multipass (raw K8s) \u2714\ufe0f (manual) \u274c (&gt;15GiB) \u2714\ufe0f (manual) \u274c \u274c K3s with Multipass \u2714\ufe0f (with tweaks) \u2705 (~10\u201312GiB) \u2714\ufe0f (external or embedded) \u2714\ufe0f \u2705 Best Kind \u274c (no multi-master easily) \u2705 (~6\u20138GiB) \u274c (uses Docker-in-Docker) \u2714\ufe0f \u26a0\ufe0f Limited (no HA) K3s with Docker \u2714\ufe0f (complex, not ideal) \u274c (~&gt;14GiB) \u2714\ufe0f \u26a0\ufe0f (Docker bloat) \u274c k0s \u2714\ufe0f (native HA) \u2705 (~12GiB) \u2714\ufe0f (etcd native) \u2714\ufe0f \u2705 Good"},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/#recommended-setup","title":"\u2705 Recommended Setup:","text":"<p>Option 1: K3s on Multipass</p> <ul> <li>Lightweight</li> <li>Native etcd HA support via embedded etcd in K3s</li> <li>Low memory footprint</li> <li>Easy to automate with cloud-init</li> </ul> <p>Option 2: k0s</p> <ul> <li>Built-in support for HA and etcd</li> <li>Lightweight binaries</li> <li>Works well on multipass or even docker</li> </ul>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/#suggested-resource-plan-ram-split","title":"\ud83d\udee0\ufe0f Suggested Resource Plan (RAM Split):","text":"Node Type Count RAM Each Total Control Plane 2 2.5 GiB 5 GiB Worker Node 2 2.0 GiB 4 GiB Buffer + Host - ~5\u20136 GiB - <p>You\u2019ll stay under 14\u201315 GiB with either k3s with multipass or k0s.</p> <p>Yes, here are other lightweight options to set up Kubernetes with HA (2 control + 2 worker) within 15 GiB RAM usage:</p> Tool/Platform Supports HA Memory Use ETCD Support Runs On Comments MicroK8s \u2714\ufe0f (manual) \u2705 Low (~3\u20134GiB per node) \u2714\ufe0f (with <code>--ha</code>) Native or VM Lightweight and simple Talos + kubeadm \u2714\ufe0f Native \u2705 (~2GiB/node) \u2714\ufe0f Full etcd control Multipass/VM Very clean, secure OS Minikube (w/ KVM) \u274c (no HA) \u274c (heavy) \u274c Local VM Not ideal for HA Flatcar + kubeadm \u2714\ufe0f (manual) \u2705 (~2\u20133GiB) \u2714\ufe0f Multipass/VM Minimal OS"},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/#recommended-if-you-want-alternatives","title":"\u2705 Recommended (if you want alternatives):","text":""},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/#1-microk8s-with-ha-mode","title":"1. MicroK8s with HA mode","text":"<ul> <li>Use snap: <code>sudo snap install microk8s --classic</code></li> <li>Enable HA: <code>microk8s enable ha-cluster</code></li> <li>Lightweight and full Kubernetes</li> <li>Simple to add/remove nodes</li> </ul>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/For%20homelab%20setup%20with/#2-talos-os-with-kubeadm","title":"2. Talos OS with kubeadm","text":"<ul> <li>OS is minimal and API-only, no SSH</li> <li>High security</li> <li>Cleanest etcd/k8s setup</li> <li>Works well even with multipass or QEMU</li> </ul>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/K8s%20HA%20clusters/","title":"K8s HA clusters","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Option Best For Pros Cons kubeadm Production/Custom setup Full control, production-ready Manual setup, complex k3s Lightweight HA Simple HA with embedded etcd, low resource Limited features vs full K8s RKE2 Secure and supported K3s Built-in HA, hardened Slightly heavier than k3s k0s Zero-friction HA No dependencies, embedded etcd Less community support Kubespray Ansible-based multi-node HA Automated, flexible Learning curve, external tooling Openshift Enterprise HA Built-in HA, fully featured Heavy, resource-intensive GKE/EKS/AKS Managed HA Auto HA, easy ops Vendor lock-in, cost kind/k3d Local testing only Fast to test HA config Not for real HA use For on-prem/self-managed: <ul> <li>Use kubeadm or k3s (with external etcd or embedded HA).</li> <li>For minimal resource: k3s with HA is ideal.</li> <li>For full control: kubeadm with external etcd is best.</li> </ul>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/Kind%20Vs%20K3d/","title":"Kind Vs K3d","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Feature Kind K3d (K3s + Docker) Based On kubeadm + containerized control-plane nodes K3s (lightweight Kubernetes) Use Case Kubernetes testing/development Lightweight local clusters / CI/CD Resource Usage Higher (full K8s) Lower (uses K3s, very lightweight) Cluster Creation Speed Slower Faster Runtime Docker or Podman Docker only Multi-node Support Yes Yes HA Support Possible with custom config Supports HA via K3s easily Production Ready No (for dev/test only) Not production, but closer than Kind Networking Custom setup required Easier with K3s built-in Traefik/load-balancer Ingress Support Manual setup Built-in with Traefik Etcd Full etcd SQLite or embedded etcd Kubernetes Version Full version (any official release) K3s versions only In short: <ul> <li>Use Kind if you want to simulate real Kubernetes clusters (same as prod).</li> <li>Use K3d if you want a fast, low-resource cluster for local dev/CI/CD with built-in goodies.</li> </ul>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/Kubernetes%20cluster%20types%20on%20your%20usecases/","title":"Kubernetes cluster types on your usecases","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>choose it wisely.</p> Category Tool/Platform Purpose Open Source Local Dev Clusters Minikube Single-node local testing \u2705 Yes Kind (Kubernetes in Docker) Runs Kubernetes in Docker containers \u2705 Yes K3d Lightweight K3s in Docker \u2705 Yes MicroK8s Lightweight snap-based K8s \u2705 Yes Lightweight Clusters K3s Lightweight Kubernetes for edge/IoT \u2705 Yes Managed Cloud Services GKE (Google Kubernetes Engine) Managed Kubernetes by Google Cloud \u274c No EKS (Elastic Kubernetes Service) AWS managed Kubernetes \u274c No AKS (Azure Kubernetes Service) Azure managed Kubernetes \u274c No Oracle OKE Oracle managed Kubernetes \u274c No IBM Cloud Kubernetes IBM managed Kubernetes \u274c No On-Prem &amp; Hybrid OpenShift (OKD = Open-source base) Enterprise-ready with CI/CD features \u2705 (OKD only) Rancher Cluster management and provisioning \u2705 Yes VMware Tanzu Enterprise Kubernetes with vSphere \u274c No Platform9 SaaS-managed Kubernetes anywhere \u274c No Installer Tools kubeadm Manual cluster setup \u2705 Yes Kubespray Ansible-based multi-node cluster setup \u2705 Yes Kops Production-ready cluster management \u2705 Yes Multi-Cluster Mgmt Anthos Hybrid/multi-cloud by Google \u274c No Fleet (Rancher) GitOps and cluster fleet management \u2705 Yes Specialized Use Amazon EKS Anywhere On-prem Kubernetes via AWS \u274c No Red Hat OpenShift Local (CRC) Local OpenShift cluster for dev/test \u274c No (OKD alt)"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/","title":"Deploy HA Kubernetes Cluster on Linux using Rancher RKE2","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Rajesh Kumar\u00a0April 2, 2023\u00a0Leave a Comment</p> <p>Table of Contents</p> <p></p> <ul> <li>What is RKE2?<ul> <li>System Requirements</li> </ul> </li> <li>Step 1 \u2013 Set up Rocky Linux 8 Nodes</li> <li>Step 2 \u2013 Configure the Fixed Registration Address</li> <li>Step 3 \u2013 Download installer script on Rocky Linux 8 Nodes</li> <li>Step 4 \u2013 Set up the First Server Node (Master Node)</li> <li>Step 7 \u2013 Deploy an Application.</li> <li>Reference</li> </ul>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#what-is-rke2","title":"What is RKE2?","text":"<p>RKE stands for\u00a0Rancher Kubernetes Engine. RKE2 also known as the (RKE Government) is a combination of RKE1 and K3s. It inherits usability, ease-of-operations, and deployment model from K3s and close alignment with upstream Kubernetes from RKE1. Normally, RKE2 doesn\u2019t rely on docker, it launches the control plane components as static pods that are managed by the kubelet.</p> <p>The diagram below will help you understand the RKE2 cluster topology.</p> <p></p> <p>RKE2 ships a number of open-source components that include:</p> <ul> <li>K3s<ul> <li>Helm Controller</li> </ul> </li> <li>K8s<ul> <li>API Server</li> <li>Controller Manager</li> <li>Kubelet</li> <li>SchedulerSet up Linux Nodes</li> <li>Proxy</li> </ul> </li> <li>etcd</li> <li>containerd/cri</li> <li>runc</li> <li>Helm</li> <li>Metrics Server</li> <li>NGINX Ingress Controller</li> <li>CoreDNS</li> <li>CNI: Canal (Calico &amp; Flannel), Cilium or Calico</li> </ul>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#system-requirements","title":"System Requirements","text":"<p>Use a system that meets the below requirements:</p> <ul> <li>RAM: 4GB Minimum (we recommend at least 8GB)</li> <li>CPU: 2 Minimum (we recommend at least 4CPU)</li> <li>3 Rocky Linux 8 Nodes</li> <li>Zero\u00a0or\u00a0more agent\u00a0nodes that are designated to run your apps and services</li> <li>A\u00a0load balancer\u00a0to direct front-end traffic to the three nodes.</li> <li>A\u00a0DNS record\u00a0to map a URL to the load balancer</li> </ul>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#step-1-set-up-rocky-linux-8-nodes","title":"Step 1 \u2013 Set up Rocky Linux 8 Nodes","text":"<p>For this guide, we will use 3 Rocky Linux nodes, a load balancer, and RKE2 agents(1 or more).</p> TASK HOSTNAME IP ADDRESS Server Node 1 server1.computingforgeeks.com 192.168.205.2 Server Node 2 server2.computingforgeeks.com 192.168.205.3 Server Node\u00a03 server3.computingforgeeks.com 192.168.205.33 Load Balancer rke.computingforgeeks.com 192.168.205.9 Agent Node1 agent1.computingforgeeks.com 192.168.205.43 Agent Node2 agent2.computingforgeeks.com 192.168.205.44 <p>Set the hostnames as shown:</p> <p></p> <p>Add the hostnames to /etc/hosts on each node</p> <p></p> <p>Configure the firewall on all the nodes as shown:</p> <p></p>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#step-2-configure-the-fixed-registration-address","title":"Step 2 \u2013 Configure the Fixed Registration Address","text":"<p>To achieve high availability, you are required to set up an odd number of server plane nodes(runs etcd, the Kubernetes API, and other control plane services). The other server nodes and agent nodes need a URL they can use to register against. This is either an IP or domain name of any of the control nodes. This is mainly done to maintain quorum so that the cluster can afford to lose connection with one of the nodes without impacting the functionality cluster.</p> <p>This can be achieved using the following:</p> <ul> <li>A layer 4 (TCP) load balancer</li> <li>Round-robin DNS</li> <li>Virtual or elastic IP addresses</li> </ul> <p>In this guide, we will configure NGINX as a layer 4 (TCP) load balancer to forward the connection to one of the RKE nodes.</p> <p></p> <pre><code>user nginx;\nworker_processes 4;\nworker_rlimit_nofile 40000;\nerror_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n\n# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents {\n    worker_connections 8192;\n}\n\nstream {\nupstream backend {\n        least_conn;\n        server &lt;IP_NODE_1&gt;:9345 max_fails=3 fail_timeout=5s;\n        server &lt;IP_NODE_2&gt;:9345 max_fails=3 fail_timeout=5s;\n        server &lt;IP_NODE_3&gt;:9345 max_fails=3 fail_timeout=5s;\n   }\n\n   # This server accepts all traffic to port 9345 and passes it to the upstream. \n   # Notice that the upstream name and the proxy_pass need to match.\n   server {\n\n      listen 9345;\n\n          proxy_pass backend;\n   }\n    upstream rancher_api {\n        least_conn;\n        server &lt;IP_NODE_1&gt;:6443 max_fails=3 fail_timeout=5s;\n        server &lt;IP_NODE_2&gt;:6443 max_fails=3 fail_timeout=5s;\n        server &lt;IP_NODE_3&gt;:6443 max_fails=3 fail_timeout=5s;\n    }\n        server {\n        listen     6443;\n        proxy_pass rancher_api;\n        }\n    upstream rancher_http {\n        least_conn;\n        server 192.168.205.2:80 max_fails=3 fail_timeout=5s;\n        server 192.168.205.3:80 max_fails=3 fail_timeout=5s;\n        server 192.168.205.33:80 max_fails=3 fail_timeout=5s;\n    }\n        server {\n        listen     80;\n        proxy_pass rancher_http;\n        }\n    upstream rancher_https {\n        least_conn;\n        server 192.168.205.2:443 max_fails=3 fail_timeout=5s;\n        server 192.168.205.3:443 max_fails=3 fail_timeout=5s;\n        server 192.168.205.33:443 max_fails=3 fail_timeout=5s;\n    }\n        server {\n        listen     443;\n        proxy_pass rancher_https;\n        }\n}\n</code></pre> <p></p>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#step-3-download-installer-script-on-rocky-linux-8-nodes","title":"Step 3 \u2013 Download installer script on Rocky Linux 8 Nodes","text":"<p>All the Rocky Linux 8 nodes intended for this use need to be configured with the RKE2 repositories that provide the required packages. Instal curl tool on your system:</p> <pre><code>sudo yum -y install curl vim wget\n</code></pre> <p>With curl download the script used to install RKE2 server on your Rocky Linux 8 servers.</p> <pre><code>curl -sfL https://get.rke2.io --output install.sh\n</code></pre> <p>Make the script executable:</p> <pre><code>chmod +x install.sh\n</code></pre> <p>To see script usage options run:</p> <pre><code>less ./install.sh \n</code></pre> <p>Once added, you can install and configure both the RKE2 server and agent on the desired nodes.</p>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#step-4-set-up-the-first-server-node-master-node","title":"Step 4 \u2013 Set up the First Server Node (Master Node)","text":"<p>Install RKE2 server:</p> <pre><code>sudo INSTALL_RKE2_TYPE=server ./install.sh\n</code></pre> <p>Expected output:</p> <pre><code>[INFO]  finding release for channel stable\n[INFO]  using 1.23 series from channel stable\nRocky Linux 8 - AppStream                                                                                                                                              19 kB/s | 4.8 kB     00:00\nRocky Linux 8 - AppStream                                                                                                                                              11 MB/s | 9.6 MB     00:00\nRocky Linux 8 - BaseOS                                                                                                                                                 18 kB/s | 4.3 kB     00:00\nRocky Linux 8 - BaseOS                                                                                                                                                 11 MB/s | 6.7 MB     00:00\nRocky Linux 8 - Extras                                                                                                                                                 13 kB/s | 3.5 kB     00:00\nRocky Linux 8 - Extras                                                                                                                                                 41 kB/s |  11 kB     00:00\nRancher RKE2 Common (stable)                                                                                                                                          1.7 kB/s | 1.7 kB     00:00\nRancher RKE2 1.23 (stable)                                                                                                                                            4.8 kB/s | 4.6 kB     00:00\nDependencies resolved.\n======================================================================================================================================================================================================\n.......\n\nTransaction Summary\n======================================================================================================================================================================================================\nInstall  5 Packages\n\nTotal download size: 34 M\nInstalled size: 166 M\nDownloading Packages:\n.....\n</code></pre> <p>Once installed, you need to create a config file manually. The config file contains the\u00a0<code>tls-san</code>parameter which avoids certificate errors with the fixed registration address.</p> <p>The config file can be created with the command:</p> <pre><code>sudo vim /etc/rancher/rke2/config.yaml\n</code></pre> <p>Add the below lines to the file replacing where required.</p> <pre><code>write-kubeconfig-mode: \"0644\"\ntls-san:\n  - rke.computingforgeeks.com\n  - 192.168.205.9\n</code></pre> <p>Replace\u00a0==rke.computingforgeeks.com==\u00a0with your fixed registration address and\u00a0==192.168.205.9==\u00a0with its IP address.</p> <p>Save the file and start the service;</p> <pre><code>sudo systemctl start rke2-server\nsudo systemctl enable rke2-server\n</code></pre> <p>Confirm status of the service after starting it:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Cluster%20Setup%20HA/Deploy%20HA%20Kubernetes%20Cluster%20on%20Linux%20using%20Rancher%20RKE2/#step-7-deploy-an-application","title":"Step 7 \u2013 Deploy an Application.","text":"<p>Once the above configurations have been made, deploy and application on your cluster. For this guide, we will deploy a demo Nginx application.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2 \n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Check if the pod is up:</p> <pre><code>$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nnginx-deployment-cc7df4f8f-frv65   1/1     Running   0          13s\nnginx-deployment-cc7df4f8f-l9xdb   1/1     Running   0          13s\n</code></pre> <p>Now expose the service:</p> <pre><code>$ kubectl expose deployment nginx-deployment --type=NodePort --port=80\nservice/nginx-deployment exposed\n</code></pre> <p>Obtain the port to which the service has been exposed:</p> <pre><code>$ kubectl get svc\nNAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nkubernetes         ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP          85m\nnginx-deployment   NodePort    10.43.135.164   &lt;none&gt;        80:31042/TCP     2s\n</code></pre> <p>In my case, the service has been exposed to port\u00a031042. Access the application using any controller or worker node IP address with the syntax\u00a0http://IP_Address:31042</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/","title":"Configuring Keycloak for Kubernetes authentication","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Configuring Keycloak for Kubernetes authentication allows centralized user management via OIDC (OpenID Connect). Below are the step-by-step instructions to set this up:</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-1-set-up-keycloak","title":"Step 1: Set Up Keycloak","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#11-install-configure-keycloak","title":"1.1 Install &amp; Configure Keycloak","text":"<ul> <li>Deploy Keycloak (standalone or in Kubernetes):   <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install keycloak bitnami/keycloak \\\n  --set auth.adminUser=admin \\\n  --set auth.adminPassword=admin \\\n  --set service.type=LoadBalancer\n</code></pre></li> <li>Access the Keycloak admin console (<code>http://&lt;keycloak-ip&gt;:8080/admin</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#12-create-a-new-realm","title":"1.2 Create a New Realm","text":"<ul> <li>Go to Admin Console \u2192 Add Realm (e.g., <code>kubernetes</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#13-create-a-client-for-kubernetes","title":"1.3 Create a Client for Kubernetes","text":"<ul> <li>Navigate to Clients \u2192 Create:</li> <li>Client ID: <code>kubernetes</code></li> <li>Client Protocol: <code>openid-connect</code></li> <li>Root URL: <code>https://&lt;k8s-api-server&gt;</code></li> <li>Under Settings:</li> <li>Access Type: <code>confidential</code></li> <li>Valid Redirect URIs: <code>*</code> (or restrict to allowed domains)</li> <li>Save.</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#14-create-users-groups","title":"1.4 Create Users &amp; Groups","text":"<ul> <li>Go to Users \u2192 Add User (e.g., <code>dev-user</code>).</li> <li>Set a password under Credentials.</li> <li>(Optional) Assign users to groups (e.g., <code>dev-team</code>, <code>admin-team</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#15-configure-mappers-for-grouprole-claims","title":"1.5 Configure Mappers (for Group/Role Claims)","text":"<ul> <li>Under Clients \u2192 <code>kubernetes</code> \u2192 Mappers \u2192 Create:</li> <li>Name: <code>groups</code></li> <li>Mapper Type: <code>Group Membership</code></li> <li>Token Claim Name: <code>groups</code></li> <li>Full group path: <code>OFF</code></li> <li>Add to ID token: <code>ON</code></li> <li>Add to access token: <code>ON</code></li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-2-configure-kubernetes-api-server-for-oidc","title":"Step 2: Configure Kubernetes API Server for OIDC","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#21-modify-api-server-flags","title":"2.1 Modify API Server Flags","text":"<p>Edit <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> (on the control plane) and add: <pre><code>spec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --oidc-issuer-url=https://&lt;keycloak-url&gt;/realms/kubernetes\n    - --oidc-client-id=kubernetes\n    - --oidc-username-claim=preferred_username\n    - --oidc-groups-claim=groups\n    - --oidc-ca-file=/etc/ssl/certs/keycloak-ca.crt  # If Keycloak uses HTTPS (recommended)\n</code></pre> - Restart the API server (if not auto-applied):   <pre><code>systemctl restart kubelet\n</code></pre></p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#22-optional-add-keycloak-ca-to-kubernetes-trust-store","title":"2.2 (Optional) Add Keycloak CA to Kubernetes Trust Store","text":"<p>If Keycloak uses HTTPS (self-signed cert): <pre><code>openssl s_client -connect &lt;keycloak-url&gt;:443 -showcerts &lt;/dev/null 2&gt;/dev/null | openssl x509 -outform PEM &gt; keycloak-ca.crt\nsudo cp keycloak-ca.crt /etc/ssl/certs/\nsudo update-ca-certificates\n</code></pre></p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-3-configure-kubectl-for-keycloak-auth","title":"Step 3: Configure kubectl for Keycloak Auth","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#31-install-kubelogin-oidc-helper","title":"3.1 Install <code>kubelogin</code> (OIDC Helper)","text":"<pre><code>wget https://github.com/int128/kubelogin/releases/download/v1.28.0/kubelogin_linux_amd64.zip\nunzip kubelogin_linux_amd64.zip\nsudo mv kubelogin /usr/local/bin/\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#32-create-a-kubeconfig-for-oidc","title":"3.2 Create a kubeconfig for OIDC","text":"<p><pre><code>kubectl config set-credentials dev-user \\\n  --auth-provider=oidc \\\n  --auth-provider-arg=idp-issuer-url=https://&lt;keycloak-url&gt;/realms/kubernetes \\\n  --auth-provider-arg=client-id=kubernetes \\\n  --auth-provider-arg=client-secret=&lt;keycloak-client-secret&gt; \\\n  --auth-provider-arg=refresh-token=&lt;optional&gt; \\\n  --auth-provider-arg=idp-certificate-authority=/etc/ssl/certs/keycloak-ca.crt\n</code></pre> - Get the <code>client-secret</code> from Keycloak (Clients \u2192 <code>kubernetes</code> \u2192 Credentials).</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#33-test-authentication","title":"3.3 Test Authentication","text":"<p><pre><code>kubectl get pods\n</code></pre> - A browser window will open for Keycloak login.</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-4-set-up-rbac-for-keycloak-users","title":"Step 4: Set Up RBAC for Keycloak Users","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#41-create-clusterrolebinding-example","title":"4.1 Create ClusterRoleBinding (Example)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: keycloak-admin-binding\nsubjects:\n- kind: User\n  name: \"admin@example.com\"  # Must match OIDC username_claim\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#42-optional-map-groups-to-roles","title":"4.2 (Optional) Map Groups to Roles","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-team-binding\n  namespace: default\nsubjects:\n- kind: Group\n  name: \"dev-team\"  # From Keycloak groups\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-5-secure-keycloak-optional","title":"Step 5: Secure Keycloak (Optional)","text":"<ul> <li>Enable MFA in Keycloak.</li> <li>Set Token Lifetimes (e.g., 15-minute access tokens).</li> <li>Use Network Policies to restrict API server access.</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#troubleshooting","title":"Troubleshooting","text":"Issue Solution <code>OIDC: invalid bearer token</code> Check <code>--oidc-issuer-url</code> matches Keycloak realm. <code>x509: certificate signed by unknown authority</code> Add Keycloak CA to trust store. No groups in token Ensure <code>groups</code> mapper is configured in Keycloak."},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#final-notes","title":"Final Notes","text":"<p>\u2705 Keycloak + OIDC is ideal for enterprise Kubernetes access. \u2705 RBAC ensures least privilege. \u2705 Short-lived tokens improve security.  </p> <p>Would you like help automating this with Terraform or ArgoCD?</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/","title":"Create a kubeconfig file for the external users","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Sharing access to a Kubernetes cluster with other users involves granting them the necessary permissions securely. Below are the steps to achieve this:</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#1-determine-the-authentication-method","title":"1. Determine the Authentication Method","text":"<p>Kubernetes supports several authentication methods: - kubeconfig File (Common for external users) - Service Accounts (For apps/automation) - Static Token File - OIDC Integration (For enterprise setups)</p> <p>For most cases, sharing a kubeconfig file (or generating one for the user) is the easiest approach.</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#2-createmodify-a-kubeconfig-file-for-the-user","title":"2. Create/Modify a kubeconfig File for the User","text":""},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#option-a-share-an-existing-admin-kubeconfig-not-recommended-for-security","title":"Option A: Share an Existing Admin kubeconfig (Not Recommended for Security)","text":"<ul> <li>Simply provide the <code>~/.kube/config</code> file (contains cluster CA, endpoint, and credentials).</li> <li>Risk: Grants full cluster access. Use only in trusted environments.</li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#option-b-generate-a-dedicated-kubeconfig-for-the-user-recommended","title":"Option B: Generate a Dedicated kubeconfig for the User (Recommended)","text":"<ol> <li>Create a new user certificate/key (if using TLS auth):    <pre><code>openssl genrsa -out username.key 2048\nopenssl req -new -key username.key -out username.csr -subj \"/CN=username/O=group\"  # O=group for RBAC\nopenssl x509 -req -in username.csr -CA /path/to/cluster-ca.crt -CAkey /path/to/cluster-ca.key -CAcreateserial -out username.crt -days 365\n</code></pre></li> <li>Create a kubeconfig for the user:    <pre><code>kubectl config set-credentials username \\\n  --client-certificate=username.crt \\\n  --client-key=username.key\nkubectl config set-context username-context \\\n  --cluster=&lt;cluster-name&gt; \\\n  --user=username\nkubectl config use-context username-context\n</code></pre></li> <li>Provide the generated config file to the user.</li> </ol>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#3-set-up-rbac-permissions-mandatory-for-least-privilege","title":"3. Set Up RBAC Permissions (Mandatory for Least Privilege)","text":"<ol> <li>Create a <code>Role</code> or <code>ClusterRole</code> (define permissions):    <pre><code># Example Role (namespace-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <pre><code># Example ClusterRole (cluster-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-viewer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre></li> <li>Bind the Role to the User:    <pre><code># RoleBinding (namespace-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: User\n  name: \"username\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code># ClusterRoleBinding (cluster-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: view-nodes\nsubjects:\n- kind: User\n  name: \"username\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: node-viewer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></li> </ol>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#4-distribute-the-kubeconfig-securely","title":"4. Distribute the kubeconfig Securely","text":"<ul> <li>Send the kubeconfig file via encrypted channels (e.g., SSH, GPG, or a secure file-sharing tool).</li> <li>Instruct the user to place it at <code>~/.kube/config</code> or use it with:   <pre><code>kubectl --kubeconfig /path/to/user-config get pods\n</code></pre></li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#5-optional-restrict-cluster-access-further","title":"5. (Optional) Restrict Cluster Access Further","text":"<ul> <li>Network Policies: Limit access to the API server by IP.</li> <li>Short-Lived Certificates: Use tools like <code>cert-manager</code> to auto-expire credentials.</li> <li>Audit Logging: Monitor user activity with <code>--audit-policy-file</code>.</li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#6-verify-access","title":"6. Verify Access","text":"<p>Ask the user to run: <pre><code>kubectl get pods\n</code></pre> If permissions are correct, they\u2019ll see only what they\u2019re allowed to.</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#alternative-temporary-access-with-kubectl-proxy","title":"Alternative: Temporary Access with <code>kubectl proxy</code>","text":"<p>If you don\u2019t want to share credentials: <pre><code>kubectl proxy --port=8080 &amp;\n</code></pre> Users can then access the cluster via <code>http://localhost:8080</code> on your machine (use SSH tunneling if remote).</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#summary","title":"Summary","text":"<ol> <li>Choose an auth method (kubeconfig, tokens, OIDC).</li> <li>Generate credentials and a kubeconfig.</li> <li>Define RBAC roles/bindings.</li> <li>Securely share the kubeconfig.</li> <li>Restrict permissions to the least privilege.</li> </ol> <p>For production, consider OpenID Connect (OIDC) or SSO integration (e.g., Dex, Keycloak) for centralized user management.</p>"},{"location":"ClusterAccess/merge%20two%20kubeconfig%20files/","title":"Merge two kubeconfig files","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik Here are the commands to merge two kubeconfig files (<code>k1config</code> and <code>k2config</code>) and use them from another machine:</p> <pre><code># Set your base config (usually your default ~/.kube/config)\nexport KUBECONFIG=/home/gouse/DevOps/multipass_scripts/k1config:/home/gouse/DevOps/multipass_scripts/k2config\n\n# Merge the configs and write to ~/.kube/config\nkubectl config view --flatten &gt; ~/.kube/config\n</code></pre>"},{"location":"ClusterAccess/merge%20two%20kubeconfig%20files/#one-liner-to-merge-and-apply","title":"One-liner to merge and apply:","text":"<pre><code>KUBECONFIG=k1config:k2config kubectl config view --flatten &gt; ~/.kube/config\n</code></pre> <p>Make sure you are in <code>/home/gouse/DevOps/multipass_scripts</code> or use absolute paths.</p>"},{"location":"ClusterAccess/merge%20two%20kubeconfig%20files/#after-merge-check-available-contexts","title":"After merge, check available contexts:","text":"<pre><code>kubectl config get-contexts\n</code></pre>"},{"location":"ClusterAccess/merge%20two%20kubeconfig%20files/#to-switch-between-clusters","title":"To switch between clusters:","text":"<pre><code>kubectl config use-context &lt;context-name&gt;\n</code></pre> <p>Replace <code>&lt;context-name&gt;</code> with the actual name listed in <code>get-contexts</code> output.</p> <p>Error: <pre><code>ouse@gouse:~/DevOps/multipass_scripts$ k config get-contexts\nCURRENT   NAME         CLUSTER      AUTHINFO   NAMESPACE\n*         devcluster   devcluster   default    \n          prdcluster   prdcluster   default    \ngouse@gouse:~/DevOps/multipass_scripts$ k get nodes\nE0703 10:19:52.025850   36078 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:19:52.036931   36078 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:19:52.048468   36078 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:19:52.060949   36078 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:19:52.073295   36078 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nerror: You must be logged in to the server (the server has asked for the client to provide credentials)\ngouse@gouse:~/DevOps/multipass_scripts$ k config set-context devcluster\nContext \"devcluster\" modified.\ngouse@gouse:~/DevOps/multipass_scripts$ k get nodes\nE0703 10:20:38.496594   36331 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:20:38.503812   36331 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:20:38.509652   36331 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:20:38.515449   36331 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nE0703 10:20:38.522562   36331 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials\nerror: You must be logged in to the server (the server has asked for the client to provide credentials)\n</code></pre></p> <pre><code>multipass exec k3s-dev -- sudo cat /etc/rancher/k3s/k3s.yaml &gt; ~/k3s-dev.yaml\nmultipass exec k3s-prd -- sudo cat /etc/rancher/k3s/k3s.yaml &gt; ~/k3s-prd.yaml\n\nKUBECONFIG=~/k3s-dev.yaml:~/k3s-prd.yaml\nkubectl config view --flatten &gt; ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre> <pre><code>kubectl get nodes\nkubectl config use-context k3s-context\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/","title":"ConfigMap is a Kubernetes object used to store non confidential data","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#what-is-a-configmap","title":"What is a ConfigMap?","text":"<p>A ConfigMap is a Kubernetes object used to store non-confidential data in key-value pairs. It allows you to decouple configuration artifacts from container images, making your applications more portable.</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#why-use-configmaps","title":"Why use ConfigMaps?","text":"<ul> <li>Separate configuration from application code</li> <li>Manage environment-specific configurations</li> <li>Update configurations without rebuilding images</li> <li>Share configurations between pods</li> <li>Maintain a single source of truth for configurations</li> </ul>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#when-to-use-configmaps","title":"When to use ConfigMaps?","text":"<ul> <li>Application configuration files</li> <li>Environment variables</li> <li>Command-line arguments</li> <li>Configuration data that doesn't contain sensitive information</li> </ul>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#how-to-use-configmaps","title":"How to use ConfigMaps?","text":""},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#creating-a-configmap","title":"Creating a ConfigMap","text":"<ol> <li> <p>Imperative command:    <pre><code>kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2\n</code></pre></p> </li> <li> <p>From a file:    <pre><code>kubectl create configmap my-config --from-file=path/to/config.file\n</code></pre></p> </li> <li> <p>YAML definition:    <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-config\ndata:\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n  ui.properties: |\n    color.good=purple\n    color.bad=yellow\n</code></pre></p> </li> </ol>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#using-configmaps-in-pods","title":"Using ConfigMaps in Pods","text":"<ol> <li> <p>As environment variables:    <pre><code>env:\n  - name: SPECIAL_LEVEL_KEY\n    valueFrom:\n      configMapKeyRef:\n        name: special-config\n        key: SPECIAL_LEVEL\n</code></pre></p> </li> <li> <p>As volume mounts:    <pre><code>volumes:\n  - name: config-volume\n    configMap:\n      name: game-config\ncontainers:\n  volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n</code></pre></p> </li> </ol>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#updating-configmaps","title":"Updating ConfigMaps","text":"<pre><code>kubectl edit configmap my-config\n# or\nkubectl create configmap my-config --from-literal=key=new-value --dry-run=client -o yaml | kubectl replace -f -\n</code></pre> <p>Note: Pods need to be restarted to pick up ConfigMap changes unless you're using a tool that watches for changes.</p> <p>Here are the commands for working with the ConfigMap manifests and examples provided earlier:</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#1-create-configmap-from-literal-values","title":"1. Create ConfigMap from literal values","text":"<pre><code>kubectl create configmap my-config \\\n  --from-literal=key1=value1 \\\n  --from-literal=key2=value2\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#2-create-configmap-from-a-file","title":"2. Create ConfigMap from a file","text":"<pre><code># Create from a single file\nkubectl create configmap game-config --from-file=game.properties\n\n# Create from multiple files\nkubectl create configmap game-config \\\n  --from-file=game.properties \\\n  --from-file=ui.properties\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#3-create-configmap-from-yaml-manifest","title":"3. Create ConfigMap from YAML manifest","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-config\ndata:\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n  ui.properties: |\n    color.good=purple\n    color.bad=yellow\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#4-create-pod-using-configmap-as-environment-variables","title":"4. Create Pod using ConfigMap as environment variables","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-env-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"env\"]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: SPECIAL_LEVEL\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#5-create-pod-using-configmap-as-volume","title":"5. Create Pod using ConfigMap as volume","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-volume-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"ls /etc/config &amp;&amp; cat /etc/config/game.properties\"]\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: game-config\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#6-view-configmaps","title":"6. View ConfigMaps","text":"<pre><code># List all ConfigMaps\nkubectl get configmaps\n\n# View details of a specific ConfigMap\nkubectl get configmap my-config -o yaml\nkubectl describe configmap my-config\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#7-update-configmap","title":"7. Update ConfigMap","text":"<pre><code># Edit directly\nkubectl edit configmap my-config\n\n# Replace entirely (using new values)\nkubectl create configmap my-config \\\n  --from-literal=key1=new-value \\\n  --from-literal=key2=new-value2 \\\n  --dry-run=client -o yaml | kubectl replace -f -\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#8-delete-configmap","title":"8. Delete ConfigMap","text":"<pre><code>kubectl delete configmap my-config\n</code></pre> <p>These commands cover all the scenarios mentioned in your original W3H explanation for ConfigMaps. You can run them in sequence to see how ConfigMaps work in Kubernetes.</p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/","title":"CronJobs in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#cronjobs-in-kubernetes-what-why-when-and-how","title":"CronJobs in Kubernetes: What, Why, When, and How","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#1-what-is-a-cronjob","title":"1. What is a CronJob?","text":"<p>A CronJob is a Kubernetes resource that runs scheduled tasks (like <code>cron</code> in Linux) at specified intervals. - It creates one-time pods that execute and terminate after completion. - Works similarly to <code>kubectl run --schedule</code> but with better management.</p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#2-why-use-cronjobs","title":"2. Why Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#key-benefits","title":"Key Benefits","text":"<p>\u2705 Automated Scheduling \u2013 Run tasks at fixed times without manual intervention. \u2705 Kubernetes-Native \u2013 Managed by K8s (retries, logging, scaling). \u2705 Idempotent Operations \u2013 Good for cleanup, backups, reports. \u2705 Failure Recovery \u2013 Can retry failed jobs automatically.  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#use-cases","title":"Use Cases","text":"<ul> <li>Database backups (e.g., daily MySQL dump)</li> <li>Log rotation (compress/delete old logs hourly)</li> <li>Batch processing (nightly report generation)</li> <li>API sync jobs (fetch external data every 5 minutes)</li> <li>Cleanup tasks (delete temporary files weekly)</li> </ul>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#3-when-to-use-cronjobs","title":"3. When to Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#best-for","title":"Best For:","text":"<p>\ud83d\udd52 Recurring tasks (e.g., every hour/day/week) \u26a1 Short-lived jobs (not long-running services) \ud83d\udd27 Maintenance operations (cleanup, backups)  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#avoid-when","title":"Avoid When:","text":"<p>\u274c You need long-running processes (use <code>Deployments</code> instead). \u274c You need per-node execution (use <code>DaemonSet</code> instead). \u274c You need real-time processing (use <code>Kafka</code>/<code>RabbitMQ</code>).  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#4-how-to-use-cronjobs","title":"4. How to Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-backup\nspec:\n  schedule: \"0 2 * * *\"  # Runs at 2 AM daily (cron syntax)\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:latest\n            command: [\"/bin/sh\", \"-c\", \"pg_dump -U user dbname &gt; /backup/db.sql\"]\n            volumeMounts:\n            - name: backup-volume\n              mountPath: /backup\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-volume\n            hostPath:\n              path: /mnt/backups\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#key-fields","title":"Key Fields","text":"Field Purpose <code>schedule</code> Cron syntax (e.g., <code>\"*/5 * * * *\"</code> = every 5 mins) <code>concurrencyPolicy</code> <code>Allow</code> (default), <code>Forbid</code> (skip if previous job runs), <code>Replace</code> (kill old job) <code>startingDeadlineSeconds</code> Max time to start a missed job (e.g., <code>300</code> = 5 mins) <code>successfulJobsHistoryLimit</code> How many completed jobs to keep (default <code>3</code>) <code>failedJobsHistoryLimit</code> How many failed jobs to keep (default <code>1</code>)"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#5-common-cronjob-patterns","title":"5. Common CronJob Patterns","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#1-running-a-script-inside-a-pod","title":"1. Running a Script Inside a Pod","text":"<pre><code>command: [\"/bin/sh\", \"-c\", \"echo 'Hello at $(date)' &gt;&gt; /logs/log.txt\"]\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#2-using-configmapssecrets","title":"2. Using ConfigMaps/Secrets","text":"<pre><code>envFrom:\n- configMapRef:\n    name: backup-config\n- secretRef:\n    name: db-credentials\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#3-handling-retries","title":"3. Handling Retries","text":"<pre><code>backoffLimit: 3  # Retry 3 times before marking as failed\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#4-preventing-overlapping-runs","title":"4. Preventing Overlapping Runs","text":"<pre><code>concurrencyPolicy: Forbid  # Skip new job if old one is running\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#6-monitoring-debugging","title":"6. Monitoring &amp; Debugging","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#check-cronjobs","title":"Check CronJobs","text":"<pre><code>kubectl get cronjobs\nkubectl describe cronjob/daily-backup\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#view-job-logs","title":"View Job Logs","text":"<pre><code>kubectl logs -l job-name=daily-backup-123456\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#manual-trigger-for-testing","title":"Manual Trigger (For Testing)","text":"<pre><code>kubectl create job --from=cronjob/daily-backup manual-run\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#7-alternatives-to-cronjobs","title":"7. Alternatives to CronJobs","text":"Tool When to Use K8s CronJob Best for simple, K8s-native scheduling. Argo Workflows For complex DAG-based jobs. Airflow Advanced scheduling with dependencies. External Cron (e.g., <code>systemd</code>) If you prefer host-level scheduling."},{"location":"CronJob/CronJobs%20in%20Kubernetes/#8-summary","title":"8. Summary","text":"<ul> <li>What? Kubernetes-native scheduled jobs.  </li> <li>Why? Automate recurring tasks (backups, cleanup, sync).  </li> <li>When? For periodic, short-lived jobs (not long-running services).  </li> <li>How? Define <code>schedule</code>, <code>jobTemplate</code>, and optional policies.  </li> </ul>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/","title":"Real World CronJob Example  Automated Database Backup n Cleanup","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik Here's a complete, production-ready example of a Kubernetes CronJob that: 1. Backs up a PostgreSQL database daily at 2 AM 2. Compresses the backup 3. Uploads it to AWS S3 4. Cleans up old backups (older than 7 days)</p>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#1-the-cronjob-definition","title":"1. The CronJob Definition","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: postgres-backup\n  namespace: production\nspec:\n  schedule: \"0 2 * * *\"  # Runs at 2 AM UTC daily\n  concurrencyPolicy: Forbid  # Prevents overlapping jobs\n  successfulJobsHistoryLimit: 3  # Keeps logs of last 3 successes\n  failedJobsHistoryLimit: 5      # Keeps logs of last 5 failures\n  jobTemplate:\n    spec:\n      backoffLimit: 2  # Retries twice on failure\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:14.5\n            env:\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-credentials\n                  key: username\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-credentials\n                  key: password\n            - name: S3_BUCKET\n              value: \"my-db-backups\"\n            - name: AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: access_key\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: secret_key\n            command: \n              - /bin/sh\n              - -c\n              - |\n                # Create timestamped backup filename\n                BACKUP_FILE=\"/backups/db-$(date +%Y-%m-%d-%H%M%S).sql\"\n\n                # Dump database\n                pg_dump -U $POSTGRES_USER -h postgres-service.production.svc.cluster.local -d myapp &gt; $BACKUP_FILE\n\n                # Compress backup\n                gzip $BACKUP_FILE\n\n                # Upload to S3\n                aws s3 cp ${BACKUP_FILE}.gz s3://${S3_BUCKET}/\n\n                # Cleanup local files\n                rm ${BACKUP_FILE}.gz\n\n                # Delete backups older than 7 days from S3\n                aws s3 ls s3://${S3_BUCKET}/ | \\\n                awk '{print $4}' | \\\n                while read -r file; do\n                  file_date=$(echo $file | cut -d'-' -f2-4)\n                  if [[ $(date -d \"$file_date\" +%s) -lt $(date -d \"7 days ago\" +%s) ]]; then\n                    aws s3 rm s3://${S3_BUCKET}/$file\n                  fi\n                done\n            volumeMounts:\n            - name: backup-storage\n              mountPath: /backups\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-storage\n            emptyDir: {}\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#2-required-supporting-resources","title":"2. Required Supporting Resources","text":""},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#postgresql-credentials-secret","title":"PostgreSQL Credentials Secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-credentials\n  namespace: production\ntype: Opaque\ndata:\n  username: base64_encoded_username\n  password: base64_encoded_password\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#aws-s3-credentials-secret","title":"AWS S3 Credentials Secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-credentials\n  namespace: production\ntype: Opaque\ndata:\n  access_key: base64_encoded_access_key\n  secret_key: base64_encoded_secret_key\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#3-key-features-explained","title":"3. Key Features Explained","text":"<ol> <li>Security:</li> <li>Uses Kubernetes Secrets for sensitive data</li> <li> <p>Never stores credentials in the CronJob definition</p> </li> <li> <p>Reliability:</p> </li> <li>Prevents concurrent runs with <code>concurrencyPolicy: Forbid</code></li> <li> <p>Retries failed jobs twice (<code>backoffLimit: 2</code>)</p> </li> <li> <p>Storage Management:</p> </li> <li>Uses <code>emptyDir</code> for temporary storage during backup</li> <li> <p>Automatically cleans up old backups (&gt;7 days) from S3</p> </li> <li> <p>Monitoring:</p> </li> <li>Keeps history of 3 successful and 5 failed runs</li> <li>Easy to check logs with <code>kubectl logs &lt;pod-name&gt;</code></li> </ol>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#4-how-to-deploy-test","title":"4. How to Deploy &amp; Test","text":"<ol> <li> <p>Apply the configuration:    <pre><code>kubectl apply -f postgres-backup-cronjob.yaml\nkubectl apply -f postgres-secret.yaml\nkubectl apply -f aws-secret.yaml\n</code></pre></p> </li> <li> <p>Verify the CronJob:    <pre><code>kubectl get cronjobs -n production\nkubectl describe cronjob/postgres-backup -n production\n</code></pre></p> </li> <li> <p>Manually trigger for testing:    <pre><code>kubectl create job --from=cronjob/postgres-backup manual-test -n production\n</code></pre></p> </li> <li> <p>Check logs:    <pre><code># Get the pod name\nkubectl get pods -n production | grep manual-test\n\n# View logs\nkubectl logs &lt;pod-name&gt; -n production\n</code></pre></p> </li> </ol>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#5-advanced-enhancements","title":"5. Advanced Enhancements","text":"<p>For production environments, consider adding:</p> <ol> <li> <p>Notifications on failure (via Slack/Email):    <pre><code># Add to the container's command:\n- curl -X POST -H 'Content-type: application/json' \\\n  --data '{\"text\":\"Database backup failed!\"}' \\\n  $SLACK_WEBHOOK_URL || true\n</code></pre></p> </li> <li> <p>Prometheus monitoring:    <pre><code>annotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n</code></pre></p> </li> <li> <p>Backup verification:    <pre><code># Add after upload to S3:\naws s3 ls s3://${S3_BUCKET}/$(basename ${BACKUP_FILE}.gz) || \\\n(echo \"Backup verification failed!\" &amp;&amp; exit 1)\n</code></pre></p> </li> </ol> <p>This example provides a complete, production-grade solution for database backups with proper security, reliability, and maintenance features. Would you like any modifications for your specific use case?</p>"},{"location":"CronJob/k8s%20Jobs/","title":"k8s Jobs","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"CronJob/k8s%20Jobs/#1-what-are-kubernetes-jobs","title":"1. WHAT are Kubernetes Jobs?","text":"<p>Definition: A Kubernetes <code>Job</code> creates one or more Pods to run a short-lived, batch-style task (unlike Deployments, which run continuously).  </p>"},{"location":"CronJob/k8s%20Jobs/#types-of-jobs","title":"Types of Jobs:","text":"Type Description Example Use Case One-shot Job Runs a Pod until completion (exit code 0). Database migration, data export. Parallel Job Runs multiple Pods in parallel (e.g., <code>completions: 5</code>). Batch image processing. CronJob Scheduled Jobs (e.g., daily backups). Log cleanup, report generation."},{"location":"CronJob/k8s%20Jobs/#2-why-use-kubernetes-jobs","title":"2. WHY Use Kubernetes Jobs?","text":""},{"location":"CronJob/k8s%20Jobs/#benefits","title":"Benefits:","text":"<p>\u2705 Automation: Replace manual/cron scripts with Kubernetes-managed tasks. \u2705 Resilience: Auto-retries failed Pods (configurable via <code>backoffLimit</code>). \u2705 Resource Efficiency: Runs on existing K8s clusters (no need for separate VMs).  </p>"},{"location":"CronJob/k8s%20Jobs/#real-world-examples","title":"Real-World Examples:","text":"<ul> <li>Netflix: Uses Jobs for encoding video files in parallel.  </li> <li>Airbnb: Runs nightly data warehouse ETL Jobs.  </li> <li>AI/ML: Training models with distributed Jobs (e.g., Kubeflow).  </li> </ul>"},{"location":"CronJob/k8s%20Jobs/#3-who-manages-kubernetes-jobs","title":"3. WHO Manages Kubernetes Jobs?","text":"Role Responsibilities DevOps Engineers Deploy/CronJob setup, error monitoring. Data Engineers Batch processing (Spark, PyTorch Jobs). SREs Ensure Jobs don\u2019t overload the cluster."},{"location":"CronJob/k8s%20Jobs/#4-how-to-run-jobs-effectively","title":"4. HOW to Run Jobs Effectively?","text":""},{"location":"CronJob/k8s%20Jobs/#a-basic-job-example","title":"A. Basic Job Example","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrator\n        image: postgres:14\n        command: [\"psql\", \"-f\", \"/scripts/migrate.sql\"]\n      restartPolicy: Never  # Or \"OnFailure\"\n  backoffLimit: 3          # Retry failed Pods up to 3 times\n</code></pre>"},{"location":"CronJob/k8s%20Jobs/#b-best-practices","title":"B. Best Practices","text":"<ol> <li>Set Resource Limits: Prevent Jobs from hogging cluster CPU/memory. <pre><code>resources:\n  limits:\n    cpu: \"1\"\n    memory: \"512Mi\"\n</code></pre></li> <li>TTL Controller: Auto-delete finished Jobs to save resources. <pre><code>ttlSecondsAfterFinished: 3600  # Delete Job after 1 hour\n</code></pre></li> <li>Use Active Deadlines: Force-fail long-running Jobs. <pre><code>activeDeadlineSeconds: 300     # Timeout after 5 minutes\n</code></pre></li> </ol>"},{"location":"CronJob/k8s%20Jobs/#c-advanced-patterns","title":"C. Advanced Patterns","text":"<ul> <li>DAG Workflows: Use Argo Workflows or Tekton for multi-step Jobs.  </li> <li>GPU Jobs: Schedule Jobs on GPU nodes (e.g., <code>nvidia.com/gpu: 1</code>).  </li> <li>CI/CD Integration: Run integration tests as Jobs in pipelines.  </li> </ul>"},{"location":"CronJob/k8s%20Jobs/#tools-for-managing-jobs","title":"\ud83d\udee0\ufe0f Tools for Managing Jobs:","text":"Tool Purpose Kubectl Basic Job management (<code>kubectl create job</code>). Argo Workflows Orchestrate complex Job DAGs. Volcano Batch scheduling for high-performance Jobs."},{"location":"CronJob/k8s%20Jobs/#common-pitfalls-fixes","title":"\u26a0\ufe0f Common Pitfalls &amp; Fixes","text":"Issue Solution Zombie Jobs (leftover Pods) Use <code>ttlSecondsAfterFinished</code>. Hung Jobs Set <code>activeDeadlineSeconds</code>. Resource Starvation Add PodAntiAffinity/ResourceQuotas."},{"location":"CronJob/k8s%20Jobs/#checklist-for-production-jobs","title":"\ud83d\ude80 Checklist for Production Jobs","text":"<ol> <li>Logging: Ensure Jobs stream logs to centralized tools (e.g., ELK, Datadog).  </li> <li>Monitoring: Alert on Job failures (Prometheus + Grafana).  </li> <li>Security: Run Jobs with least-privilege ServiceAccounts.  </li> <li>Cleanup: Automate Job deletion with TTL.  </li> </ol>"},{"location":"CronJob/k8s%20Jobs/#real-world-example-cronjob-for-daily-reports","title":"Real-World Example: CronJob for Daily Reports","text":"<pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: sales-report\nspec:\n  schedule: \"0 2 * * *\"  # 2 AM daily\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: reporter\n            image: python:3.9\n            command: [\"python\", \"/scripts/generate_report.py\"]\n          restartPolicy: OnFailure\n</code></pre> <p>**Need a deep dive on a specific Job use case (e.g., ML training, CI/CD integration)? **</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/","title":"CronJobs Vs DaemonSets","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>No, CronJobs behave differently from DaemonSets in Kubernetes. Here\u2019s the key difference and how they interact with nodes:</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#1-cronjobs-vs-daemonsets-key-differences","title":"1. CronJobs vs. DaemonSets: Key Differences","text":"Feature CronJob DaemonSet Purpose Runs scheduled tasks (like <code>cron</code> in Linux). Ensures a pod runs on all (or some) nodes. Node Coverage Runs pods on any eligible node (based on scheduling rules). Runs one pod per node (automatically covers all nodes). Use Case Periodic jobs (backups, reports, cleanup). Cluster-wide services (logging, monitoring). Persistence Pods are short-lived (die after completion). Pods stay running (persistent daemons). Example <code>0 * * * *</code> (runs every hour). <code>fluentd</code> (runs on every node forever)."},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#2-will-a-cronjob-run-on-all-nodes","title":"2. Will a CronJob Run on All Nodes?","text":"<p>No, by default, a CronJob does not run on all nodes. Instead: - It schedules pods like a regular Deployment, subject to:   - Node Selectors (<code>nodeSelector</code>)   - Taints/Tolerations (if nodes are tainted)   - Affinity/Anti-Affinity rules</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#example-scenario","title":"Example Scenario","text":"<p>If you create a CronJob: <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hourly-cleanup\nspec:\n  schedule: \"0 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleaner\n            image: busybox\n            command: [\"/bin/sh\", \"-c\", \"echo Cleaning...\"]\n          restartPolicy: OnFailure\n</code></pre> - This will run one pod per scheduled execution (on any available node). - It does not guarantee coverage on all nodes.</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#3-how-to-make-a-cronjob-run-on-all-nodes","title":"3. How to Make a CronJob Run on All Nodes?","text":"<p>If you really need a CronJob to run on every node (like a DaemonSet), you have two options:</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#option-1-use-a-daemonset-with-a-cron-like-container","title":"Option 1: Use a DaemonSet with a Cron-like Container","text":"<p>Instead of a CronJob, use a DaemonSet with a container that runs periodically (e.g., using <code>sleep</code> + <code>while</code> loop).</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-cleaner\nspec:\n  selector:\n    matchLabels:\n      name: node-cleaner\n  template:\n    metadata:\n      labels:\n        name: node-cleaner\n    spec:\n      containers:\n      - name: cleaner\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - \"while true; do echo Cleaning...; sleep 3600; done\" # Runs hourly\n</code></pre>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#option-2-use-a-cronjob-affinity-to-force-multi-node-execution","title":"Option 2: Use a CronJob + Affinity to Force Multi-Node Execution","text":"<p>If you must use a CronJob, you can: 1. Run multiple Jobs in parallel (one per node). 2. Use <code>podAntiAffinity</code> to prevent multiple pods on the same node.</p> <p><pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: per-node-cleanup\nspec:\n  schedule: \"0 * * * *\"\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      parallelism: 10 # Adjust based on node count\n      template:\n        spec:\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: job-name\n                    operator: In\n                    values:\n                    - per-node-cleanup\n                topologyKey: kubernetes.io/hostname\n          containers:\n          - name: cleaner\n            image: busybox\n            command: [\"/bin/sh\", \"-c\", \"echo Cleaning...\"]\n          restartPolicy: OnFailure\n</code></pre> (This is complex and not recommended\u2014DaemonSet is better for per-node tasks.)</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#4-which-should-you-use","title":"4. Which Should You Use?","text":"Use Case Best Choice Run a task periodically on all nodes DaemonSet (with an internal loop) Run a task periodically on any node CronJob Run a persistent service on all nodes DaemonSet"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#5-summary","title":"5. Summary","text":"<ul> <li>CronJobs are for scheduled tasks, not node coverage.</li> <li>DaemonSets ensure per-node execution (better for node-level tasks).</li> <li>If you need both scheduling and per-node execution, use a DaemonSet with a looping container.</li> </ul> <p>Would you like a real-world example (e.g., log rotation on all nodes)? \ud83d\udee0\ufe0f</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/","title":"DaemonSets in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>DaemonSets in Kubernetes:</p> Aspect Details What A DaemonSet ensures that a pod runs on every node (or specific nodes) in a Kubernetes cluster. Why To run background or node-specific services, like log collectors, monitoring agents, or storage plugins, on each node. When Use when a service must be on all nodes, such as for metrics collection (Prometheus node exporter), log aggregation (Fluentd), or network daemons (CNI). How Create a DaemonSet YAML with kind: <code>DaemonSet</code>, target <code>nodeSelector</code> (if needed), and deploy it using <code>kubectl apply -f</code>. <p>Let me know if you want the YAML example. Here's a real-time example of a DaemonSet that deploys Fluentd to collect logs from all nodes and push to a log aggregator:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd:v1.16-1\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n</code></pre> <p>This deploys Fluentd as a pod on every node, even masters (due to toleration), and mounts the host logs for collection.</p> <p>Let me know if you want more such DaemonSet use cases (e.g., node-exporter, antivirus, custom script runner, etc.).</p> <p>No, DaemonSets and Taints/Tolerations are related concepts in Kubernetes but serve different purposes. Let me clarify the difference:</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#1-daemonsets-vs-taintstolerations","title":"1. DaemonSets vs. Taints/Tolerations","text":"Feature DaemonSet Taints &amp; Tolerations Purpose Ensures a pod runs on all (or some) nodes in a cluster. Prevents pods from running on certain nodes unless they explicitly tolerate the taint. Use Case Deploying cluster-wide services (logging, monitoring, networking). Reserving nodes for specific workloads (e.g., GPU nodes for ML workloads). How It Works Automatically schedules pods on new nodes. Nodes repel pods unless the pod has a matching toleration. Example <code>fluentd</code> (logging agent) running on every node. A node with <code>NoSchedule</code> taint only allows pods that tolerate it."},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#2-how-they-work-together","title":"2. How They Work Together","text":"<p>DaemonSets often use tolerations to ensure they can run on tainted nodes (e.g., master nodes).</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#example-daemonset-with-tolerations","title":"Example: DaemonSet with Tolerations","text":"<p><pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  template:\n    spec:\n      tolerations:\n      - key: \"node-role.kubernetes.io/master\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n      containers:\n      - name: my-container\n        image: nginx\n</code></pre> - This DaemonSet will run on all nodes, including those with the <code>NoSchedule</code> taint (like master nodes).</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#3-when-to-use-each","title":"3. When to Use Each?","text":""},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#use-a-daemonset-when","title":"Use a DaemonSet When:","text":"<p>\u2705 You need a pod (e.g., logging agent, monitoring tool) on every node. \u2705 You want Kubernetes to automatically deploy pods when new nodes join.  </p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#use-taintstolerations-when","title":"Use Taints/Tolerations When:","text":"<p>\u2705 You want to restrict certain nodes to only run specific pods (e.g., GPU nodes for AI workloads). \u2705 You need to protect critical nodes (e.g., master nodes) from running arbitrary workloads.  </p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#4-summary","title":"4. Summary","text":"<ul> <li>DaemonSet = \"Run this pod on all (or selected) nodes.\"  </li> <li>Taints/Tolerations = \"Only run pods on this node if they explicitly accept the taint.\"  </li> </ul> <p>They work together\u2014DaemonSets often include tolerations to bypass taints and ensure they run everywhere.  </p>"},{"location":"DailyTasks/Critical%20Thinking/","title":"Critical Thinking","text":""},{"location":"DailyTasks/Critical%20Thinking/#critical-thinking-a-structured-approach","title":"Critical Thinking: A Structured Approach","text":"<p>Critical thinking is the disciplined process of analyzing, evaluating, and synthesizing information to form reasoned judgments. It involves questioning assumptions, identifying biases, and applying logic to solve problems effectively. Below is a breakdown of its core components, methods, and applications.</p>"},{"location":"DailyTasks/Critical%20Thinking/#1-core-principles-of-critical-thinking","title":"1. Core Principles of Critical Thinking","text":"<ol> <li>Clarity </li> <li>Ask: \"Is the information clear and unambiguous?\" </li> <li> <p>Example: Define vague terms like \"success\" or \"fairness\" before debating them.  </p> </li> <li> <p>Accuracy </p> </li> <li>Verify facts with credible sources.  </li> <li> <p>Example: Cross-check statistics before using them in an argument.  </p> </li> <li> <p>Precision </p> </li> <li> <p>Be specific. Avoid generalizations like \"Everyone knows\u2026\"  </p> </li> <li> <p>Relevance </p> </li> <li>Filter out irrelevant information.  </li> <li> <p>Example: In a medical study, ignore anecdotes and focus on clinical data.  </p> </li> <li> <p>Depth </p> </li> <li>Explore underlying causes, not just surface-level symptoms.  </li> <li> <p>Example: Instead of \"The app crashed,\" ask \"Was it due to poor code or server overload?\" </p> </li> <li> <p>Breadth </p> </li> <li>Consider alternative viewpoints.  </li> <li> <p>Example: Debate both pros and cons of AI regulation.  </p> </li> <li> <p>Logic </p> </li> <li>Ensure arguments follow coherent reasoning (no fallacies).  </li> </ol>"},{"location":"DailyTasks/Critical%20Thinking/#2-the-critical-thinking-process","title":"2. The Critical Thinking Process","text":""},{"location":"DailyTasks/Critical%20Thinking/#step-1-identify-the-problem","title":"Step 1: Identify the Problem","text":"<ul> <li>Example: \"Why are our website conversion rates dropping?\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-2-gather-information","title":"Step 2: Gather Information","text":"<ul> <li>Collect data (analytics, user feedback).  </li> <li>Question sources: \"Is this data biased or incomplete?\" </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-3-analyze-assumptions","title":"Step 3: Analyze Assumptions","text":"<ul> <li>Example: \"We assumed users prefer mobile over desktop\u2014is this true?\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-4-evaluate-evidence","title":"Step 4: Evaluate Evidence","text":"<ul> <li>Distinguish correlation from causation.  </li> <li>Example: \"Did the drop start after a design change or an external event?\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-5-consider-alternatives","title":"Step 5: Consider Alternatives","text":"<ul> <li>Brainstorm multiple explanations/solutions.  </li> <li>Example: \"Could it be slow load times or unclear CTAs?\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-6-draw-conclusions","title":"Step 6: Draw Conclusions","text":"<ul> <li>Use deductive/inductive reasoning.  </li> <li>Example: \"A/B testing shows the new button color reduced clicks \u2192 Revert it.\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#step-7-reflect-iterate","title":"Step 7: Reflect &amp; Iterate","text":"<ul> <li>Example: \"Did our solution work? What did we miss?\"  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#3-tools-for-critical-thinking","title":"3. Tools for Critical Thinking","text":""},{"location":"DailyTasks/Critical%20Thinking/#a-socratic-questioning","title":"A. Socratic Questioning","text":"<ul> <li>Clarify: \"What do you mean by\u2026?\" </li> <li>Challenge Assumptions: \"Why do we assume this is true?\" </li> <li>Probe Evidence: \"How do we know this data is reliable?\" </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#b-logical-frameworks","title":"B. Logical Frameworks","text":"<ul> <li>Deductive Reasoning: </li> <li>Premise 1: All humans are mortal.  </li> <li>Premise 2: Socrates is human.  </li> <li> <p>Conclusion: Socrates is mortal.  </p> </li> <li> <p>Inductive Reasoning: </p> </li> <li>Observation: The sun rose every morning in recorded history.  </li> <li>Prediction: The sun will rise tomorrow.  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#c-bias-detection","title":"C. Bias Detection","text":"Bias Example Counteraction Confirmation Bias Ignoring data that contradicts your belief. Seek disconfirming evidence. Dunning-Kruger Effect Overestimating your skill (e.g., \"I\u2019m great at investing!\"). Seek feedback. Anchoring Relying too heavily on the first piece of information. Compare multiple sources."},{"location":"DailyTasks/Critical%20Thinking/#4-critical-thinking-in-real-world-scenarios","title":"4. Critical Thinking in Real-World Scenarios","text":""},{"location":"DailyTasks/Critical%20Thinking/#case-1-business-decision","title":"Case 1: Business Decision","text":"<ul> <li>Problem: \"Should we adopt AI for customer service?\"  </li> <li>Critical Analysis: </li> <li>Pros: Cost savings, 24/7 support.  </li> <li>Cons: Risk of alienating human-touch customers.  </li> <li>Assumptions: \"AI will handle all queries perfectly.\" (Test this!)  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#case-2-media-literacy","title":"Case 2: Media Literacy","text":"<ul> <li>Headline: \"Study says coffee causes cancer.\"  </li> <li>Questions to Ask: </li> <li>Who funded the study?  </li> <li>Was it correlational or causal?  </li> <li>Sample size?  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#5-barriers-to-critical-thinking","title":"5. Barriers to Critical Thinking","text":"<ol> <li>Emotional Reasoning (\"I feel this is true, so it must be.\")  </li> <li>Groupthink (Following the crowd without questioning.)  </li> <li>Cognitive Laziness (Accepting the first answer without analysis.)  </li> </ol>"},{"location":"DailyTasks/Critical%20Thinking/#6-how-to-improve-critical-thinking","title":"6. How to Improve Critical Thinking","text":"<ul> <li>Practice: Solve puzzles, debate, play strategy games (e.g., chess).  </li> <li>Read Widely: Engage with diverse perspectives.  </li> <li>Write: Articulating thoughts exposes flaws in reasoning.  </li> <li>Use Frameworks: SWOT analysis, Pro/Con lists, Root Cause Analysis (5 Whys).  </li> </ul>"},{"location":"DailyTasks/Critical%20Thinking/#key-takeaway","title":"Key Takeaway","text":"<p>Critical thinking isn\u2019t about being \"smarter\"\u2014it\u2019s about being disciplined, curious, and open to revising your views. It\u2019s the antidote to misinformation, poor decisions, and superficial judgments.  </p> <p>Want to apply this to a specific problem? Share a scenario, and I\u2019ll help you break it down!</p>"},{"location":"DailyTasks/Failures%20with%20solution/","title":"Failures with solution","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"DailyTasks/Failures%20with%20solution/#cicd-sync-failures-misconfigurations","title":"\ud83d\udd01 CI/CD Sync Failures &amp; Misconfigurations","text":"Problem Why It Happens Impact Solution RBAC conflict ArgoCD lacks necessary roles on target namespaces Sync fails, app won\u2019t deploy Bind ArgoCD SA (<code>argocd-application-controller</code>) to <code>edit</code> role in target NS SA issues ArgoCD SA not authorized to manage cluster resources Permission denied errors Ensure correct RBAC roles on both ArgoCD and app namespaces Sync failures Manual drifts or cluster rejection (e.g., quota, SCC) Cluster state diverges from Git Enable auto-prune, fix invalid manifests, use <code>argocd app diff</code> Health check stuck ArgoCD doesn\u2019t recognize app\u2019s health unless customized App shows <code>Degraded</code> despite being healthy Add custom health checks via ArgoCD app annotations ### \u2638\ufe0f Kubernetes/OpenShift Compatibility Problem Why It Happens Impact Solution Ingress vs Route mismatch OpenShift uses <code>Route</code>, not <code>Ingress</code> App not accessible externally Use OpenShift Route CRDs instead of generic Ingress Resource limit enforcement OpenShift applies strict quota and limit ranges Pod scheduling fails, app crashes Add <code>resources.requests</code> and <code>resources.limits</code> in manifests CrashLoopBackOff Missing config maps, secrets, or bad probes App stuck restarting Use <code>kubectl describe pod</code> + logs to find actual cause SCC violations OpenShift enforces Security Context Constraints Pod rejected by admission controller Create custom SCC, bind to ArgoCD or app-specific SA ArgoCD pod blocked SCC or PSP restricts container permissions (e.g., root user) ArgoCD can\u2019t run apps with elevated needs Use or create relaxed SCC for ArgoCD workloads ### \ud83d\udd10 Secrets Management Problem Why It Happens Impact Solution Secret mismatch in NS Kubernetes secrets are namespace-scoped ImagePull or app config fails Ensure secrets exist and are correctly named in the app's namespace Secrets in Git Developers commit <code>.env</code> or YAML with sensitive data Security risk (credentials exposed) Use Bitnami SealedSecrets, SOPS, or Vault with ArgoCD plugin Secret sync errors Secrets are managed manually or by external systems Drift between Git and actual state Use GitOps-compatible secret generators or templates ### \ud83d\udee0\ufe0f Observability &amp; Debugging Problem Why It Happens Impact Solution ArgoCD sync status unknown Missing status info or failed sync hidden Operators can't track deployment status Use <code>argocd app sync</code>, <code>app wait</code>, and <code>argocd app history</code> Logs missing or incomplete Log aggregation not enabled or ArgoCD logs not visible Hard to troubleshoot Use <code>argocd app logs</code>, <code>kubectl logs</code>, or OpenShift console Manifests differ Rendered manifests not previewed Unexpected cluster state Use <code>argocd app manifests</code> and <code>argocd app diff</code> to compare ### \ud83c\udf10 Networking &amp; Access Problem Why It Happens Impact Solution ArgoCD UI not accessible Service not exposed via OpenShift Route Cannot access ArgoCD Web UI Use <code>oc expose svc/argocd-server -n argocd</code> Ingress conflict with Route Ingress.yaml conflicts with OpenShift Route behavior Route override doesn\u2019t work Prefer Route CRD in manifests over Ingress ArgoCD login errors OAuth not integrated Users must use local admin password Use Dex with OpenShift OIDC or integrate with OpenShift OAuth provider ### \ud83d\udcb0 Resource Optimization Problem Why It Happens Impact Solution Orphaned resources GitOps deletes missing resources only if auto-prune is enabled Resource sprawl, cost waste Enable <code>--auto-prune</code> on ArgoCD apps Overprovisioned resources Developers set excessive CPU/memory in manifests Waste of capacity or throttling Use <code>oc top pods</code> and tune HPA/limits accordingly YAML duplication No abstraction across environments Difficult to manage &amp; scale manifests Use Kustomize or Helm with overlays per environment ### \ud83d\udc65 Collaboration &amp; Governance Problem Why It Happens Impact Solution No environment isolation ArgoCD controls multiple environments with no boundaries Accidental deploys to wrong namespaces Use ArgoCD Projects to isolate apps, namespaces, repos Manual change override Cluster modified outside of GitOps ArgoCD status out of sync Enforce auto-sync with manual sync disabling + audit policy Messy Git repo No template abstraction, manual duplication Error-prone, hard to scale Use Helm, Kustomize, or Jsonnet for reusable manifests"},{"location":"DailyTasks/Important%20resources%20to%20learn%20k8s/","title":"Important resources to learn k8s","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here\u2019s a curated list of practical, real-time, deep-dive Kubernetes learning websites with hands-on labs, architecture, and troubleshooting-focused content \u2014 great for DevOps/SRE/admin roles:</p> \ud83d\udd22 Website Focus Area Highlights 1 Learnk8s.io Production-level design Real-world patterns, architecture, troubleshooting, scaling strategies 2 KodeKloud.com Hands-on labs Real-time practice labs, CKA/CKAD prep, mock scenarios 3 Katacoda (via O\u2019Reilly) (still usable via O\u2019Reilly) Interactive scenarios In-browser terminal labs for networking, operators, Helm, etc. 4 Kubernetes By Example Deep-dive use cases CNCF-backed, scenarios include observability, CI/CD, Helm, RBAC 5 Play with Kubernetes Quick test lab Launch a full lab with CLI &amp; dashboard in browser 6 Kelsey Hightower\u2019s Kubernetes The Hard Way Manual setup Pure infrastructure-level deep-dive using GCP \u2014 gold standard for understanding 7 Banzai Cloud Blog Real-time infra cases Advanced concepts like service mesh, policy, multi-cluster 8 Platform9 Blog Troubleshooting &amp; ops Real-world issues with CSI, RBAC, HA, kubeadm, upgrades 9 KubebyExample.io How-tos with live code Helm, GitOps, Kustomize, Service Mesh \u2014 updated examples 10 Kubelabs.dev Tasks + challenges Practice real tasks like HPA, autoscaler, pod disruption policies \ud83d\udd22 Website What You Learn Highlights 1 Kubernetes By Example RBAC, Helm, Network Policies, Metrics, Ingress CNCF-backed, practical labs, real cluster examples 2 Play with Kubernetes CLI practice, Cluster setup, PVCs, Services Free browser terminal to test real K8s scenarios 3 Kubelabs.dev HPA, Liveness, PDBs, CronJobs, ConfigMaps Short tasks with YAML + command challenges 4 Kelsey Hightower\u2019s Kubernetes The Hard Way Manual cluster setup, TLS, kubeadm Full infra understanding via CLI on GCP (manual setup) 5 Learnk8s Tutorials Ingress, Canary, Load Balancing, Helm Architecture-level explanations with diagrams 6 Platform9 Blog CSI, kubeadm HA, cluster upgrades, CNI Deep troubleshooting and enterprise-level insights 7 Banzai Cloud Blog CRDs, Operator pattern, multi-cluster, Istio DevOps/SRE-grade content, cloud-native stack integration"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/","title":"K8s Cmds Cheatsheet","text":"<p>Created: 2025-06-23 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#cluster-management","title":"Cluster Management","text":"<ul> <li><code>k cluster-info</code> - Display cluster information</li> <li><code>k get nodes</code> - List all nodes in the cluster</li> <li><code>k describe node &lt;node-name&gt;</code> - Show detailed information about a specific node</li> <li><code>k top node</code> - Show resource usage (CPU/Memory) for nodes</li> <li><code>k top pod</code> - Show resource usage for pods</li> <li><code>k cordon &lt;node-name&gt;</code> - Mark node as unschedulable</li> <li><code>k uncordon &lt;node-name&gt;</code> - Mark node as schedulable</li> <li><code>k drain &lt;node-name&gt;</code> - Drain node in preparation for maintenance</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#apply-and-remove-labels","title":"Apply and Remove Labels","text":"<ul> <li><code>k label &lt;resource-type&gt; &lt;resource-name&gt; &lt;key&gt;=&lt;value&gt; --overwrite</code></li> <li><code>k label pod mypod app=nginx --overwrite</code> - apply label</li> <li> <p><code>k label namespace namespace-b name=namespace-b</code> </p> </li> <li> <p><code>k label &lt;resource-type&gt; &lt;resource-name&gt; &lt;key&gt;-</code></p> </li> <li><code>k label pod mypod app-</code> - Remove label</li> <li><code>k label node &lt;node-name&gt; node-role.kubernetes.io/infra=\"\"</code> - Label a node as an infra node</li> <li><code>k get nodes --show-labels</code> - to see all labels on nodes(specific resources)\\</li> <li></li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#apply-a-taint-to-an-infra-node","title":"Apply a taint to an infra node","text":"<ul> <li><code>k adm taint node &lt;node-name&gt; node-role.kubernetes.io/infra:NoSchedule</code> - apply taint on infra node</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#namespace-operations","title":"Namespace Operations","text":"<ul> <li><code>k get namespaces</code> - List all namespaces</li> <li><code>k create namespace &lt;namespace-name&gt;</code> - Create a new namespace</li> <li><code>k config set-context --current --namespace=&lt;namespace-name&gt;</code> - Set default namespace for current context</li> <li><code>k delete namespace &lt;namespace-name&gt;</code> - Delete a namespace</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#node-role-labels","title":"Node-role labels","text":""},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#apply-node-role-label","title":"Apply node-role label","text":"<ul> <li>`k label node k3d-dev-cluster-agent-1 node-role.kubernetes.io/worker1=</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#rmove-node-role-label","title":"Rmove node-role label","text":"<ul> <li><code>k label node k3d-dev-cluster-agent-1 node-role.kubernetes.io/worker1-</code></li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#labels","title":"Labels","text":"Command / Usage Description <code>kubectl get pods --show-labels</code> Show all labels for each pod <code>kubectl get pods -l key=value</code> Filter pods by label <code>kubectl get pods -l 'key in (v1,v2)'</code> Filter by label values (set-based) <code>kubectl label pod &lt;pod-name&gt; key=value</code> Add a label to a pod <code>kubectl label pod &lt;pod-name&gt; key-</code> Remove a label from a pod <code>kubectl label --overwrite pod key=value</code> Overwrite an existing label <code>kubectl get pods --selector='env=prod'</code> Another way to filter using a label selector <code>kubectl describe pod &lt;pod-name&gt;</code> View labels (under Metadata) <code>kubectl get nodes --show-labels</code> Show labels on nodes <code>kubectl label nodes &lt;node-name&gt; key=value</code> Add a label to a node <code>kubectl label deployment &lt;dep&gt; key=value</code> Add label to deployment <code>kubectl get svc -l app=myapp</code> Get services with specific label <code>kubectl delete pod -l env=test</code> Delete pods by label <code>kubectl apply -f file.yaml</code> Apply resource with labels defined in YAML"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#annotations","title":"Annotations","text":"<ul> <li><code>k annotate pod mypod key=value</code></li> <li><code>k annotate pod mypod owner=\"gowse\" purpose=\"debugging\"</code> - apply annotation</li> <li><code>k annotate pod mypod purpose-</code> - Remove annotation</li> <li><code>kubectl get pods -A -o json | jq -r '.items[] | select(.metadata.annotations[\"w3h.why\"] == \"debugging\") | [.metadata.namespace, .metadata.name] | @tsv'</code></li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#pod-operations","title":"Pod Operations","text":"<ul> <li><code>k get pods</code> - List all pods in current namespace</li> <li><code>k get pods -A</code> - List all pods in all namespaces</li> <li><code>k get pods -o wide</code> - List pods with additional details (IP, node)</li> <li><code>k describe pod &lt;pod-name&gt;</code> - Show detailed information about a pod</li> <li><code>k logs &lt;pod-name&gt;</code> - Print pod logs</li> <li><code>k logs -f &lt;pod-name&gt;</code> - Stream pod logs (follow)</li> <li><code>k logs &lt;pod-name&gt; -c &lt;container-name&gt;</code> - Print logs from a specific container in a pod</li> <li><code>k exec -it &lt;pod-name&gt; -- /bin/bash</code> - Execute a command in a pod (interactive)</li> <li><code>k delete pod &lt;pod-name&gt;</code> - Delete a pod</li> <li><code>k port-forward &lt;pod-name&gt; &lt;local-port&gt;:&lt;pod-port&gt;</code> - Forward a local port to a pod</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#deployment-operations","title":"Deployment Operations","text":"<ul> <li><code>k get deployments</code> - List all deployments</li> <li><code>k describe deployment &lt;deployment-name&gt;</code> - Show deployment details</li> <li><code>k create deployment &lt;name&gt; --image=&lt;image&gt;</code> - Create a deployment</li> <li><code>k scale deployment &lt;deployment-name&gt; --replicas=&lt;number&gt;</code> - Scale a deployment</li> <li><code>k rollout status deployment/&lt;deployment-name&gt;</code> - Check rollout status</li> <li><code>k rollout history deployment/&lt;deployment-name&gt;</code> - View rollout history</li> <li><code>k rollout undo deployment/&lt;deployment-name&gt;</code> - Rollback to previous version</li> <li><code>k rollout undo deployment/&lt;deployment-name&gt; --to-revision=&lt;number&gt;</code> - Rollback to specific revision</li> <li><code>k set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image&gt;</code> - Update deployment image</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#service-operations","title":"Service Operations","text":"<ul> <li><code>k get services</code> - List all services</li> <li><code>k describe service &lt;service-name&gt;</code> - Show service details</li> <li><code>k expose deployment &lt;deployment-name&gt; --port=&lt;port&gt; --target-port=&lt;target-port&gt; --type=&lt;type&gt;</code> - Expose a deployment as a service</li> <li><code>k delete service &lt;service-name&gt;</code> - Delete a service</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#configmap-secrets","title":"ConfigMap &amp; Secrets","text":"<ul> <li><code>k get configmaps</code> - List all configmaps</li> <li><code>k create configmap &lt;name&gt; --from-file=&lt;path-to-file&gt;</code> - Create configmap from file</li> <li><code>k create configmap &lt;name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;</code> - Create configmap from literal</li> <li><code>k get secrets</code> - List all secrets</li> <li><code>k create secret generic &lt;name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;</code> - Create secret from literal</li> <li><code>k create secret generic &lt;name&gt; --from-file=&lt;path-to-file&gt;</code> - Create secret from file</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#statefulsets-daemonsets","title":"StatefulSets &amp; DaemonSets","text":"<ul> <li><code>k get statefulsets</code> - List all statefulsets</li> <li><code>k get daemonsets</code> - List all daemonsets</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#jobs-cronjobs","title":"Jobs &amp; CronJobs","text":"<ul> <li><code>k get jobs</code> - List all jobs</li> <li><code>k get cronjobs</code> - List all cronjobs</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#persistent-volumes-claims","title":"Persistent Volumes &amp; Claims","text":"<ul> <li><code>k get pv</code> - List persistent volumes</li> <li><code>k get pvc</code> - List persistent volume claims</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#rbac-security","title":"RBAC &amp; Security","text":"<ul> <li><code>k get roles</code> - List roles</li> <li><code>k get rolebindings</code> - List role bindings</li> <li><code>k get clusterroles</code> - List cluster roles</li> <li><code>k get clusterrolebindings</code> - List cluster role bindings</li> <li><code>k auth can-i &lt;verb&gt; &lt;resource&gt;</code> - Check if an action is allowed</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#troubleshooting-debugging","title":"Troubleshooting &amp; Debugging","text":"<ul> <li><code>k get events --sort-by=.metadata.creationTimestamp</code> - Show events sorted by timestamp</li> <li><code>k get events -w</code> - Watch events in real-time</li> <li><code>k api-resources</code> - List all API resources</li> <li><code>k explain &lt;resource&gt;</code> - Get documentation for a resource</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#configuration-context","title":"Configuration &amp; Context","text":"<ul> <li><code>k config view</code> - Show merged kubeconfig settings</li> <li><code>k config get-contexts</code> - List all contexts</li> <li><code>k config use-context &lt;context-name&gt;</code> - Switch to another context</li> <li><code>k config current-context</code> - Show current context</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#yaml-operations","title":"YAML Operations","text":"<ul> <li><code>k apply -f &lt;file.yaml&gt;</code> - Apply configuration from a YAML file</li> <li><code>k delete -f &lt;file.yaml&gt;</code> - Delete resources defined in a YAML file</li> <li><code>k get &lt;resource&gt; &lt;name&gt; -o yaml</code> - Get resource configuration in YAML format</li> <li><code>k get &lt;resource&gt; &lt;name&gt; -o json</code> - Get resource configuration in JSON format</li> </ul>"},{"location":"DailyTasks/K8s%20Cmds%20Cheatsheet/#custom-columns-output-formatting","title":"Custom Columns &amp; Output Formatting","text":"<ul> <li><code>k get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName</code> - Custom columns output</li> <li><code>k get pods --sort-by=.metadata.creationTimestamp</code> - Sort by creation time</li> <li><code>k get pods --field-selector=status.phase=Running</code> - Filter by field selector</li> </ul>"},{"location":"DailyTasks/Keywords/","title":"Keywords","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"DailyTasks/Keywords/#qdprc","title":"QDPRC:","text":"<ul> <li>Question: Start by framing the right question to ensure focus.</li> <li>Data: Collect objective information to support analysis. (.csv file)</li> <li>Principles: Use established laws, theories, or ethical guidelines.</li> <li>Reasoning: Apply deductive or inductive logic to connect data and principles.</li> <li> <p>Conclusion: Arrive at a justified answer or decision.</p> </li> <li> <p>GPT (Generative Pre-trained Transformer) is a family of large language models (LLMs) developed by OpenAI</p> </li> <li>data will be in the format of Text, video, Audio formats</li> <li>first principle</li> <li>biases (miscommunication)</li> <li>[[Critical Thinking]] (thinks about big problems)</li> <li>Pre-trained</li> <li>Questioning</li> <li></li> </ul> Model Creator Key Difference GPT OpenAI Best at creative text generation. BERT Google Better for search &amp; classification. Claude Anthropic Focuses on safety &amp; reasoning. Llama Meta Open-source alternative. <p>\u2705 Text Generation (articles, stories, emails) \u2705 Code Writing &amp; Debugging (Python, JavaScript, etc.) \u2705 Translation &amp; Summarization \u2705 Chatbots &amp; Virtual Assistants (like ChatGPT) \u2705 Answering Questions (but may hallucinate facts)</p>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/","title":"Usecases for ConfigMaps and Secrets","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#1-nginx-with-custom-config-from-configmap","title":"\u2705 1. NGINX with Custom Config from ConfigMap","text":"<p>Use Case: Inject a custom NGINX config via ConfigMap.</p> <ul> <li>What to develop: Pod running NGINX, config file (nginx.conf) mounted from ConfigMap.</li> <li>What to test: Change the ConfigMap, reload the pod, and see config take effect.</li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#2-python-flask-app-reading-config-via-env-configmap","title":"\u2705 2. Python Flask App Reading Config via ENV (ConfigMap)","text":"<p>Use Case: Pass app config (e.g., <code>APP_MODE=dev</code>) via environment variables.</p> <ul> <li>What to develop: Flask app that reads <code>APP_MODE</code> from <code>os.environ</code>.</li> <li>What to test: Update ConfigMap \u2192 redeploy \u2192 check changed behavior.</li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#3-mysql-pod-with-password-from-secret","title":"\u2705 3. MySQL Pod with Password from Secret","text":"<p>Use Case: Inject DB password using Kubernetes Secrets.</p> <ul> <li>What to develop: Pod running MySQL, <code>MYSQL_ROOT_PASSWORD</code> from a Secret.</li> <li>What to test: Try to connect using the injected password.</li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#4-nodejs-app-reading-db-credentials-configmap-secret","title":"\u2705 4. Node.js App Reading DB Credentials (ConfigMap + Secret)","text":"<p>Use Case: Use both ConfigMap and Secret.</p> <ul> <li>ConfigMap \u2192 DB Host/Port</li> <li>Secret \u2192 DB username &amp; password</li> <li>What to test: Print connection config in logs, verify secure loading.</li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#5-spring-boot-app-with-externalized-config","title":"\u2705 5. Spring Boot App with Externalized Config","text":"<p>Use Case: Use <code>application.properties</code> from a ConfigMap.</p> <ul> <li>Mount as file to <code>/config/application.properties</code></li> <li>App reads from external config path</li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#6-static-web-app-reactangular-using-configmap-for-api-url","title":"\u2705 6. Static Web App (React/Angular) Using ConfigMap for API URL","text":"<p>Use Case: Inject dynamic API endpoint using ConfigMap.</p> <ul> <li>Mount JSON file or inject via ENV at build/start</li> <li>Rebuild only when API endpoint changes</li> </ul> <p>we can go with an example of #4: Node.js App reading DB config from ConfigMap and Secret.</p>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#app-overview","title":"\ud83d\udce6 App Overview","text":"<ul> <li>Simple Node.js Express app</li> <li> <p>Reads DB config from:</p> <ul> <li><code>ConfigMap</code> \u2192 <code>DB_HOST</code>, <code>DB_PORT</code></li> <li><code>Secret</code> \u2192 <code>DB_USER</code>, <code>DB_PASS</code></li> </ul> </li> <li> <p>Prints values on <code>/config</code> endpoint</p> </li> </ul>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#1-appjs","title":"\ud83d\udcc1 1. app.js","text":"<pre><code>const express = require('express');\nconst app = express();\n\nconst config = {\n  host: process.env.DB_HOST || 'localhost',\n  port: process.env.DB_PORT || '5432',\n  user: process.env.DB_USER || 'user',\n  pass: process.env.DB_PASS || 'pass',\n};\n\napp.get('/config', (req, res) =&gt; {\n  res.json(config);\n});\n\napp.listen(3000, () =&gt; {\n  console.log(`App running on port 3000`);\n});\n</code></pre> <p>Explanation of above code. <pre><code>// Load the Express.js library to create a web server\nconst express = require('express');\n\n// Create an instance of the Express application\nconst app = express();\n\n// Define a config object that reads values from environment variables\n// If a variable is not set, it uses a default fallback value\nconst config = {\n  host: process.env.DB_HOST || 'localhost',     // DB host from ConfigMap or fallback to 'localhost'\n  port: process.env.DB_PORT || '5432',          // DB port from ConfigMap or fallback to '5432'\n  user: process.env.DB_USER || 'user',          // DB username from Secret or fallback to 'user'\n  pass: process.env.DB_PASS || 'pass',          // DB password from Secret or fallback to 'pass'\n};\n\n// Define a GET endpoint '/config' to return the config values as JSON\napp.get('/config', (req, res) =&gt; {\n  res.json(config); // Send the config object as a JSON response\n});\n\n// Start the Express server on port 3000\napp.listen(3000, () =&gt; {\n  console.log(`App running on port 3000`); // Log a message when the server starts\n});\n</code></pre></p>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#2-dockerfile","title":"\ud83d\udc33 2. Dockerfile","text":"<pre><code>FROM node:18-alpine\nWORKDIR /app\nCOPY app.js .\nRUN npm init -y &amp;&amp; npm install express\nCMD [\"node\", \"app.js\"]\n</code></pre>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#3-kubernetes-yamls","title":"\ud83d\udee0\ufe0f 3. Kubernetes YAMLs","text":""},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#configmapyaml","title":"\ud83d\udcd8 <code>configmap.yaml</code>","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: db-config\ndata:\n  DB_HOST: postgres.dev.svc.cluster.local\n  DB_PORT: \"5432\"\n</code></pre>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#secretyaml","title":"\ud83d\udd10 <code>secret.yaml</code>","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ntype: Opaque\nstringData:\n  DB_USER: admin\n  DB_PASS: mysecretpass\n</code></pre>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#deploymentyaml","title":"\ud83d\ude80 <code>deployment.yaml</code>","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-config-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-config-app\n  template:\n    metadata:\n      labels:\n        app: node-config-app\n    spec:\n      containers:\n        - name: app\n          image: node-config-app:latest\n          ports:\n            - containerPort: 3000\n          env:\n            - name: DB_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: db-config\n                  key: DB_HOST\n            - name: DB_PORT\n              valueFrom:\n                configMapKeyRef:\n                  name: db-config\n                  key: DB_PORT\n            - name: DB_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-secret\n                  key: DB_USER\n            - name: DB_PASS\n              valueFrom:\n                secretKeyRef:\n                  name: db-secret\n                  key: DB_PASS\n</code></pre>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#4-service-optional","title":"\ud83c\udf10 4. Service (Optional)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: node-config-svc\nspec:\n  selector:\n    app: node-config-app\n  ports:\n    - port: 80\n      targetPort: 3000\n</code></pre>"},{"location":"DailyTasks/Usecases%20for%20ConfigMaps%20and%20Secrets/#test","title":"\ud83e\uddea Test","text":"<pre><code>kubectl apply -f configmap.yaml\nkubectl apply -f secret.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n\n# Port forward to test locally\nkubectl port-forward svc/node-config-svc 8080:80\n\n# Then open http://localhost:8080/config\n</code></pre>"},{"location":"DailyTasks/k8s%20error%20codes/","title":"K8s error codes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"DailyTasks/k8s%20error%20codes/#http-status-codes","title":"HTTP Status Codes","text":"Code Name Description 400 Bad Request Invalid request format or parameters 401 Unauthorized Authentication required 403 Forbidden Credentials accepted but not authorized 404 Not Found Resource doesn't exist 409 Conflict Request conflicts with current state 422 Unprocessable Entity Valid request but semantic errors 429 Too Many Requests Rate limiting applied 500 Internal Server Error Unexpected server error 503 Service Unavailable Service not available"},{"location":"DailyTasks/k8s%20error%20codes/#common-podcontainer-errors","title":"Common Pod/Container Errors","text":"Error Code/Message Description CrashLoopBackOff Container crashes repeatedly ImagePullBackOff Cannot pull container image ErrImagePull General image pull error ImageInspectError Cannot inspect container image ErrNoFreePorts No free ports available CreateContainerConfigError Problem with config (secrets, volumes) CreateContainerError General container creation error RunContainerError General container runtime error OOMKilled Out of Memory killed container ContainerCreating Stuck in creation phase Terminating Stuck in deletion phase Pending Not scheduled yet Evicted Node resource pressure FailedMount Volume mounting failed FailedScheduling Scheduler cannot place pod"},{"location":"DailyTasks/k8s%20error%20codes/#node-conditions","title":"Node Conditions","text":"Condition Description Ready Node is healthy MemoryPressure Node memory is low DiskPressure Node disk space is low PIDPressure Too many processes NetworkUnavailable Network not configured OutOfDisk Node out of disk space NotReady Node not healthy"},{"location":"DailyTasks/k8s%20error%20codes/#common-kubectl-errors","title":"Common kubectl Errors","text":"Error Description \"connection refused\" Cannot connect to API server \"context deadline exceeded\" Request timeout \"the server doesn't have a resource type\" Resource type doesn't exist \"forbidden: User cannot...\" RBAC permission issue \"no matches for kind\" API version mismatch \"resource name may not be empty\" Missing resource name \"already exists\" Resource name conflict"},{"location":"DailyTasks/k8s%20error%20codes/#network-errors","title":"Network Errors","text":"Error Description NetworkPluginNotReady CNI plugin not ready FailedCreatePodSandBox Container runtime network issue FailedPortMapping Port mapping failed DNSConfigForming DNS configuration error NetworkNotReady General network issue"},{"location":"DailyTasks/k8s%20error%20codes/#storage-errors","title":"Storage Errors","text":"Error Description FailedAttachVolume Cannot attach volume FailedMount Cannot mount volume FailedUnMount Cannot unmount volume VolumeResizeFailed Cannot resize volume VolumeAlreadyExists Volume name conflict ProvisioningFailed Storage provisioning failed"},{"location":"DailyTasks/k8s%20error%20codes/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<ol> <li>Check pod details: <code>kubectl describe pod &lt;pod-name&gt;</code></li> <li>Check pod logs: <code>kubectl logs &lt;pod-name&gt; [-c container]</code></li> <li>Check events: <code>kubectl get events --sort-by=.metadata.creationTimestamp</code></li> <li>Check node status: <code>kubectl get nodes -o wide</code></li> <li>Check component status: <code>kubectl get cs</code></li> <li>Check API health: <code>kubectl get --raw /healthz</code></li> <li>Check API versions: <code>kubectl api-versions</code></li> </ol> <p>Remember that many errors will include additional details in their status messages that can help pinpoint the exact issue.</p>"},{"location":"DailyTasks/pod%20exec%20and%20extract%20info%20from%20pods/","title":"Pod exec and extract info from pods","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># For Shell\noc exec -it myingress-68dc769f7f-8cbpp -- sh\n\n# to run some command and sub commands\noc exec -it ibmmq-68dc769f7f-8cbpp -c qmgr -- cat /etc/mqm/qm.ini\noc exec -it ibmmq-68dc769f7f-8cbpp -- runmqsc &lt; mq.mqsc.rollback\noc exec -it myingress-68dc769f7f-8cbpp -- ls /path/to/dir\n\n# for multiple paths\noc exec -it myingress-68dc769f7f-8cbpp -- ls -ltr /home/user/somepath/onepath /home/user/somepath/twopath\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/","title":"Deployments Strategies","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here's a table summarizing Deployment Strategies with the What, Why, When, and How (W3H) breakdown:  </p> Strategy What? Why? When? How? Rolling Update (Default) Gradually replaces old Pods with new ones in phases. Ensures zero downtime during updates. When high availability is critical (e.g., production environments). Controlled via <code>maxUnavailable</code> (how many Pods can be down) and <code>maxSurge</code> (how many extra Pods can be created). Recreate Terminates all old Pods before creating new ones. Ensures only one version runs at a time. When the app cannot handle multiple versions running simultaneously (e.g., database schema changes). Set <code>strategy.type: Recreate</code> in the Deployment spec. Blue-Green Deploys a new version alongside the old one, then switches traffic at once. Minimizes risk and allows instant rollback. When testing new versions before full release (e.g., canary testing). Requires two identical environments (Deployments + Services). Switch traffic via Service selector update. Canary Slowly rolls out a new version to a subset of users. Reduces risk by testing in production with real users. When gradual validation is needed (e.g., A/B testing). Use labels and selectors to route a portion of traffic to the new version. A/B Testing Routes traffic based on user attributes (headers, cookies). Tests different features with real users. When comparing multiple versions for performance or UX. Requires Service Mesh (Istio, Linkerd) or Ingress controllers (Nginx, Traefik). Type Description Manual Done by developer/admin, not automated Automated (CI/CD) Triggered by code pushes using Jenkins, GitHub Actions, etc. Blue-Green Two environments \u2013 switch traffic between them to avoid downtime Canary Deploy to small % of users first, then gradually roll out Rolling Update instances gradually, one at a time Recreate Stop old version completely, then deploy new one A/B Testing Serve different app versions to test user response"},{"location":"Deployments/Deployments%20Strategies/#example-commands-for-strategies","title":"Example Commands for Strategies","text":""},{"location":"Deployments/Deployments%20Strategies/#rolling-update-default","title":"Rolling Update (Default)","text":"<pre><code>spec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1  # Only 1 Pod down at a time\n      maxSurge: 1        # Only 1 extra Pod created\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#recreate-strategy","title":"Recreate Strategy","text":"<pre><code>spec:\n  strategy:\n    type: Recreate\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#blue-green-deployment-manual-switch","title":"Blue-Green Deployment (Manual Switch)","text":"<pre><code># Deploy v1 (blue)\nkubectl apply -f blue-deployment.yaml\n\n# Deploy v2 (green)\nkubectl apply -f green-deployment.yaml\n\n# Switch traffic from blue to green\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#canary-deployment-partial-traffic","title":"Canary Deployment (Partial Traffic)","text":"<pre><code># Scale canary to 10% of traffic\nkubectl scale deployment myapp-canary --replicas=2  # If main has 18 replicas (10%)\n</code></pre> <p>This table helps choose the right strategy based on uptime requirements, risk tolerance, and testing needs. \ud83d\ude80</p>"},{"location":"Deployments/Deployments%20in%20kubernetes/","title":"Deployments in kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Deployments/Deployments%20in%20kubernetes/#deployments-in-kubernetes-a-comprehensive-guide","title":"Deployments in Kubernetes: A Comprehensive Guide","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#what-are-deployments","title":"What are Deployments?","text":"<p>Deployments are Kubernetes objects that provide declarative updates for Pods and ReplicaSets. They allow you to describe an application's life cycle, including which images to use, the number of pods, and how to update them.</p>"},{"location":"Deployments/Deployments%20in%20kubernetes/#why-use-deployments","title":"Why Use Deployments?","text":"<ol> <li>Rolling updates: Update applications with zero downtime</li> <li>Rollback capability: Revert to previous versions if something goes wrong</li> <li>Scaling: Easily scale your application up or down</li> <li>Self-healing: Automatically replaces failed pods</li> <li>Version tracking: Maintain deployment history</li> </ol>"},{"location":"Deployments/Deployments%20in%20kubernetes/#when-to-use-deployments","title":"When to Use Deployments?","text":"<ul> <li>When you need to run multiple replicas of your application</li> <li>When you need rolling updates and rollbacks</li> <li>For stateless applications (for stateful apps, consider StatefulSets)</li> <li>When you need declarative management of pods</li> </ul>"},{"location":"Deployments/Deployments%20in%20kubernetes/#how-to-use-deployments","title":"How to Use Deployments","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#basic-deployment-yaml","title":"Basic Deployment YAML","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#types-of-deployment-strategies","title":"Types of Deployment Strategies","text":"<ol> <li>Rolling Update (default)</li> <li>Gradually replaces old pods with new ones</li> <li>No downtime during update</li> <li> <p>Controlled by <code>maxUnavailable</code> and <code>maxSurge</code> parameters</p> </li> <li> <p>Recreate</p> </li> <li>Kills all old pods before creating new ones</li> <li>Results in downtime</li> <li>Useful when your application can't run multiple versions simultaneously</li> </ol>"},{"location":"Deployments/Deployments%20in%20kubernetes/#deployment-commands-cheatsheet","title":"Deployment Commands Cheatsheet","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#basic-commands","title":"Basic Commands","text":"<pre><code># Create a deployment\nkubectl create deployment nginx --image=nginx\n\n# Get deployments\nkubectl get deployments\n\n# Describe a deployment\nkubectl describe deployment &lt;deployment-name&gt;\n\n# Delete a deployment\nkubectl delete deployment &lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#scaling","title":"Scaling","text":"<pre><code># Scale a deployment\nkubectl scale deployment &lt;deployment-name&gt; --replicas=5\n\n# Auto-scale a deployment\nkubectl autoscale deployment &lt;deployment-name&gt; --min=2 --max=5 --cpu-percent=80\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#updates-and-rollbacks","title":"Updates and Rollbacks","text":"<pre><code># Update deployment image\nkubectl set image deployment/&lt;deployment-name&gt; nginx=nginx:1.16.1\n\n# Check rollout status\nkubectl rollout status deployment/&lt;deployment-name&gt;\n\n# Pause a rollout\nkubectl rollout pause deployment/&lt;deployment-name&gt;\n\n# Resume a rollout\nkubectl rollout resume deployment/&lt;deployment-name&gt;\n\n# Rollback to previous version\nkubectl rollout undo deployment/&lt;deployment-name&gt;\n\n# Rollback to specific revision\nkubectl rollout undo deployment/&lt;deployment-name&gt; --to-revision=2\n\n# View rollout history\nkubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#advanced-operations","title":"Advanced Operations","text":"<pre><code># Dry-run to test deployment changes\nkubectl apply -f deployment.yaml --dry-run=client\n\n# View deployment as YAML\nkubectl get deployment &lt;deployment-name&gt; -o yaml\n\n# View deployment as JSON\nkubectl get deployment &lt;deployment-name&gt; -o json\n\n# Edit deployment\nkubectl edit deployment &lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#troubleshooting","title":"Troubleshooting","text":"<pre><code># View deployment events\nkubectl describe deployment &lt;deployment-name&gt; | grep -i events -A10\n\n# View associated pods\nkubectl get pods -l app=&lt;label-selector&gt;\n\n# View pod logs\nkubectl logs &lt;pod-name&gt;\n</code></pre> <p>Remember that deployments work hand-in-hand with other Kubernetes objects like Services to expose your application to the network.</p>"},{"location":"Docker-Podman-images/Build%20Docker%20images%20directly%20from%20Git%20repos/","title":"Build Docker images directly from Git repos","text":""},{"location":"Docker-Podman-images/Build%20Docker%20images%20directly%20from%20Git%20repos/#basic-syntax","title":"\u2705 Basic Syntax:","text":"<pre><code>docker build https://github.com/username/repo.git#branch:folder-path -t image-name:tag\n</code></pre>"},{"location":"Docker-Podman-images/Build%20Docker%20images%20directly%20from%20Git%20repos/#example","title":"\u2705 Example:","text":"<pre><code>docker build https://github.com/nginxinc/docker-nginx.git#main:stable -t my-nginx:latest\n</code></pre> <pre><code>docker build https://github.com/user/repo.git#branch:subdir -t ttl.sh/image-name:1h &amp;&amp; docker push ttl.sh/image-name:1h\n\ndocker build https://github.com/nginxinc/docker-nginx.git#main:stable -t ttl.sh/nginx-test:1h &amp;&amp; docker push ttl.sh/nginx-test:1h\n</code></pre> Placeholder Meaning <code>user/repo.git</code> Your GitHub repo <code>#branch:subdir</code> (optional) Git branch and subdir with Dockerfile <code>ttl.sh/image-name:1h</code> TTL image name with expiry (e.g., 1h, 30m, 1d)"},{"location":"Docker-Podman-images/Build%20Docker%20images%20directly%20from%20Git%20repos/#options","title":"\ud83e\uddfe Options:","text":"Git Source Format Description <code>repo.git</code> Default branch and root directory <code>repo.git#branch</code> Specific branch <code>repo.git#:subdir</code> Specific subdirectory in default branch <code>repo.git#branch:subdir</code> Specific branch and subdirectory"},{"location":"Docker-Podman-images/Build%20Docker%20images%20directly%20from%20Git%20repos/#notes","title":"\u2705 Notes:","text":"<ul> <li>Git repo must have a <code>Dockerfile</code> at root or in specified path.</li> <li><code>git</code> must be installed on your system.</li> <li>You can pass <code>-f</code> to specify custom <code>Dockerfile</code>:</li> </ul> <pre><code>docker build -f Dockerfile.dev https://github.com/user/repo.git#branch -t custom-image\n</code></pre> <p>if the <code>Dockerfile</code> is not in the root of the Git repo, you must specify the correct subdirectory path. Otherwise, Docker will fail with:</p> <p><code>unable to prepare context: unable to find a Dockerfile</code></p>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/","title":"Docker compose to K8s","text":"<p>Great question! This is a common confusion. Docker Compose and Kubernetes are separate deployment methods - you don't deploy docker-compose.yaml to Kubernetes directly.</p>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#the-confusion-explained","title":"The Confusion Explained","text":"<pre><code>Docker Compose (Local Development)     vs     Kubernetes (Production)\n\u251c\u2500\u2500 docker-compose.yml                        \u251c\u2500\u2500 k8s/deployments/\n\u251c\u2500\u2500 Single machine deployment                 \u251c\u2500\u2500 k8s/services/\n\u251c\u2500\u2500 Simple container orchestration            \u251c\u2500\u2500 k8s/configmaps/\n\u2514\u2500\u2500 Development/testing                       \u2514\u2500\u2500 Multi-node cluster\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#two-deployment-paths","title":"Two Deployment Paths","text":""},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#path-1-local-development-with-docker-compose","title":"Path 1: Local Development with Docker Compose","text":"<pre><code># docker-compose.yml - FOR LOCAL DEVELOPMENT ONLY\nversion: '3.8'\nservices:\n  # Redis for local testing\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n\n  # API Gateway service\n  api-gateway:\n    build: ./services/api-gateway    # Build from local Dockerfile\n    ports:\n      - \"8000:8000\"\n    environment:\n      - REDIS_URL=redis://redis:6379\n      - SCRAPER_SERVICE_URL=http://scraper-service:8001\n    depends_on:\n      - redis\n      - scraper-service\n\n  # Scraper service\n  scraper-service:\n    build: ./services/scraper-service\n    environment:\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - redis\n\n  # Data service\n  data-service:\n    build: ./services/data-service\n    volumes:\n      - ./data:/app/data           # Local volume mount\n    environment:\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - redis\n\n# Usage: docker-compose up -d\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#path-2-production-deployment-with-kubernetes","title":"Path 2: Production Deployment with Kubernetes","text":"<pre><code># Kubernetes deployment - FOR PRODUCTION\nkubectl apply -f k8s/namespace.yaml\nkubectl apply -f k8s/configmaps/\nkubectl apply -f k8s/secrets/\nkubectl apply -f k8s/storage/\nkubectl apply -f k8s/infrastructure/\nkubectl apply -f k8s/deployments/\nkubectl apply -f k8s/services/\nkubectl apply -f k8s/ingress/\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#complete-deployment-workflow","title":"Complete Deployment Workflow","text":""},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#step-1-build-and-push-docker-images","title":"Step 1: Build and Push Docker Images","text":"<pre><code># scripts/build.sh\n#!/bin/bash\n\n# Build images locally\ndocker build -t amazon-affiliate/api-gateway:latest ./services/api-gateway\ndocker build -t amazon-affiliate/scraper-service:latest ./services/scraper-service\ndocker build -t amazon-affiliate/data-service:latest ./services/data-service\n\n# Tag for registry (replace with your registry)\ndocker tag amazon-affiliate/api-gateway:latest your-registry.com/amazon-affiliate/api-gateway:latest\ndocker tag amazon-affiliate/scraper-service:latest your-registry.com/amazon-affiliate/scraper-service:latest\ndocker tag amazon-affiliate/data-service:latest your-registry.com/amazon-affiliate/data-service:latest\n\n# Push to registry\ndocker push your-registry.com/amazon-affiliate/api-gateway:latest\ndocker push your-registry.com/amazon-affiliate/scraper-service:latest\ndocker push your-registry.com/amazon-affiliate/data-service:latest\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#step-2-update-kubernetes-deployments","title":"Step 2: Update Kubernetes Deployments","text":"<pre><code># k8s/deployments/api-gateway.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\n  namespace: amazon-affiliate\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      containers:\n      - name: api-gateway\n        image: your-registry.com/amazon-affiliate/api-gateway:latest  # From registry, not local build\n        ports:\n        - containerPort: 8000\n        env:\n        - name: SCRAPER_SERVICE_URL\n          value: \"http://scraper-service:8001\"  # Kubernetes service name\n        - name: DATA_SERVICE_URL\n          value: \"http://data-service:8002\"\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#step-3-deploy-to-kubernetes","title":"Step 3: Deploy to Kubernetes","text":"<pre><code># scripts/deploy.sh\n#!/bin/bash\n\necho \"Deploying to Kubernetes...\"\n\n# Create namespace\nkubectl apply -f k8s/namespace.yaml\n\n# Apply configurations\nkubectl apply -f k8s/configmaps/\nkubectl apply -f k8s/secrets/\n\n# Deploy infrastructure (Redis, etc.)\nkubectl apply -f k8s/infrastructure/\n\n# Deploy storage\nkubectl apply -f k8s/storage/\n\n# Deploy applications\nkubectl apply -f k8s/deployments/\n\n# Create services\nkubectl apply -f k8s/services/\n\n# Setup ingress\nkubectl apply -f k8s/ingress/\n\necho \"Deployment complete!\"\n\n# Check status\nkubectl get pods -n amazon-affiliate\nkubectl get services -n amazon-affiliate\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#development-vs-production-workflow","title":"Development vs Production Workflow","text":""},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#local-development-workflow","title":"Local Development Workflow","text":"<pre><code># 1. Start local environment\ndocker-compose up -d\n\n# 2. Test your services\ncurl http://localhost:8000/health\n\n# 3. Make code changes\n# Edit files in services/\n\n# 4. Rebuild specific service\ndocker-compose build api-gateway\ndocker-compose up -d api-gateway\n\n# 5. View logs\ndocker-compose logs -f api-gateway\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#production-deployment-workflow","title":"Production Deployment Workflow","text":"<pre><code># 1. Build and push images\n./scripts/build.sh\n\n# 2. Deploy to Kubernetes\n./scripts/deploy.sh\n\n# 3. Check deployment status\nkubectl get pods -n amazon-affiliate\n\n# 4. Update deployment (rolling update)\nkubectl set image deployment/api-gateway api-gateway=your-registry.com/amazon-affiliate/api-gateway:v2 -n amazon-affiliate\n\n# 5. View logs\nkubectl logs -f deployment/api-gateway -n amazon-affiliate\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#tools-to-convert-docker-compose-to-kubernetes","title":"Tools to Convert Docker Compose to Kubernetes","text":"<p>If you want to convert docker-compose.yml to Kubernetes manifests:</p>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#1-kompose-tool","title":"1. Kompose Tool","text":"<pre><code># Install kompose\ncurl -L https://github.com/kubernetes/kompose/releases/download/v1.28.0/kompose-linux-amd64 -o kompose\nchmod +x kompose\nsudo mv ./kompose /usr/local/bin/kompose\n\n# Convert docker-compose.yml to Kubernetes manifests\nkompose convert\n\n# This generates:\n# - api-gateway-deployment.yaml\n# - api-gateway-service.yaml\n# - scraper-service-deployment.yaml\n# - etc.\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#2-docker-desktop-kubernetes","title":"2. Docker Desktop Kubernetes","text":"<pre><code># If using Docker Desktop with Kubernetes enabled\ndocker stack deploy --compose-file docker-compose.yml amazon-affiliate\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#recommended-project-structure","title":"Recommended Project Structure","text":"<pre><code>amazon-affiliate-k8s/\n\u251c\u2500\u2500 docker-compose.yml          # Local development only\n\u251c\u2500\u2500 docker-compose.override.yml # Local overrides\n\u251c\u2500\u2500 k8s/                        # Kubernetes manifests for production\n\u2502   \u251c\u2500\u2500 base/                   # Base configurations\n\u2502   \u251c\u2500\u2500 overlays/               # Environment-specific overlays\n\u2502   \u2502   \u251c\u2500\u2500 development/\n\u2502   \u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2514\u2500\u2500 production/\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 dev-start.sh           # Start local development\n\u2502   \u251c\u2500\u2500 build.sh               # Build and push images\n\u2502   \u251c\u2500\u2500 deploy-dev.sh          # Deploy to dev cluster\n\u2502   \u251c\u2500\u2500 deploy-prod.sh         # Deploy to production\n\u2502   \u2514\u2500\u2500 cleanup.sh             # Cleanup resources\n\u2514\u2500\u2500 skaffold.yaml              # For continuous development\n</code></pre>"},{"location":"Docker-Podman-images/Docker-compose%20to%20K8s/#summary","title":"Summary","text":"<ul> <li>Docker Compose: Local development and testing</li> <li>Kubernetes: Production deployment</li> <li>Images: Built locally, pushed to registry, pulled by Kubernetes</li> <li>Configuration: Environment variables and volumes handled differently</li> <li>Networking: Docker Compose uses container names, Kubernetes uses service names</li> <li>Scaling: Docker Compose limited to single machine, Kubernetes scales across cluster</li> </ul> <p>The docker-compose.yml file helps you develop and test locally, while Kubernetes manifests handle production deployment.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/","title":"Enterprise grade images as safer","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes, it's normal for UBI (Universal Base Image) images to increase a bit in size when adding packages like <code>unzip</code>.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#details","title":"\ud83d\udccc Details:","text":"<ul> <li>Base UBI Image: <code>213 MB</code></li> <li>After installing <code>unzip</code>: <code>228 MB</code> \u279d \u2705 only +15 MB</li> <li>That\u2019s a reasonable and expected increase.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#why-not-aggressively-reduce-size-here","title":"\ud83e\udde0 Why not aggressively reduce size here:","text":"<ul> <li>UBI is meant for Red Hat compatibility and enterprise-grade support.</li> <li>You keep <code>dnf</code> and required system libs which add size.</li> <li>Stripping too much (like removing <code>dnf</code>, locale files, etc.) may break system tooling or future updates.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#best-practices-already-covered-in-your-dockerfile","title":"\u2705 Best practices (already covered in your Dockerfile):","text":"<ul> <li><code>dnf clean all</code> \u2705</li> <li><code>rm -rf /var/cache/dnf</code> \u2705 You already did the right optimizations.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#if-you-want-minimal-base-image-with-just-unzip","title":"\u2705 If you want minimal base image with just unzip:","text":"<p>You can consider using <code>alpine</code>: <pre><code>FROM alpine:3.20\nRUN apk add --no-cache unzip\nWORKDIR /app\nCMD [\"sh\"]\n</code></pre></p> <p>Size: ~6 MB But not compatible with RHEL environments or UBI policies.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#summary","title":"\ud83d\udd0d Summary","text":"Option Base Size Final Size Notes <code>ubi8/ubi</code> ~213 MB ~228 MB \u2705 Safe, supported, expected size <code>alpine</code> ~5 MB ~6\u20137 MB \u26a0\ufe0f Smaller but not RHEL-compatible So for enterprise or UBI environments, your image is fine. No need to reduce further."},{"location":"Docker-Podman-images/Limitations%20of%20Containers/","title":"Limitations of Containers","text":"<p>Created: 2025-07-02 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <ul> <li><code>Lack of Self Healing</code> : No built-in mechanism to automatically re-create containers</li> <li><code>High Availability Challenges</code>:  When server fails, entire application becomes unavailable, causing significant downtime</li> <li><code>Scaling hurdles</code> :  Lacks automatic scaling</li> <li><code>Load Balancing Challenges</code> : Doesn't directly handle load balancing</li> <li><code>Storage Constraints</code> : Rely on the underlying host system's storage resources</li> </ul> Limitation Why It Happens Impact Common Bias / Assumption Solution / Best Practice Limited OS-level isolation Containers share host kernel Less secure than VMs \"Containers are like lightweight VMs\" Use rootless containers, user namespaces, SELinux/AppArmor Resource contention Shared CPU/memory between containers One container can starve others \"Kubernetes handles everything\" Set CPU/memory limits/requests in pod specs Weak security boundaries Linux namespaces/cgroups offer process-level, not full system isolation Exploits can affect host or other containers \"Containers are secure by default\" Enable seccomp, AppArmor, non-root users, and readonly filesystems No persistence by default Containers are ephemeral/stateless Data loss on restarts or removal \"Containers just work out of the box\" Mount persistent volumes using PVCs or bind mounts Networking is complex Virtual networks and overlays are used Harder to debug and manage \"Networking is auto-managed\" Use well-documented CNI plugins; visualize with tools like <code>weave</code>, <code>cilium</code>, <code>lens</code> Logging is ephemeral Logs disappear when containers stop Lost logs = lost visibility \"Container logs are always available\" Use centralized logging (EFK, Loki, Fluentbit) Poor for stateful workloads Stateless is default; state requires extra setup Data consistency problems, scaling challenges \"Just deploy databases in containers\" Use StatefulSets and persistent volumes; or external DBaaS Cold start issues Images must be pulled and initialized Higher latency on scale-up \"Containers start instantly\" Pre-pull images, reduce size, keep base image in memory Vulnerable images Base images may contain CVEs or malware Security risk, compliance issues \"Using Alpine means it's secure\" Scan with Trivy, Grype; use trusted sources Image bloat / dependency hell Unnecessary libs included in images Larger attack surface, slower builds \"Dockerfile is enough\" Use multi-stage builds; prefer <code>distroless</code>, minimal images Misunderstood Why It's Misleading or Dangerous Reality Check / Correction Fix It Containers are like lightweight VMs Containers share kernel, unlike isolated VMs Containers are process-isolated, not fully OS-isolated Use user namespaces, seccomp, rootless mode Kubernetes handles everything, I don\u2019t need to worry K8s doesn't enforce security, resource limits, logging You must configure policies, limits, and monitoring Set resource requests/limits, apply PodSecurityPolicies, enable logging Using containers ensures scalability Badly designed apps won\u2019t scale Stateless, loosely coupled design is required Build stateless microservices, externalize state, use health checks All container images are safe to use Public images may have CVEs or malware Always scan and verify base images Use trusted registries, scan with Trivy/Grype, sign images Once it's containerized, it\u2019s production-ready Dev/test success doesn\u2019t mean it's hardened Security, monitoring, and compliance need to be applied Use image policies, observability stack, vulnerability scanners Dockerfile is enough to define the app No lifecycle, logging, recovery, scaling in Dockerfile Use Helm charts or K8s manifests Define full app spec using Helm/Operators One container = one service = done Ignores lifecycle, health, state, updates Requires orchestration, health checks, rollbacks Use readiness/liveness probes, rolling deployments, sidecars if needed Container logs are always accessible Logs vanish on restart unless redirected Use centralized logging stack Deploy EFK/Loki, use persistent log volumes or logging agents Containers are secure by default Defaults allow root, no seccomp, no network rules Security hardening must be done explicitly Apply securityContext, AppArmor, rootless containers Minimal base image = safe image Even Alpine or scratch can have CVEs Minimal != secure, must still be scanned Use distroless + image scanners regularly"},{"location":"Docker-Podman-images/Run%20a%20container%20with%20all%20resource%20limits/","title":"Run a container with all resource limits","text":"<p>\u2705 if you're running your own container, you can absolutely define memory, CPU, network, and block I/O limits using <code>docker run</code> options.</p>"},{"location":"Docker-Podman-images/Run%20a%20container%20with%20all%20resource%20limits/#example-run-a-container-with-all-resource-limits","title":"\u2705 Example: Run a container with all resource limits","text":"<pre><code>docker run -d \\\n  --name my-app \\\n  --memory=\"2g\" \\\n  --memory-swap=\"3g\" \\\n  --cpus=\"2.0\" \\\n  --memory-reservation=\"1g\" \\\n  --blkio-weight=\"500\" \\\n  --device-read-bps /dev/sda:10mb \\\n  --device-write-bps /dev/sda:5mb \\\n  --device-read-iops /dev/sda:100 \\\n  --device-write-iops /dev/sda:50 \\\n  --network bridge \\\n  my-app-image:latest\n</code></pre>"},{"location":"Docker-Podman-images/Run%20a%20container%20with%20all%20resource%20limits/#breakdown-of-options","title":"\ud83d\udd27 Breakdown of options:","text":"Option Description <code>--memory=\"2g\"</code> Hard memory limit (max the container can use) <code>--memory-swap=\"3g\"</code> Total memory + swap allowed <code>--memory-reservation=\"1g\"</code> Soft limit \u2014 warning if overused <code>--cpus=\"2.0\"</code> Limit container to 2 CPU cores <code>--blkio-weight=\"500\"</code> Relative block I/O weight (10\u20131000) <code>--device-read-bps</code> Limit read throughput (bytes/sec) from a device <code>--device-write-bps</code> Limit write throughput <code>--device-read-iops</code> Limit read IOPS <code>--device-write-iops</code> Limit write IOPS <code>--network bridge</code> Attach to Docker's bridge network (default) ### \ud83d\udccc Notes: - <code>--device-*</code> options need a real device path like <code>/dev/sda</code>, <code>/dev/nvme0n1</code>, etc. - Use <code>lsblk</code> or <code>df -h</code> to find your device names. - These options only apply when containers do disk or network-intensive operations. ### \ud83d\udd0d Block I/O (Input/Output) in simple terms: <p>Block I/O refers to how data is read from or written to block storage devices (like SSDs, HDDs, or virtual block devices like <code>/dev/sda</code>, <code>/dev/nvme0n1</code>).</p>"},{"location":"Docker-Podman-images/Run%20a%20container%20with%20all%20resource%20limits/#in-docker-context","title":"\ud83d\udca1 In Docker context:","text":"<p>It means how much disk read/write throughput your container is allowed to use. This affects:</p> Container does... It uses... Reads files Block I/O read Writes logs or data files Block I/O write Uses databases Block I/O (both) Caches or temp storage Block I/O ### \ud83d\udcca Example: If you run a database or file-heavy service in a container, and you don't limit block I/O, it could hog disk access, affecting performance of other containers or the host. ### \ud83d\udea6 Docker lets you control it via: <pre><code>--blkio-weight=500                      # Relative I/O weight (10\u20131000)\n--device-read-bps /dev/sda:10mb        # Max read speed\n--device-write-bps /dev/sda:5mb        # Max write speed\n--device-read-iops /dev/sda:100        # Max read IOPS\n--device-write-iops /dev/sda:50        # Max write IOPS\n</code></pre> ### \ud83d\uded1 Use case: If you run multiple containers and want to control or isolate disk usage, Block I/O limits help prevent one container from slowing down the whole system. <p>Let me know if you want a demo with test containers or benchmarking tools.</p>"},{"location":"Docker-Podman-images/Tag%20images%20to%20push/","title":"Tag images to push","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>podman tag localhost/ubi-unzip:latest 100.210.10.6:5007/cron-image/busybox:latest\n</code></pre>"},{"location":"Docker-Podman-images/To%20find%20the%20memory%20usage%20of%20containers/","title":"To find the memory usage of containers","text":"<pre><code>gouse@gouse$ docker stats --no-stream\nCONTAINER ID   NAME                        CPU %     MEM USAGE / LIMIT     MEM %     NET I/O          BLOCK I/O         PIDS\n2eb6278369ae   dev-cluster-control-plane   14.56%    543.7MiB / 15.37GiB   3.45%     491kB / 4.52MB   1.21MB / 813MB    271\n462f01578a57   dev-cluster-worker          2.62%     107.9MiB / 15.37GiB   0.69%     2.28MB / 237kB   418kB / 122MB     83\n8ea6e48720e9   dev-cluster-worker2         1.19%     108.6MiB / 15.37GiB   0.69%     2.28MB / 241kB   295kB / 122MB     83\nfdd41c690bfb   harbor-log                  0.00%     16.27MiB / 15.37GiB   0.10%     34.8kB / 126B    15.3MB / 8.19kB   10\n\n\ngouse@gouse$ docker stats --no-stream --format \"table {{.Name}}\\t{{.MemUsage}}\"\nNAME                        MEM USAGE / LIMIT\ndev-cluster-control-plane   543.1MiB / 15.37GiB\ndev-cluster-worker          107.8MiB / 15.37GiB\ndev-cluster-worker2         108.1MiB / 15.37GiB\nharbor-log                  16.27MiB / 15.37GiB\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/","title":"podman build   squash  commit and ImageStream cleanup","text":"Option Recommendation <code>podman build --squash</code> \u2705 Native support <code>docker build --squash</code> \u26a0\ufe0f Supported, but only with experimental + no BuildKit Multi-stage build \u2705 Best practice for both Docker and Podman Feature <code>docker commit</code> <code>podman commit</code> \u2705 Create image from container \u2705 Yes \u2705 Yes \ud83e\uddf1 Squash layers \u274c No (not supported) \u2705 <code>--squash</code> supported \ud83d\udd10 Rootless support \u274c No (needs root unless configured) \u2705 Fully rootless \ud83e\uddfe Add metadata \u2705 <code>--author</code>, <code>--message</code> \u2705 Same \ud83e\uddea OCI-compliant \u2705 (with extra steps) \u2705 Native \ud83e\uddf0 Set image format \u274c \u2705 <code>--format=oci/docker</code> \ud83d\ude80 Systemd-aware \u274c \u2705 Better integration with systemd \ud83d\udd0d Default history tracking \u2705 via container diff \u2705 via container diff \ud83e\uddfc Clean image from temp \u274c manual \u2705 better layered FS support Scenario Use <code>docker commit</code> Use <code>podman commit</code> One-time image capture \u2705 Yes \u2705 Yes Reproducible CI builds \u274c No \u274c No Secure release image \u274c No \u2705 With <code>--squash</code> Rootless environments \u274c Limited \u2705 Best option"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#dockerfile-very-basic","title":"\ud83d\udd27 Dockerfile (very basic):","text":"<pre><code># file: Dockerfile\nFROM alpine:latest\nRUN apk add --no-cache curl\nRUN echo \"hello world\" &gt; /hello.txt\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#build-command","title":"\ud83d\udee0\ufe0f Build Command:","text":"<pre><code>podman build --squash -t gowse/alpine-curl:v1 .\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#explanation","title":"\ud83e\uddf1 Explanation :","text":"Part Meaning <code>podman build</code> Create an image from a Dockerfile <code>--squash</code> Combines all image layers into one layer (final image is smaller &amp; cleaner) <code>-t gowse/alpine-curl:v1</code> Tags the image with a name and version <code>.</code> Build context is current directory (should contain Dockerfile)"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#result","title":"\u2705 Result:","text":"<ul> <li>Final image has a single layer (no intermediate <code>RUN</code> layers).</li> <li>Smaller and cleaner image, better for production.</li> </ul> <p>You're hitting a real-world architectural pain point that many large organizations face when dealing with incremental container builds + integration-heavy runtimes like IBM App Connect Enterprise (ACE).</p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#problem-recap","title":"\ud83c\udfaf Problem Recap:","text":"<ul> <li>You're continuously building and deploying IBM App Connect containers.</li> <li>Each Integration Server (IS) image has many .bar workflows.</li> <li>Over time, you incrementally build on top of previous Image Streams (possibly using S2I or <code>COPY</code>).</li> <li>Eventually, you're hitting this:</li> </ul> <pre><code>error: max depth exceeded / too many layers\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#deep-technical-cause-layer-limit","title":"\ud83e\udde0 Deep Technical Cause (Layer Limit)","text":"<p>Each build adds a new image layer, especially if:</p> <ul> <li>You do not squash</li> <li>You rebuild on top of prior image streams (<code>FROM imageStreamName</code>)</li> <li>You add <code>.bar</code> files via separate <code>COPY</code>/<code>RUN</code> instructions.</li> </ul> <p>You hit the OverlayFS (OCI) limit:</p> <ul> <li>Maximum = 125 layers per image on <code>overlay2</code> (used by Docker, Podman, OpenShift)</li> <li>Many base images (e.g., ACE + Ubuntu) already have 10\u201330 layers</li> <li>Every new <code>COPY</code>, <code>RUN</code>, <code>.bar</code> deployment \u2192 new layer</li> </ul> <p>\u26a0\ufe0f App Connect doesn\u2019t do \"one-bar-only\" builds. So these accumulate fast.</p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#real-symptoms","title":"\ud83d\udd0d Real Symptoms:","text":"Symptom Root Cause \"Max layer/depth exceeded\" OCI spec violation due to too many <code>diff_ids</code> (layers) App runs slow or crashes Layer lookups degrade performance Image push fails in CI/CD Registry often rejects too deep manifest trees ## \u2705 <code>--squash</code>: Benefits for Your Case Benefit Why It Matters for App Connect \ud83d\udca5 Removes all intermediate <code>.bar</code> layer diffs You don't retain N history of bar files in FS layers \u2b07\ufe0f Greatly reduces image layer count You can continuously rebuild without hitting max layer \ud83d\udd10 Hides sensitive integration flow stages (if any) Good for regulatory reasons \ud83d\udca8 Smaller images = faster deploys across DEV/QA/PROD Especially with remote registries \ud83e\uddf9 Prevents accumulation of dead bar versions Since each squash is clean slate ## \u274c Potential Downsides Risk Mitigation \u274c Caching loss Squashed images can't be layered on top of easily. \u274c Longer build times Because everything builds from scratch \u274c Debuggability Can\u2019t <code>podman history</code> to inspect past layers \u274c Squash not supported in OpenShift S2I by default You may need custom build scripts or <code>Containerfile</code> use ## \ud83d\udd04 Long-Term Strategy"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#option-1-use-squash-for-production-deploys-only","title":"\ud83d\udd27 Option 1: Use <code>--squash</code> for production deploys only","text":"<ul> <li>DEV: Use normal layered builds for fast iteration</li> <li>STAGE/PROD: Use squashed images to avoid layer bloat</li> </ul>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#option-2-split-workflows-by-domains","title":"\ud83d\udd27 Option 2: Split Workflows by Domains","text":"<p>Instead of dumping 50 <code>.bar</code> files into one IS image:</p> <ul> <li>Break into domain-specific Integration Servers<ul> <li><code>payments.is</code>, <code>orders.is</code>, <code>crm.is</code></li> </ul> </li> <li>Each one gets 5\u201310 bars max</li> <li>Now your <code>Dockerfile</code> looks like:</li> </ul> <pre><code>FROM ibm-appconnect:latest\nCOPY payment-flow1.bar /home/ace/ace-server/bars/\nCOPY payment-flow2.bar /home/ace/ace-server/bars/\n</code></pre> <ul> <li>Less <code>.bar</code> layers = less layer growth over time</li> </ul>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#option-3-flatten-image-with-custom-script","title":"\ud83d\udd27 Option 3: Flatten image with custom script","text":"<p>Use a build stage that unpacks <code>.bar</code> files, copies them directly, then squashes.</p> <p><pre><code>FROM ibm-appconnect as builder\nCOPY *.bar /tmp/bars/\nRUN for f in /tmp/bars/*.bar; do unpack-and-verify.sh \"$f\"; done\n\nFROM scratch\nCOPY --from=builder /home/ace /home/ace\n</code></pre> Then build with: <pre><code>podman build --squash -t appconnect-integrations:v1 .\n</code></pre></p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#bonus-audit-current-image-streams","title":"\ud83d\udcca Bonus: Audit Current Image Streams","text":"<p><pre><code>oc get istag -n your-namespace\nfor tag in $(oc get istag -n your-namespace -o name); do\n  echo \"$tag: $(oc get \"$tag\" -o jsonpath='{.image.metadata.name}' | wc -l) layers\"\ndone\n</code></pre> Helps track which image stream has grown unacceptably large.</p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#final-recommendation","title":"\ud83c\udfaf Final Recommendation","text":"Environment Use <code>--squash</code>? Notes DEV \u274c Keep fast caching for rebuilds QA / UAT \u2705 Use squash to test realistic image PROD \u2705 Always squash to avoid image growth over time"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#periodically-clean-up-openshift-imagestreams-to-avoid-bloating-and-hitting-the-layer-limit","title":"periodically clean up OpenShift ImageStreams to avoid bloating and hitting the layer limit:","text":""},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#goal","title":"\u2705 Goal:","text":"<ul> <li>Clean up old image tags (ImageStreamTags)</li> <li>Prune unused image layers in registry</li> <li>Avoid OCI layer depth limit</li> </ul>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#1-clean-up-old-tags-in-imagestream-appconnect-is","title":"\ud83e\uddfc 1. Clean Up Old Tags in ImageStream (AppConnect IS)","text":""},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#script-to-keep-only-last-n-tags","title":"\ud83d\udd01 Script to Keep Only Last N Tags:","text":"<pre><code>#!/bin/bash\n# keep last 5 tags in image stream\nN=5\nIMAGESTREAM=my-is-name\nNAMESPACE=my-project\n\nTAGS=$(oc get istag -n $NAMESPACE --sort-by=.metadata.creationTimestamp -o jsonpath=\"{range .items[*]}{.metadata.name}{'\\n'}{end}\" | grep \"^$IMAGESTREAM:\")\n\nCOUNT=$(echo \"$TAGS\" | wc -l)\n\nif (( COUNT &gt; N )); then\n  DELETE_TAGS=$(echo \"$TAGS\" | head -n $((COUNT - N)))\n  for tag in $DELETE_TAGS; do\n    echo \"Deleting tag: $tag\"\n    oc delete istag -n $NAMESPACE \"$tag\"\n  done\nfi\n</code></pre> <p>\ud83d\udca1 Run this via cronjob or Jenkins.</p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#2-prune-unused-images-from-internal-registry-admin-only","title":"\ud83d\udeae 2. Prune Unused Images from Internal Registry (Admin Only)","text":"<p>Run as cluster admin: <pre><code>oc adm prune images \\\n  --keep-tag-revisions=3 \\\n  --keep-younger-than=72h \\\n  --confirm\n</code></pre></p> Option Description <code>--keep-tag-revisions=3</code> Keep latest 3 image revisions per tag <code>--keep-younger-than=72h</code> Keep recent images (last 72 hours) <code>--confirm</code> Actually delete (omit for dry-run) &gt; \u26a0\ufe0f Only affects internal OpenShift registry."},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#3-monitor-image-layer-depth-per-imagestream","title":"\ud83d\udd0d 3. Monitor Image Layer Depth per ImageStream","text":"<pre><code>oc get istag -n your-namespace -o json | jq '.items[] | {name: .metadata.name, layers: .image.dockerImageLayers | length}'\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#4-add-cicd-rule-to-prevent-deploying-images-with-too-many-layers","title":"\ud83d\udee1\ufe0f 4. Add CI/CD Rule to Prevent Deploying Images with Too Many Layers","text":"<p>In your Jenkins/CI: <pre><code>LAYER_COUNT=$(podman history your-image | wc -l)\n\nif (( LAYER_COUNT &gt; 100 )); then\n  echo \"ERROR: Image has too many layers ($LAYER_COUNT). Deployment blocked.\"\n  exit 1\nfi\n</code></pre></p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#5-optionally-remove-imagestream-manually","title":"\ud83d\uddd1\ufe0f 5. Optionally Remove ImageStream Manually","text":"<p>If certain ImageStreams are outdated: <pre><code>oc delete is appconnect-is-old -n your-namespace\n</code></pre></p>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#tips","title":"\ud83d\udca1 Tips","text":"Tip Benefit Use <code>--squash</code> in QA/PROD images Prevents future bloat Automate <code>oc delete istag</code> by age/tag count Keeps env clean Use separate ImageStreams for DEV and PROD Avoids dirty reuse Run <code>oc adm prune images</code> weekly Frees storage, resets layer chains #### production-ready OpenShift CronJob YAML to periodically clean up old ImageStreamTags, keeping only the latest 5 tags in a given ImageStream."},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#openshift-cronjob-cleanup-old-tags","title":"\u2705 OpenShift CronJob: Cleanup Old Tags","text":""},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#imagestream-cleanup-cronjobyaml","title":"\ud83d\udcc1 <code>imagestream-cleanup-cronjob.yaml</code>","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: imagestream-cleanup\n  namespace: your-project\nspec:\n  schedule: \"0 2 * * *\"  # Every day at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: registry.redhat.io/openshift4/ose-cli  # OpenShift CLI image\n            command:\n            - /bin/bash\n            - -c\n            - |\n              N=5\n              IMAGESTREAM=\"your-image-stream-name\"\n              NAMESPACE=\"your-project\"\n\n              TAGS=$(oc get istag -n $NAMESPACE --sort-by=.metadata.creationTimestamp \\\n                -o jsonpath=\"{range .items[*]}{.metadata.name}{'\\n'}{end}\" | grep \"^$IMAGESTREAM:\")\n\n              COUNT=$(echo \"$TAGS\" | wc -l)\n\n              if (( COUNT &gt; N )); then\n                DELETE_TAGS=$(echo \"$TAGS\" | head -n $((COUNT - N)))\n                for tag in $DELETE_TAGS; do\n                  echo \"Deleting $tag\"\n                  oc delete istag -n $NAMESPACE \"$tag\"\n                done\n              else\n                echo \"Nothing to delete. Total tags: $COUNT\"\n              fi\n          restartPolicy: OnFailure\n          serviceAccountName: imagestream-cleanup-sa\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#service-account-permissions","title":"\ud83d\udd10 Service Account &amp; Permissions","text":""},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#imagestream-cleanup-roleyaml","title":"\ud83d\udcc1 <code>imagestream-cleanup-role.yaml</code>","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: imagestream-cleanup-sa\n  namespace: your-project\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: imagestream-cleanup-role\n  namespace: your-project\nrules:\n- apiGroups: [\"image.openshift.io\"]\n  resources: [\"imagestreamtags\"]\n  verbs: [\"get\", \"list\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: imagestream-cleanup-binding\n  namespace: your-project\nsubjects:\n- kind: ServiceAccount\n  name: imagestream-cleanup-sa\nroleRef:\n  kind: Role\n  name: imagestream-cleanup-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#deploy-in-openshift","title":"\ud83d\ude80 Deploy in OpenShift","text":"<pre><code>oc apply -f imagestream-cleanup-role.yaml\noc apply -f imagestream-cleanup-cronjob.yaml\n</code></pre>"},{"location":"Docker-Podman-images/podman%20build%20--squash%20%20commit%20and%20ImageStream%20cleanup/#notes","title":"\ud83d\udccc Notes","text":"Field Description <code>N=5</code> Keeps only the latest 5 ImageStreamTags <code>IMAGESTREAM</code> Replace with your ACE Integration Server image stream name <code>schedule</code> Change to fit your cleanup window (e.g., <code>\"0 */6 * * *\"</code> for every 6h) <code>ose-cli</code> Lightweight Red Hat image with <code>oc</code> CLI for scripting"},{"location":"Docker-Podman-images/scp%20images%20in%20tar%20format%20to%20remote%20hosts%20-%20offline/","title":"Scp images in tar format to remote hosts   offline","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>podman save -o ubi-unzip.tar localhost/ubi-unzip:latest\n\nupload to remote host, offline On Remote Host:\npodman load -i ubi-unzip.tar\n</code></pre>"},{"location":"Helm/1.%20Check%20Helm%20Release%28deployed%29%20Information/","title":"1. Check Helm Release(deployed) Information","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># List all Helm releases to find Traefik\nhelm list -A\n\n# Get detailed information about Traefik release\nhelm status traefik -n traefik-system  # Replace with actual namespace\n\n# Get the values used for Traefik installation\nhelm get values traefik -n traefik-system\n\n# Get all values (including defaults)\nhelm get values traefik -n traefik-system --all\n</code></pre>"},{"location":"Helm/Checking%20Traefik%20Helm%20Configuration/","title":"Checking Traefik Helm Configuration","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>#!/bin/bash\n\necho \"=== Checking Traefik Helm Configuration ===\"\n\n# 1. Find Traefik release\necho \"1. Finding Traefik Helm releases...\"\nhelm list -A | grep traefik\n\n# 2. Get Traefik values (replace namespace as needed)\necho -e \"\\n2. Getting Traefik Helm values...\"\nTRAEFIK_NAMESPACE=$(kubectl get pods -A | grep traefik | head -1 | awk '{print $1}')\nTRAEFIK_RELEASE=$(helm list -A | grep traefik | head -1 | awk '{print $1}')\n\nif [ ! -z \"$TRAEFIK_RELEASE\" ] &amp;&amp; [ ! -z \"$TRAEFIK_NAMESPACE\" ]; then\n    echo \"Found Traefik release: $TRAEFIK_RELEASE in namespace: $TRAEFIK_NAMESPACE\"\n\n    echo -e \"\\n=== Current Traefik Values ===\"\n    helm get values $TRAEFIK_RELEASE -n $TRAEFIK_NAMESPACE\n\n    echo -e \"\\n=== All Traefik Values (including defaults) ===\"\n    helm get values $TRAEFIK_RELEASE -n $TRAEFIK_NAMESPACE --all | grep -A 20 -B 5 \"ports:\\|entryPoints:\\|tls:\"\nelse\n    echo \"Traefik release not found via Helm\"\nfi\n\n# 3. Check Traefik service ports\necho -e \"\\n3. Checking Traefik service configuration...\"\nkubectl get service -A | grep traefik\nkubectl get service traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -A 10 -B 5 \"ports:\"\n\n# 4. Check for HTTPS/TLS configuration\necho -e \"\\n4. Checking for HTTPS/TLS configuration...\"\nkubectl get service traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -E \"443|websecure|tls\"\n\n# 5. Check Traefik deployment args\necho -e \"\\n5. Checking Traefik deployment arguments...\"\nkubectl get deployment traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -A 20 \"args:\"\n\n# 6. Check if Traefik dashboard is enabled\necho -e \"\\n6. Checking Traefik dashboard configuration...\"\nkubectl get deployment traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -E \"dashboard|api\"\n\necho -e \"\\n=== Traefik Configuration Check Complete ===\"\n</code></pre>"},{"location":"Helm/Helm%20Commands%20-%20Cheatsheet/","title":"Helm Commands   Cheatsheet","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Task Command Create chart <code>helm create &lt;name&gt;</code> Install chart <code>helm install &lt;release&gt; &lt;chart-path&gt;</code> Upgrade chart <code>helm upgrade &lt;release&gt; &lt;chart-path&gt;</code> Uninstall chart <code>helm uninstall &lt;release&gt;</code> Show values <code>helm show values &lt;chart&gt;</code> Validate chart <code>helm lint &lt;chart-path&gt;</code> Dry-run install <code>helm install --dry-run --debug ...</code> <pre><code>k delete all --all -n longhorn-system\nk delete crds -l app.kubernetes.io/part-of=longhorn\nk delete ns longhorn-system\nhelm uninstall longhorn -n longhorn-system || true\nk get crds | grep longhorn\n\nThen delete manually:\nk delete crd &lt;crd-name&gt;\nk get crds | grep longhorn | awk '{print $1}' | xargs k delete crd\nk delete ns longhorn-system --grace-period=0 --force\n\nIf stuck in `Terminating`, run:\nk get ns longhorn-system -o json | jq '.spec.finalizers=[]' | k replace --raw \"/api/v1/namespaces/longhorn-system/finalize\" -f -\n\nVerify cleanup\nk get crds | grep longhorn\nk get ns | grep longhorn\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/","title":"Helm charts for deployments","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Helm/Helm%20charts%20for%20deployments/#what-is-a-helm-chart","title":"\ud83d\udd27 What is a Helm Chart?","text":"<p>A Helm chart is a packaged Kubernetes application\u2014like a Docker image but for k8s deployments.</p>"},{"location":"Helm/Helm%20charts%20for%20deployments/#helm-chart-structure","title":"\ud83d\udce6 Helm Chart Structure","text":"<pre><code>mychart/\n\u251c\u2500\u2500 Chart.yaml          # Chart metadata (name, version, etc.)\n\u251c\u2500\u2500 values.yaml         # Default config values\n\u251c\u2500\u2500 templates/          # Kubernetes YAML templates\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2514\u2500\u2500 _helpers.tpl    # Template helpers (optional)\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#sample-chart-for-nginx-deployment","title":"\ud83e\uddea Sample Chart for Nginx Deployment","text":""},{"location":"Helm/Helm%20charts%20for%20deployments/#1-create-chart","title":"1. Create chart:","text":"<pre><code>helm create nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#2-edit-valuesyaml","title":"2. Edit <code>values.yaml</code>:","text":"<pre><code>replicaCount: 2\nimage:\n  repository: nginx\n  tag: latest\nservice:\n  type: ClusterIP\n  port: 80\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#3-deploy","title":"3. Deploy:","text":"<pre><code>helm install my-nginx ./nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#4-upgrade","title":"4. Upgrade:","text":"<pre><code>helm upgrade my-nginx ./nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#5-uninstall","title":"5. Uninstall:","text":"<pre><code>helm uninstall my-nginx\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#commands-cheat-sheet","title":"\ud83e\uddf0 Commands Cheat Sheet","text":"Task Command Create chart <code>helm create &lt;name&gt;</code> Install chart <code>helm install &lt;release&gt; &lt;chart-path&gt;</code> Upgrade chart <code>helm upgrade &lt;release&gt; &lt;chart-path&gt;</code> Uninstall chart <code>helm uninstall &lt;release&gt;</code> Show values <code>helm show values &lt;chart&gt;</code> Validate chart <code>helm lint &lt;chart-path&gt;</code> Dry-run install <code>helm install --dry-run --debug ...</code>"},{"location":"Ingress/1.%20Host%20based%20ingress%20routing/","title":"1. Host based ingress routing","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>create below manifest files - todo-app namespace - todo-green deployment with replicas, image, container port 80     - todo-green service with ClusterIP &amp; 80 port - todo-blue deployment     - todo-blue service with ClusterIP &amp; 80 port - ingress with traefik class <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo\n  namespace: todo-app\n  labels:\n    app: todo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todo\n  template:\n    metadata:\n      labels:\n        app: todo\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo\n  namespace: todo-app\nspec:\n  selector:\n    app: todo\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-ingress\n  namespace: todo-app\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todo.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo\n                port:\n                  number: 80\n</code></pre></p> <p>Add this to your <code>/etc/hosts</code>:</p> <pre><code>sudo echo \"127.0.0.1 todo.localhost\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 todo.localhost\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre> <p>Access app via:</p> <pre><code>http://todo.localhost:30080\n</code></pre> <p>(adjust port based on your k3d loadbalancer port mapping).</p>"},{"location":"Ingress/2.%20Path%20based%20ingress%20routing/","title":"2. Path based ingress routing","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>create below manifest files - todo-app namespace - todo-green deployment with replicas, image, container port 80      (image : gowseshaik/todo-app-green:1.0)     - todo-green service with ClusterIP &amp; 80 port - todo-blue deployment    (image: gowseshaik/todo-app-blue:1.0)     - todo-blue service with ClusterIP &amp; 80 port - ingress with traefik class <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-green\n  template:\n    metadata:\n      labels:\n        app: todo-green\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-blue\n  template:\n    metadata:\n      labels:\n        app: todo-blue\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-green\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-blue\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-ingress\n  namespace: todo-app\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todoapp.localhost.com\n      http:\n        paths:\n          - path: /green\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n          - path: /blue\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-blue\n                port:\n                  number: 80\n</code></pre></p> <p>Add to <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 todoapp.localhost.com\nsudo echo \"127.0.0.1 todoapp.localhost.com\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 todoapp.localhost.com\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre> <p>Access:</p> <pre><code>http://todoapp.localhost.com:30080/green\nhttp://todoapp.localhost.com:30080/blue\n</code></pre>"},{"location":"Ingress/3.%20subdomain%20based%20ingress%20routing/","title":"3. subdomain based ingress routing","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here's the corrected manifest file using subdomain routing, which will completely avoid the CSS/JS loading issues:Key changes with subdomain routing:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-green\n  template:\n    metadata:\n      labels:\n        app: todo-green\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-blue\n  template:\n    metadata:\n      labels:\n        app: todo-blue\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-blue:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-green\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-blue\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-green-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: green.todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-blue-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: blue.todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-blue\n                port:\n                  number: 80\n---\n# Optional: Main domain ingress that can redirect to one of the versions\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-main-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: todo-app-redirect-to-green@kubernetescrd\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n---\n# Optional: Redirect middleware for main domain\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: redirect-to-green\n  namespace: todo-app\nspec:\n  redirectRegex:\n    regex: \"^https?://todoapp.localhost.com/(.*)\"\n    replacement: \"http://green.todoapp.localhost.com/${1}\"\n    permanent: false\n</code></pre> <ol> <li>Green app: Accessible at <code>green.todoapp.localhost.com</code></li> <li>Blue app: Accessible at <code>blue.todoapp.localhost.com</code></li> <li>No path prefixes: Each app serves from the root path <code>/</code>, so CSS/JS files load normally</li> <li>Optional main domain: <code>todoapp.localhost.com</code> can redirect to one version or serve a landing page</li> </ol> <p>DNS Configuration Required:</p> <p>You'll need to configure DNS or your hosts file to point the subdomains to your Traefik ingress:</p> <p>For local development (add to <code>/etc/hosts</code> on Linux/Mac or <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> on Windows):</p> <pre><code>127.0.0.1 green.todoapp.localhost.com\n127.0.0.1 blue.todoapp.localhost.com\n127.0.0.1 todoapp.localhost.com\n</code></pre> <p>For production, configure your DNS provider to create A records:</p> <ul> <li><code>green.todoapp.yourdomain.com</code> \u2192 Your load balancer IP</li> <li><code>blue.todoapp.yourdomain.com</code> \u2192 Your load balancer IP</li> </ul> <p>Testing:</p> <pre><code># Test green version\ncurl http://green.todoapp.localhost.com\n\n# Test blue version  \ncurl http://blue.todoapp.localhost.com\n\n# Test main domain (should redirect to green)\ncurl http://todoapp.localhost.com\n</code></pre> <p>Advantages of subdomain routing:</p> <ul> <li>\u2705 CSS/JS files load correctly (no path issues)</li> <li>\u2705 Cleaner URLs</li> <li>\u2705 Better for SEO if needed</li> <li>\u2705 Easier to manage cookies and local storage per version</li> <li>\u2705 No middleware complexity needed</li> </ul> <p>This approach completely eliminates the static asset loading issues you were experiencing with path-based routing.</p>"},{"location":"Ingress/Access%20Traefik%20Dashboard/","title":"Access Traefik Dashboard","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># Port forward to access Traefik dashboard\nkubectl port-forward -n traefik-system service/traefik 9000:9000\n\n# Then open browser to: http://localhost:9000/dashboard/\n\n$ kubectl port-forward -n kube-system service/traefik 9000:9000\nerror: Service traefik does not have a service port 9000\n</code></pre>"},{"location":"Ingress/Check%20Traefik%20Configuration%20logs%20for%20config%20issues/","title":"Check Traefik Configuration logs for config issues","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># Check Traefik deployment\nkubectl get deployment traefik -n traefik-system -o yaml\n\n# Check Traefik service (look for ports configuration)\nkubectl get service traefik -n traefik-system -o yaml\n\n# Check Traefik ConfigMap (if using static configuration)\nkubectl get configmap -n traefik-system\nkubectl describe configmap traefik-config -n traefik-system  # if exists\n\n# Check Traefik pods\nkubectl get pods -n traefik-system\nkubectl describe pod traefik-xxx-xxx -n traefik-system\n</code></pre>"},{"location":"Ingress/Check%20Traefik%20Configuration%20logs%20for%20config%20issues/#check-traefik-logs-for-configuration-issues","title":"Check Traefik Logs for Configuration Issues","text":"<pre><code># Check Traefik logs\nkubectl logs -n traefik-system deployment/traefik\n\n# Follow logs in real-time\nkubectl logs -n traefik-system deployment/traefik -f\n\n# Check for specific configuration errors\nkubectl logs -n traefik-system deployment/traefik | grep -i \"error\\|warn\\|config\"\n</code></pre>"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/","title":"Diff between Host path subdomain based ingress routing","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#routing-methods-real-time-issues-comparison","title":"Routing Methods - Real-Time Issues Comparison","text":""},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#main-issues-summary-table","title":"Main Issues Summary Table","text":"Issue Category Path-Based Routing Host-Based Routing Subdomain Routing Static Assets (CSS/JS) \ud83d\udd34 MAJOR ISSUE - Relative paths break, requires app modification \ud83d\udfe2 NO ISSUES - Works perfectly \ud83d\udfe2 NO ISSUES - Works perfectly DNS Management \ud83d\udfe2 SIMPLE - Single domain \ud83d\udd34 COMPLEX - Multiple domains to manage \ud83d\udfe1 MODERATE - Wildcard DNS setup SSL Certificates \ud83d\udfe2 SIMPLE - Single certificate \ud83d\udd34 COMPLEX - Multiple certificates \ud83d\udfe1 MODERATE - Wildcard certificate Application Changes \ud83d\udd34 REQUIRED - Apps must handle base paths \ud83d\udfe2 NONE - Apps work as-is \ud83d\udfe2 NONE - Apps work as-is Cost \ud83d\udfe2 LOW - Single load balancer \ud83d\udd34 HIGH - Multiple hostnames = higher costs \ud83d\udfe1 MODERATE - Single wildcard cert Cookie/Session Management \ud83d\udd34 CONFLICTS - Shared domain causes issues \ud83d\udfe2 ISOLATED - Separate cookie domains \ud83d\udfe1 CONFIGURABLE - Can share or isolate WebSocket Support \ud83d\udd34 PROBLEMATIC - Many libraries don't handle paths \ud83d\udfe2 PERFECT - Full support \ud83d\udfe2 PERFECT - Full support CORS Issues \ud83d\udfe2 NONE - Same origin \ud83d\udfe1 SOME - Cross-origin requests \ud83d\udfe1 SOME - Cross-subdomain requests SEO Impact \ud83d\udfe1 MODERATE - Paths affect ranking \ud83d\udd34 SEPARATE - Different domains compete \ud83d\udfe1 MODERATE - Subdomains share authority Debugging Complexity \ud83d\udd34 HIGH - Path conflicts hard to trace \ud83d\udfe2 LOW - Clear separation \ud83d\udfe2 LOW - Clear separation Legacy App Compatibility \ud83d\udd34 POOR - Often requires code changes \ud83d\udfe2 EXCELLENT - Works with any app \ud83d\udfe2 EXCELLENT - Works with any app"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#specific-real-time-production-issues","title":"Specific Real-Time Production Issues","text":"Problem Type Path-Based Host-Based Subdomain Broken Images/CSS \u274c Very Common (60-80% of apps) \u2705 Never happens \u2705 Never happens Authentication Conflicts \u274c Apps share cookies, login conflicts \u2705 Isolated auth per domain \u26a0\ufe0f Configurable sharing Load Balancer Costs \u2705 $50-100/month \u274c $50-100/month PER hostname \u26a0\ufe0f $50-100/month total Certificate Renewal \u2705 Single cert to manage \u274c Multiple certs, complex automation \u26a0\ufe0f Wildcard cert management DNS Propagation Delays \u2705 No delays for new services \u274c 24-48hr delays for new hostnames \u26a0\ufe0f Instant for new subdomains Mobile App Integration \u26a0\ufe0f Apps must handle base paths \u2705 Standard URL handling \u26a0\ufe0f Some apps struggle with subdomains API Conflicts \u274c <code>/api/users</code> conflicts between apps \u2705 Separate namespaces \u2705 Separate namespaces Caching Issues \u274c CDN caching becomes complex \u2705 Simple per-domain caching \u2705 Simple per-subdomain caching"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#implementation-effort-required","title":"Implementation Effort Required","text":"Task Path-Based Host-Based Subdomain Initial Setup \ud83d\udfe1 Medium (2-4 hours) \ud83d\udd34 High (1-2 days) \ud83d\udfe2 Easy (30 minutes) App Modifications \ud83d\udd34 High (varies by app) \ud83d\udfe2 None \ud83d\udfe2 None DevOps Automation \ud83d\udfe2 Simple \ud83d\udd34 Complex scripts needed \ud83d\udfe1 Moderate automation Monitoring Setup \ud83d\udd34 Complex (path-based metrics) \ud83d\udfe2 Standard per-host metrics \ud83d\udfe2 Standard per-subdomain metrics Troubleshooting \ud83d\udd34 High skill required \ud83d\udfe2 Standard techniques \ud83d\udfe2 Standard techniques"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#failure-impact-severity","title":"Failure Impact Severity","text":"Failure Type Path-Based Host-Based Subdomain CSS/JS Loading Fails \ud83d\udd34 CRITICAL - App completely broken \ud83d\udfe2 Never happens \ud83d\udfe2 Never happens Certificate Expires \ud83d\udfe1 All apps affected \ud83d\udd34 CRITICAL - Specific app down \ud83d\udfe1 All subdomains affected DNS Issues \ud83d\udfe1 All apps affected \ud83d\udd34 CRITICAL - App completely unreachable \ud83d\udfe1 Specific subdomain affected Load Balancer Problems \ud83d\udfe1 All apps affected \ud83d\udfe1 Specific app affected \ud83d\udfe1 All subdomains affected Session/Auth Issues \ud83d\udd34 CRITICAL - Cross-app login conflicts \ud83d\udfe2 Isolated impact \ud83d\udfe1 Configurable impact"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#real-production-examples","title":"Real Production Examples","text":"Company/Use Case Path-Based Host-Based Subdomain Netflix \u274c Not used \u274c Not used \u2705 api.netflix.com, assets.netflix.com GitHub \u2705 github.com/user/repo \u274c Not primary \u2705 api.github.com, gist.github.com Google \u274c Not used \u2705 gmail.com, drive.google.com \u2705 mail.google.com, docs.google.com Slack \u274c Not used \u274c Not used \u2705 api.slack.com, app.slack.com AWS Console \u2705 console.aws.amazon.com/s3 \u274c Not used \u2705 s3.amazonaws.com, ec2.amazonaws.com"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#recommendation-matrix","title":"Recommendation Matrix","text":"Your Situation Best Choice Why Small startup, budget-conscious \ud83d\udfe1 Path-Based Lowest cost, but prepare for CSS/JS issues Medium company, 5-20 services \u2705 Subdomain Best balance of simplicity and functionality Large enterprise, separate teams \u2705 Host-Based Maximum isolation, teams can manage independently Legacy applications \u274c Never Path-Based Use Subdomain or Host-Based SPA/React/Vue apps \ud83d\udfe1 Path-Based OK Modern frameworks handle base paths better Blue-Green deployments \u2705 Subdomain Easy switching between versions Microservices architecture \u2705 Subdomain Clean service separation"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#bottom-line-for-your-todo-app","title":"Bottom Line for Your Todo App","text":"Aspect Recommendation Current Issue CSS/JS not loading due to relative paths in path-based routing Quick Fix Switch to subdomain routing: <code>green.todoapp.localhost.com</code> Long-term Best Subdomain routing - most production-ready approach Avoid Continuing with path-based without fixing Nginx configuration The table clearly shows that your current CSS/JS loading issue is the #1 problem with path-based routing. For your specific todo app scenario: <p>\ud83c\udfaf Immediate Recommendation: Switch to subdomain routing (<code>green.todoapp.localhost.com</code>, <code>blue.todoapp.localhost.com</code>) - it eliminates your static asset problems completely and is the most production-ready approach for your use case.</p> <p>\ud83d\udd27 Alternative: If you must stick with path-based routing, use the Nginx configuration I provided earlier to properly handle the <code>/green</code> and <code>/blue</code> paths.</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/","title":"How to confirm Ingress resources, routes, and active requests","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To confirm Ingress resources, routes, and active requests, follow these steps:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#1-list-all-ingress-resources","title":"\u2705 1. List All Ingress Resources","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#kubectl-get-ingress-a","title":"<pre><code>kubectl get ingress -A\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#2-inspect-ingress-rules-and-routes","title":"\u2705 2. Inspect Ingress Rules and Routes","text":"<pre><code>kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Look for: - <code>Host</code> - <code>Path</code> - <code>Service Name</code> - <code>IngressClassName</code></p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#3-check-ingress-controller-logs-for-requests","title":"\u2705 3. Check Ingress Controller Logs for Requests","text":"<p>For NGINX: <pre><code>kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre></p> <p>For Traefik: <pre><code>kubectl logs -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre></p> <p>For Istio: <pre><code>kubectl logs -n istio-system -l app=istio-ingressgateway\n</code></pre></p> <p>Perfect \u2014 if Traefik is already installed by default with K3d, here's the minimal setup to route traffic using ClusterIP services like ALB:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#use-default-k3d-traefik-with-ingress-alb-style","title":"\u2705 Use Default K3d Traefik with Ingress (ALB Style)","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-1-exposing-traefik-ports-in-k3d-cluster-config-yaml-file","title":"**Step 1: exposing Traefik ports in k3d cluster config yaml file","text":"<p>You're exposing Traefik port 80 to host port <code>30080</code> like this: <pre><code>- port: 30080:80\n  nodeFilters:\n    - loadbalancer\n</code></pre></p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-2-deploy-sample-app","title":"Step 2: Deploy Sample App","text":"<pre><code>kubectl create namespace demo\n\nkubectl create deployment whoami --image=traefik/whoami -n demo\nkubectl expose deployment whoami --port=80 --name=whoami -n demo\n</code></pre> <p>\u2705 This app uses a <code>ClusterIP</code> service (default)</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-3-create-ingress-resource","title":"Step 3: Create Ingress Resource","text":"<p>whoami-ingress.yaml <pre><code># whoami-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: whoami-ingress\n  namespace: demo\nspec:\n  rules:\n    - host: whoami.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: whoami\n                port:\n                  number: 80\n</code></pre></p> <p>Apply it:</p> <pre><code>kubectl apply -f whoami-ingress.yaml\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#gousegousedevopsk3d-k-get-ingress-a-namespace-name-class-hosts-address-ports-age-demo-whoami-ingress-traefik-whoamilocalhost-172180217218031721804-80-8m13s","title":"<pre><code>gouse@gouse:~/DevOps/k3d$ k get ingress -A\nNAMESPACE   NAME             CLASS     HOSTS              ADDRESS                            PORTS   AGE\ndemo        whoami-ingress   traefik   whoami.localhost   172.18.0.2,172.18.0.3,172.18.0.4   80      8m13s\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-3-add-host-mapping","title":"Step 3: Add Host Mapping","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#sudo-echo-127001-whoamilocalhost-etchosts-localhost-for-homelab-sudo-echo-1721802-whoamilocalhost-etchosts-loadbalancer-ip-for-production-grade-setup-and-configurations","title":"<pre><code>sudo echo \"127.0.0.1 whoami.localhost\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 whoami.localhost\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-4-access-in-browser","title":"Step 4: Access in Browser","text":"<p>Open: http://whoami.localhost</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#optional-if-traefik-doesnt-route","title":"\u26a0\ufe0f Optional: If Traefik Doesn't Route","text":"<p>Check if the default Traefik has IngressClass enabled: <pre><code>kubectl get ingressclass\n</code></pre></p> <p>You should see something like: <pre><code>NAME      CONTROLLER             PARAMETERS   AGE\ntraefik   traefik.io/ingress-controller   ...   ...\n</code></pre></p> <p>If not, you can explicitly set it in your Ingress: <pre><code>spec:\n  ingressClassName: traefik\n</code></pre></p> <p>You're now running a ClusterIP-only app with Ingress routing through default Traefik, exactly like an AWS ALB setup. Let me know if you want TLS added.</p> <p>Here's a Traefik Dashboard Ingress manifest for your K3d setup using existing port <code>30900:9000</code> (from your YAML) and ClusterIP service:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#traefik-dashboardyaml","title":"\u2705 <code>traefik-dashboard.yaml</code>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#apiversion-networkingk8siov1-kind-ingress-metadata-name-traefik-dashboard-namespace-kube-system-annotations-traefikingresskubernetesiorouterentrypoints-web-spec-ingressclassname-traefik-rules-host-traefiklocalhost-http-paths-path-pathtype-prefix-backend-service-name-traefik-port-number-9000","title":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: traefik-dashboard\n  namespace: kube-system\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: traefik.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: traefik\n                port:\n                  number: 9000\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#etchosts-entry","title":"\u2705 <code>/etc/hosts</code> Entry","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#127001-traefiklocalhost","title":"<pre><code>127.0.0.1 traefik.localhost\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#access-dashboard","title":"\u2705 Access Dashboard","text":"<pre><code>http://traefik.localhost:30900\n</code></pre> <p>OR (if you prefer Ingress via port 80 \u2192 30080): <pre><code>http://traefik.localhost:30080\n</code></pre></p> <p>In that case, change <code>port: 9000</code> to <code>port: 80</code> in the Ingress and make sure Traefik listens on web entrypoint for both.</p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/","title":"How to confirm the exact Ingress Controller used in your Kubernetes cluster","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To confirm the exact Ingress Controller used in your Kubernetes cluster, follow these steps:</p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#1-check-ingressclass-resource","title":"\u2705 1. Check IngressClass Resource","text":"<p><pre><code>kubectl get ingressclass\n</code></pre> Output example: <pre><code>NAME             CONTROLLER                   ...\nnginx            k8s.io/ingress-nginx         ...\ntraefik          traefik.io/ingress-controller ...\n</code></pre></p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#2-describe-the-active-ingressclass","title":"\u2705 2. Describe the Active IngressClass","text":"<pre><code>kubectl describe ingressclass &lt;name&gt;\n</code></pre> <p>Look for the <code>controller:</code> field. It tells you which controller manages it.</p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#3-verify-the-ingress-controller-deployment","title":"\u2705 3. Verify the Ingress Controller Deployment","text":"<p>Use the controller name from above to filter pods: <pre><code>kubectl get pods --all-namespaces | grep ingress\n</code></pre></p> <p>Or more specifically: <pre><code>kubectl get deployments --all-namespaces | grep ingress\n</code></pre></p> <p>Look for deployments like:</p> <ul> <li><code>ingress-nginx-controller</code></li> <li><code>traefik</code></li> <li><code>istio-ingressgateway</code></li> </ul>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#4-confirm-by-ingress-resource","title":"\u2705 4. Confirm by Ingress Resource","text":"<p><pre><code>kubectl get ingress -A\n</code></pre> Then: <pre><code>kubectl describe ingress &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> Check if it refers to a specific <code>ingressClassName</code>.</p>"},{"location":"Ingress/Types%20of%20Ingress%20Controllers%20with%20their%20pros%2C%20cons%2C%20and%20pricing%20details/","title":"Types of Ingress Controllers with their pros, cons, and pricing details","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Principle NGINX HAProxy Traefik Kong Istio Gateway Traffic Entry \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes Path/Host Routing \u2705 Basic \u2705 Basic \u2705 Advanced \u2705 Advanced \u2705 Advanced (Envoy) TLS Termination \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes Request Auth \u274c (plugin) \u274c \u2705 Middleware \u2705 JWT/OIDC Plugins \u2705 Native JWT Rate Limiting \u274c (plugin) \u274c \u2705 Middleware \u2705 Native \u2705 Policy-based Metrics/Logging \ud83d\udfe1 Basic \ud83d\udfe1 Basic \u2705 Prometheus \u2705 Plugins \u2705 Envoy Metrics Dynamic Config Reload \u274c (reload) \u274c (reload) \u2705 Hot reload \u2705 CRDs \u2705 Envoy xDS Plugin Ecosystem \u274c \u274c \u2705 Middlewares \u2705 Strong Plugins \u274c No plugins Mesh/Zero Trust Support \u274c \u274c \u274c \u274c \u2705 Full support"},{"location":"Ingress/Types%20of%20Ingress%20Controllers%20with%20their%20pros%2C%20cons%2C%20and%20pricing%20details/#what-to-pick","title":"\u2705 What to pick","text":"Need Best Option Simple, stable ingress NGINX High performance, low memory HAProxy Dynamic routing + modern DevOps Traefik API gateway + plugin-based auth Kong Zero trust + mesh + advanced routing/policy Istio Ingress Controller Pros Cons Pricing NGINX Ingress Widely used, well-documented, flexible config, supports custom rules Limited advanced routing, lower performance at scale Free (Open Source), NGINX Plus is commercial (~$2,500/year/node) HAProxy Ingress High performance, low latency, advanced routing Smaller community, fewer features than NGINX Free (Open Source), Enterprise support available (custom pricing) Traefik Auto-discovery, native K8s, easy dashboard Learning curve, fewer advanced features vs. NGINX Free (Open Source), Traefik Enterprise (starts ~$2,000/year) Istio Ingress Gateway Deep integration with service mesh, security features (mTLS, etc.) Heavy, complex to configure and manage Free (Open Source) Kong Ingress Controller API Gateway features, plugins, scalability Paid features gated in Enterprise version OSS free, Kong Enterprise (starts ~$2,500+/year) AWS ALB Ingress Managed by AWS, auto-scalable, native integration AWS only, slower config propagation Pay-per-use (based on ALB usage - ~$0.0225/hr + LCU costs) GKE Ingress (Google) Managed, autoscaling, native GCP integration GCP only, limited customization Free tier + Pay-per-use (based on Load Balancer usage) Azure Application Gateway Ingress Native Azure integration, WAF support Azure only, limited custom routing Pay-per-use (~$0.008/hr + data processing fees) Ambassador Edge Stack API Gateway features, easy developer interface Full features need paid version OSS free, Enterprise (starts ~$3,000+/year) Contour Built on Envoy, fast updates, CRD support Smaller community, fewer features than Istio or NGINX Free (Open Source) <p>Let me know if you want a filtered list for cloud-native, performance-focused, or self-hosted only.</p>"},{"location":"Ingress/create%20ingress%20with%20traefik/","title":"Create ingress with traefik","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Ingress/create%20ingress%20with%20traefik/#ingress-access","title":"ingress access","text":"<pre><code># argocd-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-ingress\n  namespace: argocd\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: argocd.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:\n                  number: 80\n</code></pre> <p>update in /etc/hosts <pre><code>&lt;host_ip&gt; argocd.local\n\nthen access http://argocd.local\n</code></pre></p>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/","title":"Complete Traefik Setup Guide for k3d","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Great! I'm glad it's working now. Let me provide you with a comprehensive, clean setup guide for Traefik with dashboard on k3d:This guide provides a complete, production-ready setup for Traefik on k3d. The key points to remember:</p> <ol> <li>Disable default k3s Traefik to install your own version with full control</li> <li>Use NodePort service type with specific port mappings matching your k3d config</li> <li>Enable dashboard explicitly with <code>--api.dashboard=true</code> and <code>--api.insecure=true</code> flags</li> <li>Access patterns:<ul> <li>Direct dashboard: <code>http://localhost:30900/dashboard/</code></li> <li>Through ingress: <code>http://traefik.localhost:30080/</code></li> </ul> </li> <li>Always include the trailing slash when accessing <code>/dashboard/</code></li> </ol> <p>The setup is now modular and easy to extend with additional services, middleware, and security features. You can use this as a template for all your k3d clusters with Traefik.</p>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#complete-traefik-setup-guide-for-k3d","title":"Complete Traefik Setup Guide for k3d","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-1-create-k3d-cluster-configuration","title":"Step 1: Create k3d Cluster Configuration","text":"<p>Create <code>dev-cluster.yaml</code>:</p> <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:9000     # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\n  # Add your other application ports here\noptions:\n  k3s:\n    extraArgs:\n      - arg: --disable=traefik\n        nodeFilters:\n          - server:*\n  kubeconfig:\n    updateDefaultKubeconfig: true\n    switchCurrentContext: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-2-create-the-cluster","title":"Step 2: Create the Cluster","text":"<pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-3-install-traefik-with-helm","title":"Step 3: Install Traefik with Helm","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#31-add-helm-repository","title":"3.1 Add Helm Repository","text":"<pre><code>helm repo add traefik https://traefik.github.io/charts\nhelm repo update\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#32-create-traefik-namespace","title":"3.2 Create Traefik Namespace","text":"<pre><code>kubectl create namespace traefik-system\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#33-create-traefik-values-file","title":"3.3 Create Traefik Values File","text":"<p>Create <code>traefik-values.yaml</code>:</p> <pre><code># Traefik Helm Chart Values\nimage:\n  pullPolicy: IfNotPresent\n\ndeployment:\n  enabled: true\n  replicas: 1\n  annotations: {}\n  podAnnotations: {}\n  additionalContainers: []\n  initContainers: []\n\n# Configure ports\nports:\n  web:\n    port: 8000\n    expose:\n      default: true\n    exposedPort: 80\n    nodePort: 30080\n    protocol: TCP\n  websecure:\n    port: 8443\n    expose:\n      default: true\n    exposedPort: 443\n    nodePort: 30443\n    protocol: TCP\n  traefik:\n    port: 9000\n    expose:\n      default: true\n    exposedPort: 9000\n    nodePort: 30900\n    protocol: TCP\n\n# Service configuration\nservice:\n  enabled: true\n  type: NodePort\n  annotations: {}\n  labels: {}\n  spec: {}\n  loadBalancerSourceRanges: []\n  externalIPs: []\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    annotations: {}\n    labels: {}\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true\n\n# Configure providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n    allowExternalNameServices: true\n    allowEmptyServices: true\n  kubernetesIngress:\n    enabled: true\n    allowExternalNameServices: true\n    allowEmptyServices: true\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--global.checknewversion=false\"\n  - \"--global.sendanonymoususage=false\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--api.dashboard=true\"\n  - \"--api.insecure=true\"\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n  - \"--entrypoints.web.http.redirections.entrypoint.to=websecure\"\n  - \"--entrypoints.web.http.redirections.entrypoint.scheme=https\"\n\n# Enable persistent storage for acme certificates\npersistence:\n  enabled: false\n  name: data\n  accessMode: ReadWriteOnce\n  size: 128Mi\n  storageClass: \"\"\n  path: /data\n  annotations: {}\n\n# Security configurations\npodSecurityContext:\n  fsGroup: 65532\n\nsecurityContext:\n  capabilities:\n    drop: [ALL]\n  readOnlyRootFilesystem: true\n  runAsGroup: 65532\n  runAsNonRoot: true\n  runAsUser: 65532\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#34-install-traefik","title":"3.4 Install Traefik","text":"<pre><code>helm install traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values.yaml \\\n  --wait\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-4-configure-dashboard-access","title":"Step 4: Configure Dashboard Access","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#41-create-ingressroute-for-dashboard","title":"4.1 Create IngressRoute for Dashboard","text":"<p>Create <code>traefik-dashboard-ingress.yaml</code>:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik-system\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n      kind: Rule\n      services:\n        - name: api@internal\n          kind: TraefikService\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f traefik-dashboard-ingress.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#42-optional-secure-dashboard-with-basic-auth","title":"4.2 (Optional) Secure Dashboard with Basic Auth","text":"<p>Generate password:</p> <pre><code># Generate password (replace 'admin' and 'your-password')\nhtpasswd -nb admin your-password | base64\n</code></pre> <p>Create <code>traefik-dashboard-auth.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\ndata:\n  users: |\n    &lt;base64-encoded-htpasswd-output&gt;\n---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard-secure\n  namespace: traefik-system\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n      kind: Rule\n      middlewares:\n        - name: traefik-dashboard-auth\n      services:\n        - name: api@internal\n          kind: TraefikService\n</code></pre> <p>Apply authentication:</p> <pre><code>kubectl apply -f traefik-dashboard-auth.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-5-configure-dns-optional","title":"Step 5: Configure DNS (Optional)","text":"<p>Add to <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 traefik.localhost\n127.0.0.1 app.localhost\n# Add other domains as needed\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-6-deploy-sample-application","title":"Step 6: Deploy Sample Application","text":"<p>Create <code>whoami-app.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  selector:\n    app: whoami\n  ports:\n    - port: 80\n      targetPort: 80\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`whoami.localhost`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre> <p>Deploy the application:</p> <pre><code>kubectl apply -f whoami-app.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-7-access-methods","title":"Step 7: Access Methods","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#dashboard-access","title":"Dashboard Access:","text":"<ul> <li>Direct Port: http://localhost:30900/dashboard/</li> <li>Via Ingress: http://traefik.localhost:30080/dashboard/</li> </ul>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#application-access","title":"Application Access:","text":"<ul> <li>Via Ingress: http://whoami.localhost:30080/</li> </ul>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-8-useful-commands","title":"Step 8: Useful Commands","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#check-traefik-status","title":"Check Traefik Status:","text":"<pre><code># Check pods\nkubectl get pods -n traefik-system\n\n# Check services\nkubectl get svc -n traefik-system\n\n# Check logs\nkubectl logs -n traefik-system -l app.kubernetes.io/name=traefik -f\n\n# Check IngressRoutes\nkubectl get ingressroute -A\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#debug-commands","title":"Debug Commands:","text":"<pre><code># Check if dashboard is accessible\ncurl http://localhost:30900/api/overview\n\n# Test ingress routing\ncurl -H \"Host: whoami.localhost\" http://localhost:30080/\n\n# Port forward for debugging\nkubectl port-forward -n traefik-system deployment/traefik 9000:9000\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-9-common-middleware-examples","title":"Step 9: Common Middleware Examples","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#rate-limiting","title":"Rate Limiting:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: rate-limit\nspec:\n  rateLimit:\n    average: 100\n    burst: 50\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#headers","title":"Headers:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: security-headers\nspec:\n  headers:\n    frameDeny: true\n    sslRedirect: true\n    browserXssFilter: true\n    contentTypeNosniff: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#redirect","title":"Redirect:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: redirect-https\nspec:\n  redirectScheme:\n    scheme: https\n    permanent: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#troubleshooting","title":"Troubleshooting","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#dashboard-not-accessible","title":"Dashboard Not Accessible:","text":"<ol> <li> <p>Check if the service has port 9000 exposed:</p> <pre><code>kubectl get svc traefik -n traefik-system -o yaml | grep -A 5 \"port: 9000\"\n</code></pre> </li> <li> <p>Verify dashboard is enabled:</p> <pre><code>kubectl get deployment traefik -n traefik-system -o yaml | grep -E \"(dashboard|insecure)\"\n</code></pre> </li> <li> <p>Check endpoints:</p> <pre><code>kubectl get endpoints -n traefik-system\n</code></pre> </li> </ol>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#ingress-not-working","title":"Ingress Not Working:","text":"<ol> <li> <p>Check IngressRoute:</p> <pre><code>kubectl describe ingressroute -n &lt;namespace&gt; &lt;name&gt;\n</code></pre> </li> <li> <p>Check Traefik logs for errors:</p> <pre><code>kubectl logs -n traefik-system -l app.kubernetes.io/name=traefik --tail=100\n</code></pre> </li> </ol>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#best-practices","title":"Best Practices","text":"<ol> <li>Always use HTTPS in production - Configure proper TLS certificates</li> <li>Secure the dashboard - Use authentication middleware</li> <li>Use namespaces - Organize your IngressRoutes by namespace</li> <li>Monitor logs - Set up proper logging and monitoring</li> <li>Resource limits - Set appropriate resource requests and limits</li> <li>Regular updates - Keep Traefik updated for security patches</li> </ol> <p>This setup provides a solid foundation for using Traefik in your k3d development environment!</p>"},{"location":"K3d/K3d%20cluster%20setup%20with%20HA/","title":"K3d cluster setup with HA","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <ul> <li>1 server node \u2192 uses SQLite (default for k3s)  </li> <li>3 or more server nodes \u2192 uses embedded etcd</li> </ul> <pre><code>k3d cluster create ha-cluster \\\n  --servers 3 \\\n  --agents 2 \\\n  --wait\n\ngouse@gouse:~/DevOps/k3d$ k get nodes\nNAME                      STATUS   ROLES                       AGE   VERSION\nk3d-ha-cluster-agent-0    Ready    &lt;none&gt;                      35s   v1.31.5+k3s1\nk3d-ha-cluster-agent-1    Ready    &lt;none&gt;                      35s   v1.31.5+k3s1\nk3d-ha-cluster-server-0   Ready    control-plane,etcd,master   71s   v1.31.5+k3s1\nk3d-ha-cluster-server-1   Ready    control-plane,etcd,master   52s   v1.31.5+k3s1\nk3d-ha-cluster-server-2   Ready    control-plane,etcd,master   39s   v1.31.5+k3s1\n</code></pre> <pre><code>$ k3d cluster delete ha-cluster\n\n# Traefik ingress disable\n$ k3d cluster create ha-cluster \\\n  --servers 3 \\\n  --agents 2 \\\n  --k3s-arg \"--cluster-init@server:0\" \\\n  --k3s-arg \"--server=https://k3d-ha-cluster-server-0:6443@server:1\" \\\n  --k3s-arg \"--server=https://k3d-ha-cluster-server-0:6443@server:2\" \\\n  --k3s-arg \"--tls-san=127.0.0.1@server:0\" \\\n  --k3s-arg \"--disable=traefik@server:*\" \\\n  --k3s-arg \"--disable=metrics-server@server:*\" \\\n  --k3s-arg \"--disable-network-policy@server:*\" \\\n  --wait\n\n\n# Without ingress disable, ingress enable by default by k3d(k3s inside)\n$ k3d cluster delete ha-cluster\n\n$ k3d cluster create ha-cluster \\\n  --servers 3 \\\n  --agents 2 \\\n  --k3s-arg \"--cluster-init@server:0\" \\\n  --k3s-arg \"--server=https://k3d-ha-cluster-server-0:6443@server:1\" \\\n  --k3s-arg \"--server=https://k3d-ha-cluster-server-0:6443@server:2\" \\\n  --k3s-arg \"--tls-san=127.0.0.1@server:0\" \\\n  --wait\n</code></pre> <p>$ k3d cluster create --config k3d-etcd-ha.yaml (it wont restart nodes, if host machine restarted) <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: ha-cluster\nservers: 3\nagents: 2\noptions:\n  k3s:\n    extraArgs:\n      - arg: \"--cluster-init\"\n        nodeFilters:\n          - server:0\n      - arg: \"--server=https://k3d-ha-cluster-server-0:6443\"\n        nodeFilters:\n          - server:1\n          - server:2\n      - arg: \"--tls-san=127.0.0.1\"\n        nodeFilters:\n          - server:0\n      - arg: \"--disable=servicelb\"\n        nodeFilters:\n          - server:*\n      - arg: \"--disable=metrics-server\"\n        nodeFilters:\n          - server:*\nports:\n  - port: 80:80\n    nodeFilters:\n      - loadbalancer\n  - port: 443:443\n    nodeFilters:\n      - loadbalancer\n</code></pre></p> <p>we fixed the cluster to connect without issues by adding 6443 port.</p> <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: ha-cluster\nservers: 3\nagents: 2\noptions:\n  k3s:\n    extraArgs:\n      - arg: \"--cluster-init\"\n        nodeFilters:\n          - server:0\n      - arg: \"--server=https://k3d-ha-cluster-server-0:6443\"\n        nodeFilters:\n          - server:1\n          - server:2\n      - arg: \"--tls-san=127.0.0.1\"\n        nodeFilters:\n          - server:0\n      - arg: \"--disable=servicelb\"\n        nodeFilters:\n          - server:*\n      - arg: \"--disable=metrics-server\"\n        nodeFilters:\n          - server:*\nports:\n  - port: 80:80\n    nodeFilters:\n      - loadbalancer\n  - port: 443:443\n    nodeFilters:\n      - loadbalancer\n  - port: 6443:6443\n    nodeFilters:\n      - loadbalancer\n</code></pre>"},{"location":"K3d/K3d%20cluster%20setup/","title":"K3d cluster setup","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p><code>k3d</code> also supports cluster creation using a YAML config file, similar to `kind.</p>"},{"location":"K3d/K3d%20cluster%20setup/#1-sample-k3d-cluster-config-yaml-k3d-configyaml","title":"\u2705 1. Sample k3d Cluster Config YAML (<code>k3d-config.yaml</code>)","text":"<pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:3000     # Grafana (optional)\n    nodeFilters:\n      - loadbalancer\n</code></pre>"},{"location":"K3d/K3d%20cluster%20setup/#2-create-cluster-using-config","title":"\u2705 2. Create Cluster Using Config","text":"<p><pre><code>k3d cluster create --config k3d-config.yaml\n</code></pre> This will expose your services directly on the host ports without needing port-forward. You can add or change ports anytime by editing the config.</p>"},{"location":"K3d/k3d%20and%20kind%20commands/","title":"K3d and kind commands","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"K3d/k3d%20and%20kind%20commands/#kind-admin-commands","title":"\ud83d\udfea Kind Admin Commands","text":"<pre><code># Create a cluster\nkind create cluster --name demo\nkind create cluster --config dev-cluster.yaml\n\n# List clusters\nkind get clusters\n\n# Delete cluster\nkind delete cluster --name demo\n\n# Get kubeconfig path\nkind get kubeconfig-path --name demo\n\n# Load docker image into kind cluster\nkind load docker-image myimage:tag --name demo\n\n# Export logs\nkind export logs ./logs --name demo\n</code></pre>"},{"location":"K3d/k3d%20and%20kind%20commands/#k3d-admin-commands","title":"\ud83d\udfe9 K3d Admin Commands","text":"<pre><code># Create a cluster\nk3d cluster create demo\nked create cluster --config dev-cluster.yaml\n\n# List clusters\nk3d cluster list\n\n# Get kubeconfig\nk3d kubeconfig get demo\n\n# Delete cluster\nk3d cluster delete demo\n\n# Add new node to cluster\nk3d node create new-agent --role=agent --cluster=demo\n\n# Start/Stop cluster\nk3d cluster stop demo\nk3d cluster start demo\n\n# Load image into k3d cluster\nk3d image import myimage:tag -c demo\n\n# List nodes\nk3d node list\n</code></pre>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/","title":"K3d cluster with traefik dashboard","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To enable and expose the Traefik dashboard in <code>k3d</code> at cluster creation, you need to do two things in the config file:</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#1-enable-traefik-dashboard-via-k3s-arg","title":"\u2705 1. Enable Traefik Dashboard via k3s Arg","text":"<p>Add this under the <code>k3s</code> section:</p> <pre><code>k3s:\n  extraArgs:\n    - arg: \"--traefik.dashboard=true\"\n      nodeFilters:\n        - server:*\n</code></pre>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#2-expose-traefik-dashboard-port-default-is-9000","title":"\u2705 2. Expose Traefik Dashboard Port (default is 9000)","text":"<p>Add this under the <code>ports:</code> section: <pre><code>  - port: 30900:9000      # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\n</code></pre></p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#final-config-k3d-configyaml","title":"\u2705 \u2705 Final Config: <code>k3d-config.yaml</code>","text":"<pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:3000     # Grafana (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:9000     # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\nk3s:                      # not working for k3s version, so you can remove it.\n  extraArgs:\n    - arg: \"--traefik.dashboard=true\"\n      nodeFilters:\n        - server:*\n</code></pre> <p>but it getting below error for k3s  <pre><code> k3d cluster create --config dev-cluster.yaml\nFATA[0000] Schema Validation failed for config file dev-cluster.yaml: - (root): Additional property k3s is not allowed\n</code></pre></p> <p>You're seeing the error because <code>k3d v5+</code> no longer supports the <code>k3s:</code> block in the config file. That field is not valid in the latest schema (v1alpha5).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#solution-use-k3s-arg-from-cli-instead","title":"\u2705 Solution: Use <code>--k3s-arg</code> from CLI instead","text":"<p>Yes, exactly.</p> <p>Remove the entire <code>k3s:</code> section from your config file.</p> <p>Use only the cluster definition and port mappings in the YAML, then pass k3s arguments like <code>--traefik.dashboard=true</code> via the CLI when creating the cluster.</p> <p>Since the config file doesn't support <code>k3s:</code> anymore, you can enable the Traefik dashboard like this:</p> <pre><code>k3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre> <p>This passes the arg <code>--traefik.dashboard=true</code> to the first server node.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#optional-alias-for-simplicity","title":"\u2705 Optional: Alias for Simplicity","text":"<p>If you want to reuse this, put it in a shell script:</p> <pre><code>#!/bin/bash\nk3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:*\"\n</code></pre> <p>Let me know if you want a workaround to fully control the k3s startup args from config (e.g., custom entrypoint container).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#why-cant-we-use-these-values-in-config-file","title":"why can't we use these values in config file ?","text":"<p>Great question \u2014 here\u2019s a straight answer:---</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#why-you-cant-use-k3s-in-k3d-config-anymore","title":"\ud83d\udd34 Why you can't use <code>k3s:</code> in <code>k3d</code> config anymore","text":"<ul> <li>In <code>k3d v5+</code>, the config file follows a strict JSON schema (<code>v1alpha5</code>) that does not include a <code>k3s:</code> field.</li> <li>The field was removed to simplify the config format and separate cluster structure (YAML) from runtime behavior (CLI flags).</li> <li>So, <code>k3s:</code> and <code>extraArgs</code> were deprecated and replaced by <code>--k3s-arg</code> flags in the CLI only.</li> </ul>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#current-rule","title":"\u2705 Current Rule:","text":"<ul> <li>Cluster topology (nodes, ports, volumes, registries) \u2192 \u2705 defined in YAML.</li> <li>Runtime args (like <code>--traefik.*</code>, <code>--disable-*</code>, <code>--tls-san</code>, etc.) \u2192 \u274c not in YAML \u2192 \u2705 only via CLI <code>--k3s-arg</code>.</li> </ul>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#legacy-compatibility","title":"\ud83d\udd04 Legacy Compatibility?","text":"<p>No official backward support. Even if you add <code>k3s:</code> in the YAML, <code>k3d</code> will fail schema validation.</p> <p>Let me know if you want to build a wrapper that merges YAML + CLI for easier reuse.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#access-traefik-dashboard","title":"\ud83c\udf10 Access Traefik Dashboard:","text":"<p>After cluster creation:</p> <pre><code>http://localhost:30900/dashboard/\n</code></pre> <p>Let me know if you want to secure it or route it via Ingress host like <code>traefik.localhost</code>.</p> <p>If your cluster creation hangs on starting the server, here are the most common and effective fixes:</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#1-check-for-docker-conflicts","title":"\u2705 1. Check for Docker Conflicts","text":"<p>Run this to see if containers are stuck or conflicting: <pre><code>docker ps -a | grep k3d\n</code></pre></p> <p>If any containers are in a <code>Created</code> or <code>Exited</code> state, clean them: <pre><code>k3d cluster delete dev-cluster\ndocker system prune -f\n</code></pre> Then try again.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#2-make-sure-ports-arent-already-in-use","title":"\u2705 2. Make Sure Ports Aren\u2019t Already in Use","text":"<p>You\u2019re mapping ports like <code>31446</code>, <code>31447</code>, etc. Run: <pre><code>lsof -i -P -n | grep LISTEN\n</code></pre></p> <p>Look for conflicting ports. If a port is in use, either: - Stop the app using it, or - Change the port in <code>dev-cluster.yaml</code></p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#3-disable-traefik-temporarily","title":"\u2705 3. Disable Traefik Temporarily","text":"<p>Test if the arg is causing the issue by removing it: <pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre> If that works, try: <pre><code>k3d cluster create --config dev-cluster.yaml --k3s-arg \"--disable=traefik@server:0\"\n</code></pre> Then install your own ingress later (e.g., Traefik/NGINX manually).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#4-increase-docker-memory","title":"\u2705 4. Increase Docker Memory","text":"<p>If your Docker is low on memory (common with 2+ agents), bump it up in Docker Desktop:</p> <p>Settings \u2192 Resources \u2192 Memory \u2192 Increase to at least 4\u20136 GB</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#5-check-logs","title":"\u2705 5. Check Logs","text":"<p>If it still hangs, grab logs from the server node: <pre><code>docker logs k3d-dev-cluster-server-0\n</code></pre> Paste the output here if you'd like me to help debug further.</p> <p>Thanks \u2014 this confirms it clearly:</p> <p>Port 9000 is mapped, but nothing is listening on it inside the cluster. \u2192 That means the Traefik dashboard is not enabled in your k3s setup.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#fix-recreate-cluster-with-dashboard-enabled","title":"\u2705 Fix: Recreate Cluster with Dashboard Enabled","text":"<p>You need to add this flag during cluster creation: <pre><code>--k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre></p> <p>So update your <code>k3d</code> create command like: <pre><code>k3d cluster delete dev-cluster\n\nk3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre></p> <p>Make sure this is already in your <code>dev-cluster.yaml</code>: <pre><code>- port: 30900:9000\n  nodeFilters:\n    - loadbalancer\n</code></pre></p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#optional-also-expose-dashboard-via-ingress-cleaner","title":"\ud83d\udd0d Optional: Also expose dashboard via ingress (cleaner)","text":"<p>You can create an ingress like this (after cluster up): <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: traefik-dashboard\n  namespace: kube-system\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: kube-system-auth@kubernetescrd\nspec:\n  rules:\n    - host: traefik.localhost\n      http:\n        paths:\n          - path: /dashboard/\n            pathType: Prefix\n            backend:\n              service:\n                name: traefik\n                port:\n                  number: 9000\n</code></pre></p> <p>Then access: <pre><code>http://traefik.localhost/dashboard/\n</code></pre></p> <p>(using a <code>hosts</code> file entry if needed)</p> <p>Let me know once you recreate the cluster with the dashboard flag \u2014 you should be able to access: <pre><code>http://localhost:30900/dashboard/\n</code></pre></p> <p>To access <code>http://traefik.localhost/dashboard/</code> from your browser, you need to map the custom domain <code>traefik.localhost</code> to your local machine.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#steps-to-add-entry-in-etchosts","title":"\u2705 Steps to Add Entry in <code>/etc/hosts</code>","text":"<ol> <li>Open the <code>/etc/hosts</code> file with root privileges:     <pre><code>sudo nano /etc/hosts\n</code></pre></li> <li>Add this line at the end of the file:     <pre><code>127.0.0.1 traefik.localhost\n</code></pre></li> <li>Save and exit:<ul> <li>Press <code>Ctrl + O</code> to save</li> <li>Press <code>Enter</code> to confirm</li> <li>Press <code>Ctrl + X</code> to exit</li> </ul> </li> </ol>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#after-that","title":"\ud83d\udccc After That","text":"<p>Make sure: - You\u2019ve created an Ingress with <code>host: traefik.localhost</code> - Traefik is listening on port 80 (mapped via <code>30080:80@loadbalancer</code>)</p> <p>Then access: <pre><code>http://traefik.localhost:30080/dashboard/\n</code></pre></p> <p>(or just <code>http://traefik.localhost/dashboard/</code> if port 80 is directly forwarded)</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/","title":"K3d with traefik dashboard setup","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Your config file looks good, but I notice a few things to optimize. Here's an improved version with some corrections and the steps to deploy Traefik with dashboard access:</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#1-updated-k3d-cluster-config-2-create-the-cluster","title":"1. Updated k3d cluster config## 2. Create the cluster","text":"<p>create dev-cluster confi file to setup a cluster by disabling traefik  <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:30099    # Grafana (optional) - Fixed port mapping\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:30900    # Traefik Dashboard - Fixed port mapping\n    nodeFilters:\n      - loadbalancer\noptions:\n  k3s:\n    extraArgs:\n      - arg: --disable=traefik\n        nodeFilters:\n          - server:*\n  kubeconfig:\n    updateDefaultKubeconfig: true\n    switchCurrentContext: true\n</code></pre> create a cluster with this config file <pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#3-install-traefik-using-helm","title":"3. Install Traefik using Helm","text":"<p>Since we're disabling the default k3s Traefik, we'll install it manually with custom configuration:</p> <p>create helm traefik values: <pre><code># Traefik Helm Chart Values\nimage:\n  tag: \"3.0\"\n\ndeployment:\n  replicas: 1\n\nservice:\n  type: NodePort\n  # Map standard ports\n  ports:\n    web:\n      port: 80\n      nodePort: 30080\n    websecure:\n      port: 443\n      nodePort: 30443\n    traefik:\n      port: 9000\n      expose: true\n      nodePort: 30900\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n    entryPoints: [\"traefik\"]\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true  # For development only\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Enable providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n  kubernetesIngress:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--api.insecure=true\"\n  - \"--api.dashboard=true\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n  - \"--metrics.prometheus=true\"\n  - \"--metrics.prometheus.entryPoint=metrics\"\n\n# Prometheus metrics\nmetrics:\n  prometheus:\n    entryPoint: metrics\n\n# Enable persistent storage for certificates (optional)\npersistence:\n  enabled: false\n</code></pre></p> <pre><code># Add Traefik Helm repository\nhelm repo add traefik https://traefik.github.io/charts\nhelm repo update\n\n# Create namespace\nkubectl create namespace traefik-system\n\n# Create Traefik values file\n## 4. Install Traefik\n\nhelm install traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values.yaml \\\n  --wait\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#5-verify-traefik-installation","title":"5. Verify Traefik installation","text":"<pre><code># Check pods\nkubectl get pods -n traefik-system\n\n# Check services\nkubectl get svc -n traefik-system\n\n# Check if dashboard is accessible\ncurl http://localhost:30900/dashboard/\n# checked not accessible, getting below error\ngouse@gouse:~$ curl http://localhost:30900/dashboard/\ncurl: (52) Empty reply from server\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#6-create-an-ingressroute-for-easier-dashboard-access-optionalapply-the-ingressroute","title":"6. Create an IngressRoute for easier dashboard access (optional)Apply the IngressRoute:","text":"<p>create traefik dashboard ingress route traefik-dashboard-ingress.yaml <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik-system\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.dev.local`)\n      kind: Rule\n      services:\n        - name: api@internal\n          kind: TraefikService\n---\n# Optional: Middleware for basic auth\napiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\ntype: kubernetes.io/basic-auth\nstringData:\n  username: admin\n  password: changeme\n---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n</code></pre></p> <pre><code>kubectl apply -f traefik-dashboard-ingress.yaml\n\ngouse@gouse:~/DevOps/k3d$ kubectl apply -f traefik-dashboard-ingress.yaml\nWarning: resource ingressroutes/traefik-dashboard is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ningressroute.traefik.io/traefik-dashboard configured\nsecret/traefik-dashboard-auth created\nmiddleware.traefik.io/traefik-dashboard-auth created\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#7-access-the-traefik-dashboard","title":"7. Access the Traefik Dashboard","text":"<p>You can now access the Traefik dashboard in multiple ways:</p> <ol> <li>Direct NodePort access: http://localhost:30900/dashboard/ <pre><code>in browser, it tried but getting as below\n\n# The connection was reset\nThe connection to the server was reset while the page was loading.\n</code></pre></li> <li>Via Ingress (if you added the host to /etc/hosts): http://traefik.dev.local</li> </ol>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#8-deploy-a-sample-application-to-test-traefikdeploy-the-sample-app","title":"8. Deploy a sample application to test TraefikDeploy the sample app:","text":"<p>create a sample app: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  selector:\n    app: whoami\n  ports:\n    - port: 80\n      targetPort: 80\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`whoami.dev.local`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre></p> <pre><code>kubectl apply -f sample-app.yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#important-notes","title":"Important Notes:","text":"<ol> <li> <p>Port mappings: I fixed the port mappings for Grafana and Traefik dashboard. They should map to the same ports (30099:30099 and 30900:30900).</p> </li> <li> <p>Traefik installation: Since k3s comes with Traefik by default, I added the <code>--disable=traefik</code> flag to install our own version with custom configuration.</p> </li> <li> <p>Dashboard access: The dashboard will be available at http://localhost:30900/dashboard/ (note the trailing slash).</p> </li> <li> <p>Security: For production, always secure the dashboard with authentication and use HTTPS.</p> </li> <li> <p>Host entries: Add these to your <code>/etc/hosts</code> file for easier access:</p> <pre><code>127.0.0.1 traefik.dev.local\n127.0.0.1 whoami.dev.local\n</code></pre> </li> </ol> <p>This setup gives you full control over Traefik configuration and makes it easy to add more services with their respective ingress routes.</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#post-above-setup-my-observations-on-not-working","title":"post above setup, my observations on not working","text":"<pre><code>in my k3d conifg file i routed with below details for traefik - port: 30900:30900 # Traefik Dashboard - Fixed port mapping nodeFilters: - loadbalancer\n\nbut created service with different port, i think this is the issue which not able to access the curl [http://localhost:30900/dashboard/](http://localhost:30900/dashboard/) curl: (52) Empty reply from server\n\n$ kubectl get svc -n traefik-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik NodePort 10.43.48.148 &lt;none&gt; 80:32560/TCP,443:31788/TCP 22m\n\nand also i added values in /etc/hosts 127.0.0.1 traefik.dev.local 127.0.0.1 whoami.dev.local\n\nand i think it should be loadbalancer instead of nodeport for traefik-system svc what could be the issue, that i have not able to access\n</code></pre> <p>You're absolutely right! The issue is that the Traefik service isn't exposing port 9000 (the dashboard port) and the NodePort mappings don't match your k3d configuration. Let's fix this:</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#1-first-lets-check-the-current-traefik-service-configuration","title":"1. First, let's check the current Traefik service configuration:","text":"<pre><code>kubectl get svc traefik -n traefik-system -o yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#2-update-the-traefik-values-file-to-properly-expose-the-dashboard-port-3-upgrade-traefik-with-the-corrected-configuration","title":"2. Update the Traefik values file to properly expose the dashboard port:## 3. Upgrade Traefik with the corrected configuration:","text":"<p>update traefik-values-fixed.yaml <pre><code># Traefik Helm Chart Values - Fixed Schema\nimage:\n  tag: \"3.0\"\n\ndeployment:\n  replicas: 1\n\nservice:\n  type: NodePort\n  spec: {}\n\nports:\n  web:\n    port: 8000\n    expose:\n      default: true\n    exposedPort: 80\n    nodePort: 30080\n    protocol: TCP\n  websecure:\n    port: 8443\n    expose:\n      default: true\n    exposedPort: 443\n    nodePort: 30443\n    protocol: TCP\n  traefik:\n    port: 9000\n    expose:\n      default: true\n    exposedPort: 9000\n    nodePort: 30900\n    protocol: TCP\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\n    entryPoints: [\"traefik\"]\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true  # For development only\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Enable providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n  kubernetesIngress:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--api.insecure=true\"\n  - \"--api.dashboard=true\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n\n# Enable persistent storage for certificates (optional)\npersistence:\n  enabled: false\n</code></pre></p> <pre><code>helm upgrade traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values-fixed.yaml \\\n  --wait\n</code></pre> <pre><code>$ helm upgrade traefik traefik/traefik   --namespace traefik-system   --values traefik-values-fixed.yaml   --wait\nRelease \"traefik\" has been upgraded. Happy Helming!\nNAME: traefik\nLAST DEPLOYED: Sat Jun  7 11:15:27 2025\nNAMESPACE: traefik-system\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\ntraefik with docker.io/traefik:3.0 has been deployed successfully on traefik-system namespace !\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#4-if-the-above-doesnt-work-k3d-sometimes-has-issues-with-loadbalancer-type-lets-patch-the-service-directlyapply-the-patch","title":"4. If the above doesn't work (k3d sometimes has issues with LoadBalancer type), let's patch the service directly:Apply the patch:","text":"<pre><code>kubectl apply -f traefik-service-patch.yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#5-alternative-create-a-separate-service-for-the-dashboardbash","title":"5. Alternative: Create a separate service for the dashboard:```bash","text":"<p>kubectl apply -f traefik-dashboard-service.yaml</p> <pre><code>## 6. Verify the service has the correct ports:\n\n```bash\n# Check services\nkubectl get svc -n traefik-system\n\n# You should see something like:\n# NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                     AGE\n# traefik             NodePort   10.43.48.148    &lt;none&gt;        80:30080/TCP,443:30443/TCP,9000:30900/TCP  22m\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#7-check-if-traefik-pod-is-exposing-port-9000","title":"7. Check if Traefik pod is exposing port 9000:","text":"<pre><code># Get pod name\nkubectl get pods -n traefik-system\n\n# Check if port 9000 is exposed\nkubectl describe pod -n traefik-system &lt;traefik-pod-name&gt; | grep -A 5 \"Ports:\"\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#8-test-access-again","title":"8. Test access again:","text":"<pre><code># Direct dashboard access\ncurl http://localhost:30900/dashboard/\n\n# Or check the API endpoint\ncurl http://localhost:30900/api/rawdata\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#9-if-still-having-issues-check-traefik-logs","title":"9. If still having issues, check Traefik logs:","text":"<pre><code>kubectl logs -n traefik-system -l app.kubernetes.io/name=traefik -f\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#quick-debug-commands","title":"Quick Debug Commands:","text":"<pre><code># Check if the port is actually listening\nkubectl exec -n traefik-system deployment/traefik -- netstat -tlnp | grep 9000\n\n# Port forward directly to test\nkubectl port-forward -n traefik-system deployment/traefik 9000:9000\n# Then try: curl http://localhost:9000/dashboard/\n</code></pre> <p>The main issue was that the Traefik service wasn't configured to expose port 9000 with the correct NodePort (30900) that matches your k3d configuration. The above solutions should fix this issue.</p> <p>Still not working: <pre><code>https://traefik.dev.local:30900/dashboard/` also getting 404 error\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#check-traefik-deployment-arguments","title":"Check Traefik deployment arguments:","text":"<pre><code>kubectl get deployment traefik -n traefik-system -o yaml | grep -A 30 \"args:\"\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#if-dashboard-is-not-enabled-lets-force-enable-it","title":"If dashboard is not enabled, let's force enable it:","text":"<p>create a script and check <pre><code>#!/bin/bash\n\n# Patch Traefik deployment to ensure dashboard is enabled\nkubectl patch deployment traefik -n traefik-system --type='json' -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.dashboard=true\"\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.insecure=true\"\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.debug=true\"\n  }\n]'\n\n# Wait for rollout\nkubectl rollout status deployment/traefik -n traefik-system\n\n# Check the service has the traefik port\nkubectl get svc traefik -n traefik-system -o jsonpath='{.spec.ports[?(@.name==\"traefik\")].nodePort}'\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#summary","title":"Summary:","text":"<p>DO NOT USE: <code>https://traefik.dev.local:30900/dashboard/</code></p> <p>USE ONE OF THESE:</p> <ul> <li><code>http://localhost:30900/dashboard/</code> (direct dashboard access)</li> <li><code>http://traefik.dev.local:30080/</code> (through ingress HTTP)</li> <li><code>https://traefik.dev.local:30443/</code> (through ingress HTTPS)</li> </ul> <p>The key is understanding that:</p> <ul> <li>Port 30900 = Direct dashboard port (use with localhost)</li> <li>Port 30080/30443 = Ingress ports (use with domain names)</li> </ul>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/","title":"Efficient way to to Set Up Keycloak (with Helm)","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/#steps-to-set-up-keycloak-with-helm","title":"\u2705 Steps to Set Up Keycloak (with Helm)","text":"<ol> <li>Add Bitnami repo <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre></li> <li>Create namespace <pre><code>kubectl create namespace keycloak\n</code></pre></li> <li>Install Keycloak Yes \u2014 since you're using k3d with the default Traefik ingress controller, you should expose Keycloak via an Ingress.</li> </ol>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/#steps-to-create-ingress-for-keycloak-on-k3d-with-traefik","title":"\u2705 Steps to Create Ingress for Keycloak on k3d with Traefik","text":"<ol> <li>Install Keycloak with hostname  ```bash     helm install keycloak bitnami/keycloak -n keycloak \\       --set auth.adminUser=admin \\       --set auth.adminPassword=adminpassword \\       --set service.type=ClusterIP \\       --set ingress.enabled=true \\       --set ingress.hostname=keycloak.local \\       --set ingress.annotations.\"traefik.ingress.kubernetes.io/router.entrypoints\"=web</li> </ol> <p>helm install keycloak bitnami/keycloak -n keycloak \\       --set auth.adminUser=admin \\       --set auth.adminPassword=admin123 \\       --set service.type=ClusterIP \\       --set ingress.enabled=true \\       --set ingress.hostname=keycloak.local \\       --set ingress.annotations.\"traefik.ingress.kubernetes.io/router.entrypoints\"=web <pre><code>```bash\ngouse@gouse:~/DevOps/k3d$ helm install keycloak bitnami/keycloak -n keycloak \\\n      --set auth.adminUser=admin \\\n      --set auth.adminPassword=admin123 \\\n      --set service.type=ClusterIP \\\n      --set ingress.enabled=true \\\n      --set ingress.hostname=keycloak.local \\\n      --set ingress.annotations.\"traefik\\.ingress\\.kubernetes\\.io/router\\.entrypoints\"=web\nNAME: keycloak\nLAST DEPLOYED: Mon Jun  9 18:31:36 2025\nNAMESPACE: keycloak\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: keycloak\nCHART VERSION: 24.7.3\nAPP VERSION: 26.2.5\n\nDid you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.\n\n** Please be patient while the chart is being deployed **\n\nKeycloak can be accessed through the following DNS name from within your cluster:\n\n    keycloak.keycloak.svc.cluster.local (port 80)\n\nTo access Keycloak from outside the cluster execute the following commands:\n\n1. Get the Keycloak URL and associate its hostname to your cluster external IP:\n\n   export CLUSTER_IP=$(minikube ip) # On Minikube. Use: `kubectl cluster-info` on others K8s clusters\n   echo \"Keycloak URL: http://keycloak.local/\"\n   echo \"$CLUSTER_IP  keycloak.local\" | sudo tee -a /etc/hosts\n\n2. Access Keycloak using the obtained URL.\n3. Access the Administration Console using the following credentials:\n\n  echo Username: admin\n  echo Password: $(kubectl get secret --namespace keycloak keycloak -o jsonpath=\"{.data.admin-password}\" | base64 -d)\n\nWARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n  - resources\n+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n</code></pre> 2. Add host entry (on your machine) <pre><code>echo \"127.0.0.1 keycloak.local\" | sudo tee -a /etc/hosts\nget the ip from ingress deployed for keycloak\necho \"172.18.0.2 keycloak.local\" | sudo tee -a /etc/hosts\n</code></pre> 3. Check ingress is created <pre><code>kubectl get ingress -n keycloak\n</code></pre> 4. Access Keycloak     - Open browser: http://keycloak.local</p>"},{"location":"Keywords/Must%20know%20keywords/","title":"Must know keywords","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>==Ephemeral== means temporary \u2014 it exists only while the pod is running and gets deleted when the pod is deleted or restarted.</p> Term Definition Ephemeral Temporary \u2014 data or resource that disappears after session/pod is gone Immutable Cannot be changed after creation (e.g., Docker images, versioned infra) Idempotent Can be repeated with same result, no side effects (used in scripts, APIs) Stateless Doesn\u2019t store data between requests (all info comes from request itself) Stateful Stores data or state across sessions (e.g., databases, login sessions) Orchestration Automated management of multiple containers/apps (e.g., Kubernetes) Provisioning Setting up infrastructure automatically (servers, storage, etc.) Taint Marking a node to repel certain pods (Kubernetes term) Toleration Allowing a pod to be scheduled on a tainted node Throttling Limiting usage of CPU/memory or API rate Drain Gracefully removing a node (evicts all pods) in Kubernetes Scaling Adding/removing resources (pods, instances) based on load Bootstrap Initial setup process (e.g., first-time server config or cluster init) Latency Time delay between request and response Throughput Amount of data processed in a given time Saturation A resource being fully utilized (CPU, disk, etc.) Cold Start First-time boot/load of an app/container; slower due to init time Warm Start Restart where some resources/state are already cached A/B Testing Testing two versions of an app to compare performance Canary Deploy Releasing new version to a small group before full rollout Image Read-only template used to create containers (like a snapshot of an app) Container Running instance of an image Registry Storage/repository for container images (Docker Hub, Quay, GitHub Container Registry) Dockerfile Text file with instructions to build a container image Layer Each step in a Dockerfile creates a filesystem layer Volume Persistent storage that can be attached to containers Bind Mount Maps a directory from host into a container (live sync) Overlay Network Docker\u2019s network allowing containers to communicate across hosts Namespace Linux kernel feature isolating container processes (PID, network, mount) Cgroup Linux kernel feature limiting resource usage (CPU, memory) for containers Entrypoint Command that runs when container starts CMD Default command/arguments for container; overridden by runtime command Daemon Background service managing containers (Docker daemon or Podman service) Pod Group of one or more containers sharing the same network/volume (Podman/K8s) Image Tag Version label of an image (e.g., <code>nginx:1.21</code>) Docker Compose Tool to define and run multi-container apps with YAML config Container Runtime Software that runs containers (Docker, containerd, Podman, CRI-O) Build Context Files and directories sent to daemon to build the image Entrypoint vs CMD Entrypoint is fixed command, CMD is default args, CMD can override Entrypoint Container Orchestration Managing multiple containers (e.g., Kubernetes, Docker Swarm) Sidecar Container Helper container running alongside main container (logging, proxy) Rootless Mode Running containers without root privileges (safer) Image Pull Downloading an image from a registry Image Push Uploading a built image to a registry Container Snapshot Saving the current state of a container as an image Dangling Images Unused images without tags, can be cleaned to free space Garbage Collection Removing unused containers, images, volumes"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/","title":"Full Sequence to Implement Kubernetes with kubeadm","text":"<p>Created: 2025-07-08 | Updated: 2025-07-08 | Author: Gouse Shaik</p> <p> </p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#1-infra-readiness","title":"1. Infra Readiness","text":"<p>Provision VM or physical machines with minimum: 2 vCPU, 4GB RAM, 20GB disk</p> <ul> <li>Assign unique hostnames and static IPs</li> <li>Ensure NTP/time sync is active (chronyd or systemd-timesyncd)</li> </ul>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#2-open-required-network-ports-firewall-security-groups","title":"2. Open Required Network Ports (Firewall / Security Groups)","text":"<p>Critical: Must be done before kubeadm runs. These are required for node registration, etcd, kubelet API.</p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#master-node-inbound-ports","title":"\ud83d\udd39 Master Node Inbound Ports:","text":"Port Purpose <code>6443</code> Kubernetes API server <code>2379-2380</code> etcd client &amp; peer <code>10250</code> Kubelet API <code>10257</code> Controller manager <code>10259</code> Scheduler <code>10256</code> kube-proxy (for service LB) #### \ud83d\udd39 Worker Node Inbound Ports: Port Purpose <code>10250</code> Kubelet API <code>30000\u201332767</code> NodePort services <code>10256</code> kube-proxy <p>\ud83e\udde0 First Principle: A distributed system is communication-first. Blocked ports = broken cluster.</p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#3-bootstrap-os-configuration-all-nodes","title":"3. Bootstrap OS Configuration (All Nodes)","text":"<p>Use this script on all nodes (master + workers)</p> <pre><code>#!/bin/bash\nset -e\n\n# Disable swap\nswapoff -a\nsed -i '/ swap / s/^/#/' /etc/fstab\n\n# Set hostnames (optional)\n# hostnamectl set-hostname master-node-1 or worker-node-1\n\n# Enable required kernel modules\nmodprobe br_netfilter\nmodprobe overlay\n\n# System-level networking configs\ntee /etc/sysctl.d/k8s.conf &lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\nsysctl --system\n\n# Install containerd\napt update &amp;&amp; apt install -y containerd\nmkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml &gt; /dev/null\nsed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml\nsystemctl restart containerd\nsystemctl enable containerd\n\n# Install kubeadm, kubelet, kubectl\napt install -y apt-transport-https ca-certificates curl\ncurl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /usr/share/keyrings/k8s-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/k8s-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" &gt; /etc/apt/sources.list.d/kubernetes.list\napt update\napt install -y kubelet kubeadm kubectl\napt-mark hold kubelet kubeadm kubectl\n</code></pre>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#4-initialize-master-node","title":"4. Initialize Master Node","text":"<p>Run on master node only</p> <p><pre><code>kubeadm init \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --apiserver-advertise-address=&lt;MASTER_NODE_IP&gt;\n</code></pre> Copy the <code>kubeadm join</code> command at the end.</p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#5-configure-kubectl-on-master","title":"5. Configure <code>kubectl</code> on Master","text":"<pre><code>mkdir -p $HOME/.kube\ncp /etc/kubernetes/admin.conf $HOME/.kube/config\nchown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#6-install-pod-network-add-on-eg-flannel","title":"6. Install Pod Network Add-on (e.g., Flannel)","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n</code></pre> <p>You\u2019ll see node change from <code>NotReady</code> to <code>Ready</code>.</p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#7-join-worker-nodes","title":"7. Join Worker Nodes","text":"<p>Run the saved <code>kubeadm join</code> command from step 4 on each worker node</p> <pre><code>kubeadm join &lt;MASTER_IP&gt;:6443 --token ... --discovery-token-ca-cert-hash sha256:...\n</code></pre>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#8-verify-cluster","title":"8. Verify Cluster","text":"<p><pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre> You should see all nodes in <code>Ready</code> state and <code>kube-system</code> pods running.</p>"},{"location":"Kubernetes/Full%20Sequence%20to%20Implement%20Kubernetes%20with%20kubeadm/#critical-checkpoints","title":"\ud83e\udde0 Critical Checkpoints","text":"Checkpoint Why it Matters Ports opened first Else <code>kubeadm join</code> or API calls silently fail Swap disabled before kubelet runs Prevents runtime crashes and cgroup failures Containerd uses <code>SystemdCgroup=true</code> Aligns with kubelet's expectations Network add-on installed after init Else Pods stuck in <code>ContainerCreating</code> Hostname and IP consistency Prevents kubelet re-registration or duplication Static IP or reservation via DHCP Prevents cluster break after reboot ## \ud83d\udce6 Optional (Post Setup) <ul> <li>Install metrics-server: <code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</code></li> <li>Install Ingress: NGINX or Traefik</li> <li>Setup RBAC, PSA, audit logs, cert-manager</li> </ul> <p>Let me know if you want:</p> <ul> <li> <p>Air-gapped version of the setup</p> </li> <li> <p>HA control plane (with stacked etcd or external)</p> </li> <li> <p>Ansible script to automate all of the above</p> </li> </ul> <p>Your infra. Your control. No fluff.</p>"},{"location":"Kubernetes/Installing%20metric%20server/","title":"Installing metric server","text":"<p>Basic steps to install Kubernetes Metrics Server on your cluster:</p>"},{"location":"Kubernetes/Installing%20metric%20server/#1-download-the-official-components","title":"\u2705 1. Download the official components","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> <p>This installs <code>metrics-server</code> in <code>kube-system</code> namespace with default configuration.</p>"},{"location":"Kubernetes/Installing%20metric%20server/#2-edit-deployment-optional-for-bare-metal-or-self-signed-clusters","title":"\u2705 2. Edit Deployment (optional for bare-metal or self-signed clusters)","text":"<p>If you're running on a local cluster, Minikube, or custom TLS setup, patch the deployment to skip TLS verification: <pre><code>kubectl -n kube-system edit deployment metrics-server\n</code></pre></p> <p>\u27a1 Add this under <code>containers.args</code>:</p> <pre><code>- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP\n</code></pre> <p>Then save &amp; exit.</p>"},{"location":"Kubernetes/Installing%20metric%20server/#3-verify-the-deployment","title":"\u2705 3. Verify the deployment","text":"<pre><code>kubectl get deployment metrics-server -n kube-system\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml\nkubectl top nodes\nkubectl top pods --all-namespaces\n</code></pre>"},{"location":"Kubernetes/K8s%20-%20manifest%20files%20structure/","title":"K8s   manifest files structure","text":"<pre><code>Kubernetes Objects -&gt;\n                - apiVersions : groups/version\n                - kind\n                - metadata\n                - specs\n                - status\n</code></pre>"},{"location":"Kubernetes/K8s%20Pod%20Architecture%20%26%20Lifecycle/","title":"K8s Pod Architecture & Lifecycle","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>This diagram provides a deep-dive cheat sheet on Kubernetes Pods, covering their structure, lifecycle, networking, spec fields, commands, and management principles from a first-principles perspective..</p> <p> </p>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/","title":"Kube API server Architechture   deep dive","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p></p> <p>Let's go into deep-dive explanation of the diagram, walking you through the complete internal workflow of Kubernetes from scratch, including basics of each resource/component being used by Kubernetes</p>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#1-what-is-kubernetes","title":"\ud83e\uddf1 1. What Is Kubernetes?","text":"<p>Kubernetes (K8s) is an open-source container orchestration system that automates:</p> <ul> <li>Deployment</li> <li>Scaling</li> <li>Networking</li> <li>Management of containerized applications</li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#2-kubernetes-core-building-blocks","title":"\ud83d\udd17 2. Kubernetes Core Building Blocks","text":"Component Role Pod Smallest deployable unit in Kubernetes. Contains one or more containers. Deployment Manages the desired state of pods (replicas, updates, rollback). Service Exposes pods to other services/pods or external traffic. Node A machine (VM/physical) that runs containerized workloads (pods). Kubelet An agent on each node. Ensures containers are running in a pod. Kube-proxy Handles network traffic routing for services/pods on a node. etcd A key-value store that saves the entire cluster state. API Server Central access point for the Kubernetes control plane (REST interface). Scheduler Assigns unscheduled pods to suitable nodes. Controller Manager Runs background loops (controllers) that watch and adjust cluster state. Cloud Controller Manager Integrates cloud-specific operations (e.g., load balancers)."},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#3-step-by-step-explanation-of-diagram","title":"\u2699\ufe0f 3. Step-by-Step Explanation of Diagram","text":""},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-1-submit-configuration-yaml","title":"\ud83e\uddd1\u200d\ud83d\udcbb Step 1: Submit Configuration (YAML)","text":"<ul> <li>A developer or admin writes a Kubernetes manifest in YAML.</li> <li>This defines a Pod, Deployment, or other resource.</li> <li> <p>The user runs:</p> <pre><code>kubectl apply -f app.yaml\n</code></pre> </li> <li> <p>The request hits the Kubernetes API Server. </p> </li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-2-api-server-receives-request","title":"\ud83d\udce5 Step 2: API Server Receives Request","text":"<ul> <li> <p>The API Server:</p> <ul> <li>Validates the syntax/schema.</li> <li>Authenticates the user.</li> <li>Authorizes the request (via RBAC).</li> </ul> </li> <li> <p>If valid, forwards the object to etcd (backing store).</p> </li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-3-etcd-stores-the-cluster-state","title":"\ud83e\udde0 Step 3: etcd Stores the Cluster State","text":"<ul> <li>etcd stores all the key-value configurations, including the new pod spec.</li> <li>It acts like a source of truth.</li> <li>Every change to the cluster (creating, updating, deleting resources) is recorded here.</li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-45-controllers-detect-the-change","title":"\ud83d\udd04 Step 4\u20135: Controllers Detect the Change","text":"<ul> <li>The Controller Manager continuously watches etcd.</li> <li>It notices a new pod spec and checks if the current state matches the desired state.</li> <li>If not, it triggers actions (e.g., create a pod).</li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-6-scheduler-binds-pod-to-a-node","title":"\ud83d\udce6 Step 6: Scheduler Binds Pod to a Node","text":"<ul> <li>The Scheduler:<ul> <li>Detects the new unassigned pod.</li> <li>Evaluates all available nodes.</li> <li>Based on resources, taints/tolerations, node selectors, and affinity rules \u2192 selects a node.</li> <li>The scheduler updates etcd with node assignment.</li> </ul> </li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-7-node-kubelet-watches-etcd","title":"\ud83d\udce4 Step 7: Node Kubelet Watches etcd","text":"<ul> <li> <p>Kubelet on the selected node sees a new pod assigned to it.</p> </li> <li> <p>It:</p> <ul> <li>Pulls the container images (from Docker Hub or private registry).</li> <li>Creates containers using the container runtime (containerd, CRI-O, etc).</li> <li>Starts the pod.</li> </ul> </li> </ul> <p></p>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-810-kube-proxy-configures-networking","title":"\ud83c\udf10 Step 8\u201310: Kube-proxy Configures Networking","text":"<ul> <li>Kube-proxy sets up:<ul> <li>iptables or IPVS rules for service load-balancing.</li> <li>Ensures networking for the pod is in place.</li> <li>Handles traffic routing to/from the pod.</li> </ul> </li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#step-1112-internet-access","title":"\ud83c\udf0d Step 11\u201312: Internet Access","text":"<ul> <li>If the app needs internet:<ul> <li>NAT / kube-proxy routes the traffic outside the cluster.</li> <li>The pod can access external services like databases, APIs, etc.</li> </ul> </li> </ul>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#4-bottom-section-api-server-clustering","title":"\ud83e\udde0 4. Bottom Section: API Server Clustering","text":"<ul> <li> <p>Shows multiple API server replicas (API-1 to API-7) behind a load balancer for High Availability (HA).</p> </li> <li> <p>Follows etcd quorum rule:</p> <ul> <li>Minimum <code>n/2 + 1</code> nodes must be alive to maintain consistency.</li> <li>For 7 API servers: Quorum = 4.</li> </ul> </li> <li> <p>Helps prevent split-brain scenarios and ensures fault tolerance.</p> </li> </ul> <p>]</p>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#summary-kubernetes-control-plane-data-flow","title":"\ud83d\udd01 Summary \u2013 Kubernetes Control Plane Data Flow","text":"<pre><code>User YAML -&gt; API Server -&gt; ETCD -&gt; Controller Manager -&gt; Scheduler -&gt; Kubelet -&gt; Containers (Pod)\n</code></pre>"},{"location":"Kubernetes/Kube%20API%20server%20Architechture%20-%20deep%20dive/#summary","title":"\u2705 Summary","text":"Principle Kubernetes Implementation State Machine etcd as the consistent, distributed key-value store Reconciliation Loop Controllers continuously watch and reconcile desired vs actual state Declarative Intent YAML files define what you want, not how to do it Loose Coupling Modular architecture (API Server, Scheduler, Kubelet, etc.) Fault Tolerance by Design HA API servers + etcd quorum logic Idempotency Reapplying manifests results in the same outcome"},{"location":"Kubernetes/Kuberentes%20usecases-deepdive%20flow/","title":"Kuberentes usecases deepdive flow","text":""},{"location":"Kubernetes/Kubernetes%20history/","title":"Kubernetes history","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p></p> <p></p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/","title":"Loadbalancer Types","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>In Kubernetes, there are three main types of load balancers commonly used:</p> Type Description Internal LoadBalancer Used for internal services, only accessible within the cluster or VPC. External LoadBalancer Exposes services to the internet using a cloud provider\u2019s external LB. Ingress Controller Acts as a reverse proxy and manages HTTP/S traffic with routing rules. LB Type Layer Performance Use Case ELB L4+L7 Moderate Legacy/simple apps NLB L4 High Real-time, fast apps like APIs ALB L7 HTTP-aware Use with Ingress, not <code>Service</code> Additionally, you might use: Type Description NodePort Exposes service on each node\u2019s IP at a static port (not a full LB, but basic). MetalLB A load balancer implementation for bare metal clusters."},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#when-and-how-to-use-internal-vs-external-loadbalancers","title":"When and How to Use Internal vs External LoadBalancers","text":"Type When to Use How to Use Internal LoadBalancer For intra-cluster or private communication between services (e.g., backend APIs, internal DBs) Use annotation to make it internal:<code>service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"</code> External LoadBalancer When exposing a service to the public internet (e.g., front-end web app, public API) Just define the service type as <code>LoadBalancer</code>, Kubernetes/cloud will handle provisioning."},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#example-yamls","title":"Example YAMLs:","text":"<p>Internal LoadBalancer (AWS/Azure/GCP-specific annotations): <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre></p> <p>External LoadBalancer: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre></p> <p>You're almost spot on \u2014 let me break it down clearly:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#correct-understanding","title":"\u2705 Correct Understanding:","text":"<ul> <li> <p>LoadBalancers work with Services</p> </li> <li> <p>Services route traffic to Pods</p> </li> <li> <p>Pods are created by Deployments (or StatefulSets, etc.)</p> </li> <li> <p>So, indirectly, LoadBalancers affect traffic to Deployments via Services.</p> </li> </ul>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#but-not-directly-tied-to-deployment-strategies","title":"\u274c But NOT directly tied to Deployment Strategies","text":"<ul> <li> <p>Deployment strategies like <code>RollingUpdate</code> or <code>Recreate</code> are just about how pods are updated</p> </li> <li> <p>LoadBalancers don\u2019t care how or when the pods are created/updated \u2014 they just route to available, healthy pods via the Service.</p> </li> </ul>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#flow-summary","title":"Flow Summary:","text":"<p><code>LoadBalancer \u2192 Service \u2192 Pods \u2190 Deployment (with strategy)</code></p> <ul> <li> <p>LoadBalancer: handles external/internal traffic</p> </li> <li> <p>Service: handles routing logic</p> </li> <li> <p>Deployment: manages pod lifecycle</p> </li> <li> <p>Strategy: manages how pods get updated, not how traffic is routed</p> </li> </ul> <p>So yes, they all work together, but the LoadBalancer doesn\u2019t depend on deployment strategy.</p> <p>In your k3d cluster using Traefik, you're using an Ingress Controller, which is a type of Layer 7 (HTTP) Load Balancer.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#details","title":"Details:","text":"Component Role Traefik Acts as an Ingress Controller (application-level load balancer) Ingress Defines routing rules (host/path based) to backend services Service Maps traffic from Traefik to Pods Deployment Manages Pods running your apps"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#so-the-type-of-load-balancer-youre-using-is","title":"So, the type of Load Balancer you're using is:","text":"<p>\u2705 Ingress-based Load Balancer (via Traefik)</p> <p>It\u2019s not a Service of type <code>LoadBalancer</code> (which works on cloud providers), but rather an internal reverse proxy that routes external traffic based on Ingress rules.</p> <p>Here's a full list of commonly used annotations for different types of LoadBalancers across major cloud providers and Traefik:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#1-internal-loadbalancer-private","title":"\ud83d\udd39 1. Internal LoadBalancer (Private)","text":"Cloud Annotation AWS <code>service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"</code> Azure <code>service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"</code> GCP <code>cloud.google.com/load-balancer-type: \"Internal\"</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#2-external-loadbalancer-public","title":"\ud83d\udd39 2. External LoadBalancer (Public)","text":"<p>(These are usually default; no annotation needed, but here\u2019s how to customize)</p> Cloud Annotation Example AWS <code>service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"</code> (or \"elb\") Azure <code>service.beta.kubernetes.io/azure-load-balancer-internal: \"false\"</code> GCP <code>cloud.google.com/load-balancer-type: \"External\"</code> (default)"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#3-ingress-based-loadbalancer-eg-traefik","title":"\ud83d\udd39 3. Ingress-based LoadBalancer (e.g., Traefik)","text":"Use Case Annotation Traefik IngressClass <code>kubernetes.io/ingress.class: \"traefik\"</code> TLS redirection <code>traefik.ingress.kubernetes.io/redirect-entry-point: https</code> Rewrite Path <code>traefik.ingress.kubernetes.io/rewrite-target: /</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#4-metallb-for-bare-metal-local","title":"\ud83d\udd39 4. MetalLB (for bare metal / local)","text":"Use Case Annotation Assign static IP <code>metallb.universe.tf/address-pool: \"default\"</code> <p>Great \u2014 here are ready-to-use YAML examples with annotations for each type of LoadBalancer based on your setup:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#1-aws","title":"\ud83d\udd38 1. AWS","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#external-loadbalancer-public","title":"\u2705 External LoadBalancer (Public)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-public-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer-private","title":"\u2705 Internal LoadBalancer (Private)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#azure","title":"\ud83d\udd38 Azure","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer","title":"\u2705 Internal LoadBalancer","text":"<pre><code>metadata:\n  name: my-internal-azure-svc\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#gcp","title":"\ud83d\udd38 GCP","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer_1","title":"\u2705 Internal LoadBalancer","text":"<pre><code>metadata:\n  name: my-internal-gcp-svc\n  annotations:\n    cloud.google.com/load-balancer-type: \"Internal\"\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#metallb-localbare-metal","title":"\ud83d\udd38 MetalLB (Local/Bare Metal)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-metallb-service\n  annotations:\n    metallb.universe.tf/address-pool: \"default\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#traefik-ingress-for-your-k3d-setup","title":"\ud83d\udd38 Traefik Ingress (for your k3d setup)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-traefik-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"traefik\"\n    traefik.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: myapp.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-service\n                port:\n                  number: 80\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#what-is-nlb-in-aws-annotation","title":"\ud83d\udd39 What is <code>nlb</code> in AWS Annotation?","text":"<p><code>nlb</code> stands for Network Load Balancer in AWS. It\u2019s a high-performance, Layer 4 (TCP/UDP) load balancer that handles millions of requests per second with low latency.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#aws-annotation-meaning","title":"\ud83d\udd39 AWS Annotation Meaning:","text":"<pre><code>service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n</code></pre> <p>This tells Kubernetes to create an AWS Network Load Balancer (NLB) instead of the default Classic Load Balancer (ELB).</p> LB Type Layer Performance Use Case ELB L4+L7 Moderate Legacy/simple apps NLB L4 High Real-time, fast apps like APIs ALB L7 HTTP-aware Use with Ingress, not <code>Service</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#are-these-services-predefined","title":"\ud83d\udd39 Are These Services Predefined?","text":"<p>No \u2014 these are not predefined. When you create a <code>Service</code> of type <code>LoadBalancer</code>, Kubernetes asks the cloud provider (via the Cloud Controller Manager) to provision a real load balancer resource dynamically.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#creation-process-simplified","title":"\ud83d\udd39 Creation Process (Simplified):","text":"<ol> <li> <p>You define a <code>Service</code> with <code>type: LoadBalancer</code></p> </li> <li> <p>Kubernetes talks to the cloud provider API</p> </li> <li> <p>The cloud creates:</p> <ul> <li> <p>A load balancer (ELB, NLB, etc.)</p> </li> <li> <p>Configures listeners and target groups</p> </li> <li> <p>Maps it to the appropriate nodes/pods</p> </li> </ul> </li> <li> <p>The LB's public or private IP is reflected in the service status (<code>EXTERNAL-IP</code>)</p> </li> </ol>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#so-you-control","title":"\ud83d\udd39 So, You Control:","text":"<ul> <li> <p>Type (<code>LoadBalancer</code>, <code>ClusterIP</code>, <code>NodePort</code>)</p> </li> <li> <p>Provider behavior via annotations</p> </li> <li> <p>Target pods via selectors</p> </li> <li> <p>Protocols via ports</p> </li> </ul> <p>Let me know if you want a visual flow or deeper dive into a specific cloud (e.g., NLB with TLS, ALB via Ingress, etc.).</p>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/","title":"Reverse proxy means","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>A reverse proxy is a server that sits in front of one or more backend servers and forwards client requests to those servers. It hides the backend servers from clients and can provide:</p> <ul> <li>Load balancing</li> <li>SSL termination</li> <li>Caching</li> <li>Compression</li> <li>Security filtering</li> </ul>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/#simple-flow","title":"Simple flow:","text":"<pre><code>Client \u2192 Reverse Proxy \u2192 Backend Server\n</code></pre>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/#example","title":"Example:","text":"<p>When you access a website: <pre><code>User \u2192 Nginx (Reverse Proxy) \u2192 Web App Server\n</code></pre></p> <p>The client doesn\u2019t see or talk directly to the backend server\u2014only to the reverse proxy.</p>"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/","title":"Can You Use Longhorn  on top of  NFS ?","text":""},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#can-you-use-longhorn-on-top-of-nfs","title":"\u2753 Can You Use Longhorn on top of NFS?","text":"<p>\ud83d\udd34 No \u2014 Longhorn does not support NFS as a backend. Longhorn requires direct block storage (local SSD/HDD attached to nodes). You cannot map NFS to Longhorn as storage.</p>"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#what-you-can-do-instead","title":"\u2705 What You Can Do Instead","text":"<p>You have two correct and supported paths:</p> Option Description Option A: Use NFS separately Setup NFS, deploy NFS CSI driver, and use it for PV/PVC (for shared read or general storage) Option B: Use Longhorn normally Use Longhorn's default storage (local disks), and export Longhorn volumes via NFS if needed (advanced) ## \u2705 Option A: Use NFS Directly for PVCs (Recommended for Shared Storage)"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#1-you-already-installed-nfs-server","title":"1. \u2705 You already installed NFS server","text":"<p>(from your Ubuntu host)</p>"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#2-install-nfs-subdir-external-provisioner-in-k3s","title":"2. \ud83d\udd3d Install NFS Subdir External Provisioner (in K3s)","text":"<pre><code>kubectl create ns nfs-provisioner\n\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner\nhelm repo update\n\nhelm install nfs-client nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  --namespace nfs-provisioner \\\n  --set nfs.server=&lt;NFS_SERVER_IP&gt; \\\n  --set nfs.path=/srv/nfs/k3s-volumes \\\n  --set storageClass.name=nfs-client \\\n  --set storageClass.defaultClass=false\n</code></pre> <p>Replace <code>&lt;NFS_SERVER_IP&gt;</code> with your Ubuntu IP (e.g. <code>192.168.122.1</code> if you're using Multipass)</p>"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#3-use-nfs-storageclass-in-pvc","title":"3. \ud83d\udce6 Use NFS StorageClass in PVC","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: nfs-client\n</code></pre>"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#why-you-cannot-use-nfs-with-longhorn","title":"\u26d4 Why You Cannot Use NFS with Longhorn","text":"Reason Explanation Longhorn requires block storage It works with <code>/dev/sdX</code>, not NFS mount NFS is a file system protocol Longhorn replicates block volumes across nodes Longhorn and NFS serve different purposes Longhorn = high-availability block storageNFS = shared file system"},{"location":"Longhorn/Can%20You%20Use%20Longhorn%20_on%20top%20of_%20NFS%20%3F/#summary","title":"\ud83e\udde0 Summary","text":"Goal Use Shared read/write across pods \u2705 Use NFS with CSI driver Stateful apps needing high availability \u2705 Use Longhorn Backups Use Velero + Longhorn snapshots or NFS Combining Longhorn + NFS \u274c Not supported or needed"},{"location":"Longhorn/Create%20a%20PV%20and%20PVC%20using%20longhorn-static/","title":"Create a PV and PVC using longhorn static","text":""},{"location":"Longhorn/Create%20a%20PV%20and%20PVC%20using%20longhorn-static/#example-use-case","title":"\ud83e\uddfe Example Use Case","text":"<ul> <li> <ol> <li>You created a volume called <code>manual-volume</code> in Longhorn UI</li> </ol> </li> <li> <ol> <li>You now want to mount it to a Pod</li> </ol> </li> </ul> <p>Create a <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> using <code>longhorn-static</code>: <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: manual-pv\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: longhorn-static\n  csi:\n    driver: driver.longhorn.io\n    volumeHandle: manual-volume # &lt;-- This must match the volume name in Longhorn UI\n    fsType: ext4\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: manual-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: longhorn-static\n  volumeName: manual-pv\n</code></pre></p> <p>\ud83d\udd0d How to List Existing Longhorn Volumes <pre><code>kubectl -n longhorn-system exec -it &lt;longhorn-manager-pod&gt; -- longhorn ls\n</code></pre></p> <p>Or view via Longhorn UI \u2192 <code>Volumes</code> tab \u2192 copy volume name to use in <code>volumeHandle</code>.</p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/","title":"Hands On   Deploying Longhorn on Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#hands-on-demo-deploying-longhorn-on-kubernetes","title":"Hands-On Demo: Deploying Longhorn on Kubernetes","text":"<p>In this demo, we\u2019ll: \u2705 Install Longhorn on a Kubernetes cluster \u2705 Create a Persistent Volume Claim (PVC) \u2705 Deploy a MySQL database using Longhorn storage \u2705 Test fault tolerance by killing a pod \u2705 Take a snapshot &amp; restore </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, K3s, EKS, AKS, etc.)  </li> <li><code>kubectl</code> configured  </li> <li>Helm (for installation)  </li> </ul>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-1-install-longhorn","title":"Step 1: Install Longhorn","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#option-a-using-helm-recommended","title":"Option A: Using Helm (Recommended)","text":"<pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\nhelm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#option-b-using-kubectl","title":"Option B: Using Kubectl","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/deploy/longhorn.yaml\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#verify-installation","title":"Verify Installation","text":"<p><pre><code>kubectl get pods -n longhorn-system\n</code></pre> \u2705 Expected Output: All pods should be <code>Running</code>.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-2-access-longhorn-dashboard","title":"Step 2: Access Longhorn Dashboard","text":"<p>Port-forward the UI: <pre><code>kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80\n</code></pre> Now, open http://localhost:8080 in your browser.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-3-create-a-persistentvolumeclaim-pvc","title":"Step 3: Create a PersistentVolumeClaim (PVC)","text":"<p>Create <code>mysql-pvc.yaml</code>: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-pvc.yaml\n</code></pre> Check in Longhorn UI \u2192 Volume tab.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-4-deploy-mysql-with-longhorn-storage","title":"Step 4: Deploy MySQL with Longhorn Storage","text":"<p>Create <code>mysql-deployment.yaml</code>: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n</code></pre> Deploy it: <pre><code>kubectl apply -f mysql-deployment.yaml\n</code></pre> Verify: <pre><code>kubectl get pods\nkubectl logs &lt;mysql-pod-name&gt;\n</code></pre></p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-5-test-fault-tolerance","title":"Step 5: Test Fault Tolerance","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#simulate-a-nodepod-failure","title":"Simulate a Node/Pod Failure","text":"<p><pre><code>kubectl delete pod &lt;mysql-pod-name&gt;\n</code></pre> \u2705 Observe: - A new pod starts automatically. - Data persists because Longhorn ensures replicated storage.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-6-take-a-snapshot-restore","title":"Step 6: Take a Snapshot &amp; Restore","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#manual-snapshot-via-longhorn-ui","title":"Manual Snapshot via Longhorn UI","text":"<ol> <li>Go to Volumes \u2192 Select your MySQL volume.  </li> <li>Click \"Take Snapshot\".  </li> <li>(Optional) Backup to S3 (if configured).  </li> </ol>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#restore-from-snapshot","title":"Restore from Snapshot","text":"<ol> <li>In the UI, select the snapshot \u2192 \"Create Standby Volume\".  </li> <li>Update the MySQL deployment to use the new volume.  </li> </ol>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-7-clean-up","title":"Step 7: Clean Up","text":"<pre><code>kubectl delete deployment mysql\nkubectl delete pvc mysql-pvc\nhelm uninstall longhorn -n longhorn-system\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#conclusion","title":"Conclusion","text":"<p>You\u2019ve successfully: \u2714 Deployed Longhorn on Kubernetes \u2714 Created a MySQL database with persistent storage \u2714 Tested fault tolerance \u2714 Used snapshots &amp; backups </p> <p>\ud83d\ude80 Next Steps: - Try cross-cluster replication for disaster recovery. - Integrate with Rancher for easier management.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/","title":"Longhorn Access Modes & Volume Resizing Guide","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#1-understanding-access-modes-in-longhorn","title":"1. Understanding Access Modes in Longhorn","text":"<p>Kubernetes Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) support different access modes, which define how volumes can be mounted by pods.</p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#supported-access-modes-in-longhorn","title":"Supported Access Modes in Longhorn","text":"Access Mode Description Use Case <code>ReadWriteOnce</code> (RWO) Only one pod (on a single node) can mount the volume in read-write mode. Single-instance databases (MySQL, PostgreSQL) <code>ReadOnlyMany</code> (ROX) Multiple pods (across different nodes) can mount the volume in read-only mode. Shared configuration files (e.g., <code>nginx.conf</code>) <code>ReadWriteMany</code> (RWX) Multiple pods (across different nodes) can mount the volume in read-write mode. Shared file storage (NFS-like, e.g., WordPress uploads)"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#which-access-modes-does-longhorn-support","title":"Which Access Modes Does Longhorn Support?","text":"<p>\u2705 Longhorn supports: - <code>ReadWriteOnce</code> (RWO) \u2013 Default and most stable. - <code>ReadWriteMany</code> (RWX) \u2013 Requires NFSv4 or Samba (experimental).  </p> <p>\u274c Longhorn does NOT natively support <code>ReadOnlyMany</code> (ROX). </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#2-how-to-change-access-modes-in-longhorn","title":"2. How to Change Access Modes in Longhorn","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-a-define-in-pvc-yaml","title":"Option A: Define in PVC YAML","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce  # Change to ReadWriteMany for RWX\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-b-using-rwx-readwritemany","title":"Option B: Using RWX (ReadWriteMany)","text":"<ol> <li>Enable RWX in Longhorn UI: </li> <li>Go to Settings \u2192 Enable NFSv4 (or Samba).  </li> <li>Create an RWX PVC: <pre><code>accessModes:\n  - ReadWriteMany\n</code></pre></li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#3-how-to-increase-or-decrease-volume-size","title":"3. How to Increase or Decrease Volume Size","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-1-expand-a-volume-increase-size","title":"Option 1: Expand a Volume (Increase Size)","text":"<ol> <li> <p>Edit the PVC: <pre><code>kubectl edit pvc &lt;pvc-name&gt;\n</code></pre>    Change <code>storage: 5Gi</code> \u2192 <code>storage: 20Gi</code>.  </p> </li> <li> <p>Verify Expansion in Longhorn UI: </p> </li> <li> <p>Go to Volumes \u2192 Check the new size.  </p> </li> <li> <p>Resize Filesystem (if needed): <pre><code>kubectl exec -it &lt;pod-name&gt; -- df -h /var/lib/mysql  # Check current size\nkubectl exec -it &lt;pod-name&gt; -- resize2fs /dev/longhorn/&lt;volume-name&gt;  # For ext4\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-2-shrink-a-volume-decrease-size","title":"Option 2: Shrink a Volume (Decrease Size)","text":"<p>\u26a0 Shrinking is risky and not officially supported! - Workaround:   1. Take a snapshot.   2. Create a new smaller volume and restore data.   3. Update the PVC to point to the new volume.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#4-real-world-scenarios","title":"4. Real-World Scenarios","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#scenario-1-scaling-mysql-storage","title":"Scenario 1: Scaling MySQL Storage","text":"<ul> <li>Problem: MySQL is running out of space.  </li> <li>Solution: <pre><code>kubectl edit pvc mysql-pvc  # Change 10Gi \u2192 50Gi\n</code></pre>   Longhorn automatically expands the volume.  </li> </ul>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#scenario-2-shared-storage-for-wordpress","title":"Scenario 2: Shared Storage for WordPress","text":"<ul> <li>Problem: Multiple WordPress pods need shared uploads.  </li> <li>Solution: <pre><code>accessModes:\n  - ReadWriteMany  # Uses NFS under the hood\n</code></pre></li> </ul>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#5-best-practices","title":"5. Best Practices","text":"<p>\u2714 For databases: Use <code>ReadWriteOnce</code> (RWO). \u2714 For shared files: Use <code>ReadWriteMany</code> (RWX) with NFS. \u2714 Always take snapshots before resizing. \u274c Avoid shrinking volumes (data loss risk).  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#conclusion","title":"Conclusion","text":"<ul> <li>Access Modes: </li> <li><code>RWO</code> = Single pod (default).  </li> <li><code>RWX</code> = Multi-pod (requires NFS).  </li> <li>Resizing: </li> <li>Increase size \u2192 Edit PVC.  </li> <li>Decrease size \u2192 Backup &amp; restore to a new volume.  </li> </ul> <p>\ud83d\ude80 Next Steps: - Try dynamic volume provisioning with <code>storageClassName: longhorn</code>. - Explore backup &amp; restore to S3.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-by-step-demo-resizing-longhorn-volumes-in-kubernetes","title":"Step-by-Step Demo: Resizing Longhorn Volumes in Kubernetes","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#what-well-cover","title":"\ud83d\udd25 What We'll Cover","text":"<ol> <li>Creating a PVC (5Gi \u2192 20Gi expansion)  </li> <li>Attaching it to a MySQL Pod </li> <li>Expanding the Volume (Live resize)  </li> <li>Shrinking Safely (Via backup &amp; restore)  </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-1-deploy-a-test-mysql-pod-with-longhorn-pvc","title":"Step 1: Deploy a Test MySQL Pod with Longhorn PVC","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#11-create-a-pvc-mysql-pvcyaml","title":"1.1 Create a PVC (<code>mysql-pvc.yaml</code>)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi  # We'll expand this later\n</code></pre> <p>Apply it: <pre><code>kubectl apply -f mysql-pvc.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#12-deploy-mysql-mysql-deploymentyaml","title":"1.2 Deploy MySQL (<code>mysql-deployment.yaml</code>)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n</code></pre> <p>Deploy: <pre><code>kubectl apply -f mysql-deployment.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#13-verify-pvc-pod","title":"1.3 Verify PVC &amp; Pod","text":"<pre><code>kubectl get pvc  # Should show 5Gi\nkubectl get pods  # Wait for MySQL to run\n</code></pre>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-2-expand-volume-from-5gi-20gi","title":"Step 2: Expand Volume from 5Gi \u2192 20Gi","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#21-edit-pvc-live","title":"2.1 Edit PVC Live","text":"<p><pre><code>kubectl edit pvc mysql-pvc\n</code></pre> Change: <pre><code>resources:\n  requests:\n    storage: 20Gi  # Update from 5Gi \u2192 20Gi\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#22-verify-expansion-in-longhorn-ui","title":"2.2 Verify Expansion in Longhorn UI","text":"<ol> <li> <p>Open Longhorn Dashboard:    <pre><code>kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80\n</code></pre>    Visit http://localhost:8080 \u2192 Check volume size.</p> </li> <li> <p>Confirm in Kubernetes:    <pre><code>kubectl get pvc mysql-pvc  # Should show 20Gi\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#23-resize-filesystem-if-needed","title":"2.3 Resize Filesystem (If Needed)","text":"<p>If MySQL doesn\u2019t see the new space: <pre><code># Enter the MySQL pod\nkubectl exec -it &lt;mysql-pod-name&gt; -- bash\n\n# Check current disk space\ndf -h /var/lib/mysql  # Likely still shows 5Gi\n\n# Resize ext4 filesystem (for Longhorn volumes)\nresize2fs /dev/longhorn/&lt;volume-name&gt;  # Autocomplete with `ls /dev/longhorn/`\n\n# Verify\ndf -h /var/lib/mysql  # Now shows 20Gi\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-3-safely-shrink-volume-20gi-10gi","title":"Step 3: Safely Shrink Volume (20Gi \u2192 10Gi)","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#31-take-a-snapshot-longhorn-ui","title":"3.1 Take a Snapshot (Longhorn UI)","text":"<ol> <li>Go to Volumes \u2192 Select <code>mysql-pvc</code>.  </li> <li>Click \"Take Snapshot\".  </li> <li>(Optional) Backup to S3 for disaster recovery.  </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#32-create-a-new-smaller-pvc","title":"3.2 Create a New Smaller PVC","text":"<pre><code># new-mysql-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc-small\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi  # Smaller size\n</code></pre> <p>Apply: <pre><code>kubectl apply -f new-mysql-pvc.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#33-restore-data-to-new-pvc","title":"3.3 Restore Data to New PVC","text":"<ol> <li>In Longhorn UI:  </li> <li>Go to Snapshot \u2192 \"Create Volume\" from snapshot.  </li> <li> <p>Attach it to <code>mysql-pvc-small</code>.  </p> </li> <li> <p>Update MySQL Deployment:    <pre><code>kubectl edit deployment mysql\n</code></pre>    Change:    <pre><code>volumes:\n- name: mysql-storage\n  persistentVolumeClaim:\n    claimName: mysql-pvc-small  # Point to new PVC\n</code></pre></p> </li> <li> <p>Verify:    <pre><code>kubectl get pvc  # Should show mysql-pvc-small (10Gi)\nkubectl exec -it &lt;mysql-pod&gt; -- df -h /var/lib/mysql  # Should show 10Gi\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Expanding is easy: Just edit PVC \u2192 Longhorn handles the rest. \u26a0 Shrinking is risky: Requires backup \u2192 restore to a new volume. \ud83d\udd27 Always test resizing in staging first! </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#next-steps","title":"Next Steps","text":"<ul> <li>Automate snapshots with Longhorn\u2019s Recurring Jobs.  </li> <li>Try RWX volumes for shared storage (e.g., WordPress).  </li> </ul> <p>Need help with specific workloads (PostgreSQL, MongoDB)? Ask away! \ud83d\ude80</p>"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/","title":"Longhorn for stateful applications","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes, using Longhorn for stateful applications in Kubernetes is a good practice, especially for:</p> <ul> <li>Lightweight production or staging clusters</li> <li>On-prem or edge environments where cloud storage is unavailable</li> <li>Learning and testing persistent storage</li> </ul>"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/#why-longhorn-is-good-for-stateful-apps","title":"Why Longhorn is good for stateful apps:","text":"Benefits Details Distributed storage Replicates data across nodes for fault tolerance Easy management Web UI + CLI for volume control Dynamic provisioning Supports PVC creation on demand Snapshot and backup support Point-in-time data recovery Compatible with Kubernetes Native CSI driver integration"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/#when-not-to-use-longhorn","title":"When NOT to use Longhorn:","text":"<ul> <li>Very large-scale or highly performance-sensitive workloads (may need enterprise storage solutions)</li> <li>Clusters without enough disk resources or unreliable node storage</li> </ul>"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/#summary","title":"Summary:","text":"Use case Recommended? Stateful app on Kind/dev Yes (for learning/testing) Stateful app on prod Kubernetes Yes, if nodes have good disks Very high performance storage Consider other enterprise options # You can check existing StorageClasses by running: <pre><code>kubectl get storageclass\n\nroot@dev-cluster-control-plane:/# kubectl get storageclass\nNAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  46m\n</code></pre> Field What it Does Common Values reclaimPolicy Tells Kubernetes what to do with the actual disk when you delete a PVC \u2022 <code>Delete</code> \u2013 also deletes the PV and its data\u2022 <code>Retain</code> \u2013 keeps the PV and data for manual cleanup volumeBindingMode Controls when Kubernetes assigns a PV to a PVC \u2022 <code>Immediate</code> \u2013 binds as soon as PVC is created\u2022 <code>WaitForFirstConsumer</code> \u2013 waits until a pod uses the PVC, so PV lands on the right node allowVolumeExpansion Lets you grow the size of a volume after the PVC is already created \u2022 <code>true</code> \u2013 you can increase PVC\u2019s storage size\u2022 <code>false</code> \u2013 size is fixed once PVC is made - Use <code>Delete</code> reclaimPolicy if you want cleanup to happen automatically. - Use <code>WaitForFirstConsumer</code> bindingMode for StatefulSets or multi-zone clusters so the volume shows up on the pod\u2019s node. - Set allowVolumeExpansion to <code>true</code> if you expect to need more space later. <pre><code># longhorn-storageclass.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn\nprovisioner: driver.longhorn.io\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\n\n---\n# statefulset-with-longhorn.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-stateful\nspec:\n  serviceName: \"nginx\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: longhorn\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/#steps-to-apply","title":"Steps to apply:","text":"<pre><code>kubectl apply -f longhorn-storageclass.yaml\nkubectl apply -f statefulset-with-longhorn.yaml\n</code></pre>"},{"location":"Longhorn/Longhorn%20for%20stateful%20applications/#what-it-does","title":"What it does:","text":"<ul> <li> <p>Creates a Longhorn StorageClass that supports dynamic provisioning and volume expansion.</p> </li> <li> <p>Deploys an Nginx StatefulSet with 3 replicas.</p> </li> <li> <p>Each pod gets its own 5Gi Longhorn volume mounted at <code>/usr/share/nginx/html</code>.</p> </li> <li> <p>Data persists across pod restarts and rescheduling.</p> </li> </ul> <p>Check PVCs and pods:</p> <pre><code>kubectl get pvc\nkubectl get pods -l app=nginx\n</code></pre> <p>You can exec into any pod and add files inside <code>/usr/share/nginx/html</code> to test persistence.</p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/","title":"Longhorn in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Longhorn is a Kubernetes-native, lightweight block storage solution that runs inside your K8s/K3s cluster and provides high-availability persistent volumes (PVs) using replication, snapshots, and backups.</p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#longhorn-features-overview","title":"\ud83d\udd11 Longhorn Features Overview","text":"Feature Description Block Storage Provides RWO PVCs (ReadWriteOnce) using local disks of Kubernetes nodes High Availability Replicates volumes across multiple nodes (1, 2, 3 replicas) Snapshots Instant, crash-consistent, internal snapshots Backups Backups to S3/MinIO; usable across clusters Incremental Backup Smart, delta-based backups to reduce data transfer CSI Driver Fully CSI-compliant \u2014 integrates with PVCs, Velero, VolumeSnapshots Web UI Intuitive UI to manage volumes, nodes, backups, replicas, snapshots Live Volume Expansion PVCs can be resized without downtime Node/Volume Health Monitoring Auto-recovery, replica rebalancing, volume rebuilding on failure Cross-Cluster Restore You can backup from one cluster and restore into another Disaster Recovery (DR) Volumes Automatically syncs DR volumes from backups to a secondary cluster Soft Anti-Affinity Replicas are placed across nodes to tolerate failures Data Locality Optimizes read IO to prefer local replicas for performance Recurring Jobs Automated snapshot and backup schedules REST API Full management access through HTTP API Metrics + Prometheus Exporter Exposes health, usage, and volume stats for observability Security (RBAC, namespace isolation) All operations secured with Kubernetes RBAC and isolation Support for ARM64 &amp; x86_64 Works on Raspberry Pi, edge, and cloud-native infra ## \ud83d\udce6 Storage Capabilities Feature Support PVC Expansion \u2705 ReadWriteOnce \u2705 ReadWriteMany (RWX) \u274c (use NFS or shared PVC) Snapshot to Volume Restore \u2705 Clone Volume \u2705 Backup/Restore to S3 \u2705 Restic Integration (Velero) \u2705 ## \ud83d\udd27 System Management Features Area Features Volume Mgmt Create, delete, expand, attach/detach Replica Mgmt Self-healing, auto-rebuild, anti-affinity Node Mgmt Drain nodes, enable/disable scheduling Backup Mgmt Manual/automatic backup to S3 Scheduling Soft anti-affinity, disk tagging, node zoning Monitoring Alerts, metrics, Prometheus, health checks ## \ud83d\udee1\ufe0f HA &amp; Failure Recovery Scenario Longhorn Behavior Node failure Volume auto-detached &amp; reattached to healthy node Replica failure Auto rebuild using healthy replicas Pod rescheduling PVCs follow pods automatically Volume corruption Restore from snapshot/backup ## \ud83c\udf9b\ufe0f Longhorn Architecture (Internal Components) Component Purpose Longhorn Manager Control plane to manage volumes, replicas, scheduling Instance Manager Runs actual volume processes (controller/replica) Engine Per-volume controller that talks to replicas Replica Lightweight process storing blocks on disk UI Frontend Web dashboard CSI Plugin Handles PVC operations"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#use-cases","title":"\ud83e\udde0 Use Cases","text":"Use Case Longhorn Fit? Notes HA persistent volumes \u2705 Use 3 replicas across nodes Stateful apps (MySQL, Mongo) \u2705 Use crash-consistent snapshots Dev/test backup pipelines \u2705 Automate recurring backup jobs Cross-cluster migration \u2705 Backup on cluster A, restore on cluster B Shared volume across pods (RWX) \u274c Use NFS or RWX wrapper (experimental only) <p>When you setup Longhorn in your cluster, it uses the storage available on the cluster nodes (e.g., the mounted <code>/mnt/longhorn</code> directory in Kind) as its underlying storage pool.</p> <ul> <li>Longhorn manages that storage to create persistent volumes (block devices).</li> <li>It dynamically allocates space from that pool when you create PVCs.</li> <li>So, total available storage = sum of the storage Longhorn can access on all nodes (like <code>/mnt/longhorn</code>).</li> </ul> Action Effect Mount <code>/mnt/longhorn</code> on nodes Provides raw storage space for Longhorn Deploy Longhorn Manages and exposes block storage volumes Create PVC with Longhorn SC Allocates storage from Longhorn pool #### 1. What is Longhorn? Longhorn is a cloud-native, distributed storage system for Kubernetes that provides persistent block storage using dynamically provisioned volumes. It is lightweight, easy to deploy, and highly resilient, making it ideal for stateful applications in Kubernetes. <p>Real-Time Scenario: A company running a MySQL database on Kubernetes needs persistent storage that survives pod restarts. Longhorn provides replicated volumes, ensuring data remains available even if a node fails.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#2-why-use-longhorn-in-kubernetes","title":"2. Why Use Longhorn in Kubernetes?","text":"<ul> <li>Decentralized Storage: No single point of failure.  </li> <li>Snapshot &amp; Backup: Supports point-in-time snapshots and backups to S3-compatible storage.  </li> <li>Thin Provisioning: Efficient disk space usage.  </li> <li>Cross-Cluster Replication: Enables disaster recovery.  </li> </ul> <p>Real-Time Scenario: An e-commerce platform running MongoDB needs automated backups. Longhorn takes snapshots every hour and backs them up to AWS S3, ensuring data recovery in case of corruption.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#3-how-does-longhorn-work","title":"3. How Does Longhorn Work?","text":"<ul> <li>Volume Replication: Each volume is replicated across multiple nodes (default: 3 replicas).  </li> <li>Scheduling: Longhorn dynamically schedules replicas based on node availability.  </li> <li>Recovery: If a node fails, Longhorn rebuilds replicas automatically.  </li> </ul> <p>Real-Time Scenario: A Node failure occurs in a Kubernetes cluster running a PostgreSQL database. Longhorn detects the failure and rebuilds the lost replica on another healthy node, ensuring high availability.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#4-where-is-longhorn-used","title":"4. Where is Longhorn Used?","text":"<ul> <li>On-Premises Kubernetes Clusters (e.g., Rancher, K3s)  </li> <li>Hybrid &amp; Multi-Cloud Deployments (AWS EKS, GCP GKE, Azure AKS)  </li> <li>Edge Computing &amp; IoT (Lightweight storage for distributed apps)  </li> </ul> <p>Real-Time Scenario: A financial services firm uses Longhorn in multi-cloud Kubernetes (EKS + on-prem) to ensure consistent storage for Fraud Detection Microservices, avoiding vendor lock-in.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#key-benefits-challenges","title":"Key Benefits &amp; Challenges","text":"Benefits Challenges \u2705 Easy to deploy &amp; manage \u274c Not suitable for high-throughput workloads (e.g., big data) \u2705 Self-healing &amp; fault-tolerant \u274c Requires proper resource planning (CPU/memory for replicas) \u2705 Works across multiple clouds \u274c Backup performance depends on network bandwidth ### Conclusion Longhorn is an ideal choice for Kubernetes users needing reliable, distributed block storage with easy backup &amp; recovery. It shines in stateful applications (databases, message queues) but may not be optimal for high-performance storage needs."},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/","title":"Longhorn with loopback file on Kind","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#what","title":"\ud83d\udd39 What:","text":"<p>Run Longhorn (distributed block storage for Kubernetes) on a Kind cluster.</p>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#why","title":"\ud83d\udd39 Why:","text":"<p>To test or demo Longhorn in a local Kubernetes environment using containers (Kind) without provisioning real disks or cloud block storage.</p>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#when","title":"\ud83d\udd39 When:","text":"<p>Use only for: - Dev/test/demo - Learning how Longhorn works - CI/CD pipeline checks (not real performance)</p> <p>\ud83d\uded1 Not for production due to limitations of Kind (runs in Docker, lacks real disks).</p>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#how","title":"\ud83d\udd39 How:","text":"Step Description 1. Create loopback file as fake disk on host 2. Mount it into Kind node(s) via <code>extraMounts</code> 3. Deploy Longhorn using Helm or manifest 4. Use StorageClass from Longhorn for PVCs ### Example: loopback setup for Kind"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#create-loop-disk","title":"Create loop disk:","text":"<pre><code>mkdir -p /mnt/longhorn\ndd if=/dev/zero of=/mnt/longhorn/longhorn-disk.img bs=1G count=10\nlosetup -fP /mnt/longhorn/longhorn-disk.img\nmkfs.ext4 /dev/loopX  # replace X with correct loop number\nmount /dev/loopX /mnt/longhorn\n</code></pre>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#kind-config","title":"Kind config:","text":"<pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraMounts:\n  - hostPath: /mnt/longhorn\n    containerPath: /mnt/longhorn\n- role: worker\n  extraMounts:\n  - hostPath: /mnt/longhorn\n    containerPath: /mnt/longhorn\n</code></pre>"},{"location":"Longhorn/Longhorn%20with%20loopback%20file%20on%20Kind/#deploy-longhorn","title":"Deploy Longhorn:","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn.yaml\n</code></pre> <p>Set <code>defaultDataPath: /mnt/longhorn</code> in Longhorn settings.</p>"},{"location":"Longhorn/Nginx%20pod%20with%20a%20volume%20backed%20by%20Longhorn/","title":"Nginx pod with a volume backed by Longhorn","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>When you setup Longhorn in your cluster, it uses the storage available on the cluster nodes (e.g., the mounted <code>/mnt/longhorn</code> directory in Kind) as its underlying storage pool.</p> <ul> <li>Longhorn manages that storage to create persistent volumes (block devices).</li> <li>It dynamically allocates space from that pool when you create PVCs.</li> <li>So, total available storage = sum of the storage Longhorn can access on all nodes (like <code>/mnt/longhorn</code>).</li> </ul> Action Effect Mount <code>/mnt/longhorn</code> on nodes Provides raw storage space for Longhorn Deploy Longhorn Manages and exposes block storage volumes Create PVC with Longhorn SC Allocates storage from Longhorn pool <pre><code># longhorn-pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: longhorn-pv-demo\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: longhorn\n  csi:\n    driver: driver.longhorn.io\n    fsType: ext4\n    volumeHandle: longhorn-pv-demo\n\n---\n# longhorn-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: longhorn-pvc-demo\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: longhorn\n  volumeName: longhorn-pv-demo\n\n---\n# app-with-pvc.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-longhorn-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-longhorn\n  template:\n    metadata:\n      labels:\n        app: nginx-longhorn\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable\n        volumeMounts:\n        - name: storage\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: longhorn-pvc-demo\n</code></pre>"},{"location":"Longhorn/Nginx%20pod%20with%20a%20volume%20backed%20by%20Longhorn/#usage","title":"Usage:","text":"<pre><code>kubectl apply -f longhorn-pv.yaml\nkubectl apply -f longhorn-pvc.yaml\nkubectl apply -f app-with-pvc.yaml\n</code></pre> <p>Check pod and PVC status:</p> <pre><code>kubectl get pvc\nkubectl get pv\nkubectl get pods\n</code></pre> <p>This deploys an Nginx pod with a volume backed by Longhorn storage mounted at <code>/usr/share/nginx/html</code> (serving static content with persistence). You can exec into the pod and create files there to verify persistence.</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/","title":"Steps to Install Longhorn on K3d","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#prerequisites","title":"\u2705 Prerequisites","text":"Requirement Status K3d HA cluster Already running \u2705 RWX/RWO filesystem ext4/xfs (not btrfs) Kernel modules <code>open-iscsi</code> not required inside Docker ## \ud83e\uddf1 Can Longhorn Be Inside Kubernetes or External? Excellent question. The answer depends on how you want to design your storage architecture. Here's a full breakdown: Deployment Mode Supported? Description \u2705 Inside Kubernetes \u2705 Yes (Default) Longhorn runs as a Kubernetes-native storage system (pods + CRDs inside your cluster) \u274c Fully external \u274c Not supported Longhorn cannot run as a standalone external storage appliance \u26a0\ufe0f Hybrid (dedicated storage cluster) \u26a0\ufe0f Yes, with advanced config You can build a dedicated Longhorn K3s cluster acting as networked storage for other clusters (via NFS/iSCSI)** ## \u2705 Default: Longhorn Inside Your K3s/K8s Cluster This is the recommended, supported, and most stable setup: - Longhorn runs in <code>longhorn-system</code> namespace - Each node contributes local disks (e.g., <code>/var/lib/longhorn</code>) - Volumes are created, replicated, snapshotted via Kubernetes APIs - Integrated with CSI ### Pros: - Simple, Kubernetes-native - No external dependencies - Works with Velero, VolumeSnapshotClass, PVCs ### Example Setup: <pre><code>kubectl get pods -n longhorn-system\nkubectl get storageclass longhorn\n</code></pre>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#advanced-dedicated-longhorn-cluster-hybrid-storage-mode","title":"\ud83e\uddea Advanced: Dedicated Longhorn Cluster (Hybrid Storage Mode)","text":"<p>You can run Longhorn in a separate K3s cluster, and expose volumes to other clusters via:</p> <ul> <li>iSCSI (manual)</li> <li>NFS (using Longhorn volumes + nfs-provisioner)</li> <li>Restic/NFS for Velero restore/backup</li> </ul> <p>But this is not supported directly by Longhorn as external storage \u2014 it's a workaround.</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#not-possible-longhorn-as-a-pure-external-storage-appliance","title":"\u274c Not Possible: Longhorn as a Pure External Storage Appliance","text":"<p>Unlike Ceph, TrueNAS, or NetApp: - Longhorn is not designed to be installed on bare metal or VMs outside Kubernetes - It must run inside a Kubernetes cluster (even a minimal K3s)</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#first-insight","title":"\ud83e\udde0 First Insight","text":"Storage Type Suitable For Longhorn inside K3s \u2705 Most apps, PVCs, CSI, snapshots Longhorn in separate K3s cluster (storage-only) \u26a0\ufe0f Shared storage for other clusters, requires NFS/iSCSI export External NAS (e.g. NFS, Ceph, SAN) \u2705 Multi-cluster, legacy workloads, direct access Longhorn as external appliance \u274c Not supported or intended"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#want-to-try-dedicated-storage-k3s-cluster-with-longhorn","title":"\ud83e\uddf0 Want to Try: Dedicated Storage K3s Cluster with Longhorn?","text":"<p>If you're interested in running Longhorn as a storage service using a separate K3s setup, I can provide:</p> <ul> <li>Setup steps for <code>longhorn + nfs-provisioner</code></li> <li>Export volumes from storage cluster</li> <li>Mount them on other K3s clusters via NFS PVC</li> </ul> <p>Let me know if you want that architecture.</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#steps-to-install-longhorn-on-k3d","title":"\ud83d\udee0\ufe0f Steps to Install Longhorn on K3d","text":""},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#1-label-a-node-with-storage-role","title":"1. Label a node with storage role","text":"<p>Longhorn needs at least one node to be storage-enabled:</p> <pre><code>kubectl label node k3d-ha-cluster-agent-0 longhorn-node=true\n</code></pre>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#2-install-longhorn-using-helm","title":"2. Install Longhorn using Helm","text":"<pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\n\nkubectl create namespace longhorn-system\n\nhelm install longhorn longhorn/longhorn \\\n  --namespace longhorn-system \\\n  --set defaultSettings.createDefaultDiskLabeledNodes=true \\\n  --set defaultSettings.defaultDataPath=\"/var/lib/longhorn\" \\\n  --set persistence.defaultClassReplicaCount=1\n\nor\n\nhelm install longhorn longhorn/longhorn --namespace longhorn-system\n# Verify CSI + StorageClass\nkubectl get storageclass\nShould show:\nNAME       PROVISIONER             DEFAULT\nlonghorn   driver.longhorn.io      no\n\n# To make longhorn as default:\nkubectl patch storageclass longhorn -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n$ kubectl get pods -n longhorn-system\n\ngouse@gouse:~$ k get storageclass\nNAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  8d\nlonghorn (default)     driver.longhorn.io      Delete          Immediate              true                   4m20s\nlonghorn-static        driver.longhorn.io      Delete          Immediate              true                   4m17s\n</code></pre> <p>\ud83e\uddfe What This Means</p> Attribute Value StorageClass Name <code>local-path</code> Provisioner <code>rancher.io/local-path</code> (default for K3s) Reclaim Policy <code>Delete</code> \u2014 PVs will be deleted when PVCs are deleted Binding Mode <code>WaitForFirstConsumer</code> \u2014 volume is not provisioned until Pod is scheduled Expansion Support \u274c <code>false</code> \u2014 PVCs cannot be resized dynamically Default? \u2705 Yes ## \ud83d\udce6 What is <code>longhorn-static</code> StorageClass? <p><code>longhorn-static</code> is a StorageClass used for statically provisioning volumes that were already created manually in the Longhorn UI or API \u2014 i.e., volumes that exist before any PVC binds to them.</p> Concept <code>longhorn</code> (dynamic) <code>longhorn-static</code> (static) Who creates the volume? Kubernetes (via PVC) You (via Longhorn UI, script, etc.) When is it created? When a PVC is applied Before PVC exists How is it bound? PVC \u2192 Longhorn creates volume PVC binds to pre-existing volume Use case Dynamic workloads Manual restore, pre-imported disks, migration ## \u2705 When to Use <code>longhorn-static</code> <p>You use <code>longhorn-static</code> when:</p> <ol> <li>You manually created a volume in the Longhorn UI     (e.g., from a backup, snapshot, or disk import)</li> <li>You want to bind it to a PVC inside Kubernetes</li> <li>You don\u2019t want K8s to create a new volume, but reuse an existing one</li> </ol> Concept Why It Matters CSI = Container Storage Interface Standard for dynamic volume provisioning across all K8s distros Longhorn/Ceph use CSI drivers To allow <code>PersistentVolumeClaim</code> to dynamically bind to real volumes Works with Velero For CSI snapshots (backup/restore PVCs)"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#3-access-longhorn-ui","title":"3. Access Longhorn UI","text":"<p>Expose it via port-forward: <pre><code>kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80\n</code></pre> Open http://localhost:8080</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#note-for-k3d","title":"\u26a0\ufe0f Note for K3d","text":"<p>By default, K3d agent containers use Docker volumes, so Longhorn's block device support is virtual. It still works fine for dev/testing.</p> <p>If Longhorn fails to schedule volumes, check: <pre><code>kubectl get nodes -o wide\nkubectl get pods -n longhorn-system\n</code></pre></p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#verify","title":"\u2705 Verify","text":"<pre><code>kubectl get sc\nkubectl get volumes -n longhorn-system\n</code></pre> <p>Let me know if you want it added to your K3d YAML config or automated as a script.</p>"},{"location":"Longhorn/Steps%20to%20Install%20Longhorn%20on%20K3d/#how-to-list-existing-longhorn-volumes","title":"How to List Existing Longhorn Volumes","text":"<pre><code>kubectl -n longhorn-system exec -it &lt;longhorn-manager-pod&gt; -- longhorn ls\n</code></pre>"},{"location":"Longhorn/Use%20Longhorn%20for%20PV%20and%20mount%20to%20MinIO/","title":"Use Longhorn for PV and mount to MinIO","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>\u2705 Yes \u2014 your setup is correct and valid:</p> <p>Use Longhorn to create PersistentVolume \u2192 Mount it in MinIO \u2192 Store etcd backups in MinIO (S3-compatible)</p>"},{"location":"Longhorn/Use%20Longhorn%20for%20PV%20and%20mount%20to%20MinIO/#setup-flow","title":"\ud83d\udd01 Setup Flow","text":"<pre><code>Longhorn Volume (PVC)\n       \u2193\n     MinIO Pod (Mounted /data)\n       \u2193\n   S3-Compatible Bucket (via MinIO UI/API)\n       \u2193\n   etcd Backups stored here\n</code></pre>"},{"location":"Longhorn/Use%20Longhorn%20for%20PV%20and%20mount%20to%20MinIO/#why-this-setup-is-good","title":"\u2705 Why This Setup is Good","text":"\u2705 Advantage Description Fully on-prem No external cloud needed Durable storage via Longhorn Replicated, snapshot-ready S3 compatibility Works with Velero, Stash, etcdctl scripts Scalable Add more buckets, users, creds as needed ### \u2699\ufe0f Suggested Setup Steps <ol> <li>Create Longhorn volume via UI or PVC.</li> <li>Deploy MinIO with that PVC mounted to <code>/data</code>.</li> <li>Access MinIO UI \u2192 Create a bucket (e.g. <code>etcd-backups</code>).</li> <li> <p>Use MinIO endpoint + access key in backup tools like:</p> <ul> <li>Stash <code>Repository</code> CRD</li> <li><code>aws-cli</code> for etcdctl-based backup</li> <li><code>kube-etcd-backup</code> config YAML</li> </ul> </li> </ol>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/","title":"Using Longhorn Storage with Kind Cluster","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Sure! Here's the final concise documentation on Using Longhorn with Kind cluster and storage behavior:</p>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#using-longhorn-storage-with-kind-cluster","title":"Using Longhorn Storage with Kind Cluster","text":""},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#overview","title":"Overview","text":"<p>Kind runs Kubernetes inside Docker containers, providing ephemeral nodes without persistent storage by default. To enable persistent volumes (PV) with dynamic provisioning, you can deploy Longhorn, a distributed block storage solution.</p>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#1-default-storage-behavior-in-kind","title":"1. Default Storage Behavior in Kind","text":"<ul> <li>Pods without PVC use ephemeral container storage inside the node container.</li> <li>Data is lost if pod or node restarts.</li> <li>PVC requests without a storage provisioner remain in Pending state.</li> <li>No auto volume expansion without external storage configured.</li> </ul>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#2-setting-up-longhorn-on-kind","title":"2. Setting Up Longhorn on Kind","text":""},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a loopback file on the host as a fake block device:</li> </ul> <pre><code>mkdir -p /mnt/longhorn\ndd if=/dev/zero of=/mnt/longhorn/longhorn-disk.img bs=1G count=10\nlosetup -fP /mnt/longhorn/longhorn-disk.img\nmkfs.ext4 /dev/loopX  # Replace X with actual loop device number\nmount /dev/loopX /mnt/longhorn\n</code></pre>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#kind-cluster-config-with-mounts","title":"Kind Cluster Config with Mounts","text":"<p><pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraMounts:\n  - hostPath: /mnt/longhorn\n    containerPath: /mnt/longhorn\n- role: worker\n  extraMounts:\n  - hostPath: /mnt/longhorn\n    containerPath: /mnt/longhorn\n</code></pre> Create cluster using this config.</p>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#deploy-longhorn","title":"Deploy Longhorn","text":"<p><pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn.yaml\n</code></pre> Set Longhorn\u2019s default data path to <code>/mnt/longhorn</code> in the Longhorn UI or via manifest.</p>"},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#3-storage-usage","title":"3. Storage Usage","text":"Storage Type Location Notes Pod ephemeral storage Inside container FS Lost on restart PVC with Longhorn <code>/mnt/longhorn</code> on Kind nodes Persistent, managed by Longhorn PVC without provisioner PVC stays Pending No storage provisioned ## 4. Storage Expansion - Loopback file size does NOT auto-expand. - To increase storage, manually create and mount a larger loopback file or add new devices. - Real production environments use real disks/cloud storage which support dynamic expansion."},{"location":"Longhorn/Using%20Longhorn%20Storage%20with%20Kind%20Cluster/#5-recommendations","title":"5. Recommendations","text":"<ul> <li>Use Longhorn on Kind only for dev/test/demo.</li> <li>For production or real HA with persistent storage, use K3s or kubeadm clusters on VMs with real storage devices.</li> <li>Consider using Ingress controllers to reduce host port mappings and complexity.</li> </ul>"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/","title":"MinIO cli with MinIO and velero","text":"<p>The <code>mc</code> command is the MinIO Client \u2014 a command-line tool used to interact with S3-compatible object storage like MinIO.</p>"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/#install-mc-minio-client","title":"\ud83d\udd27 Install <code>mc</code> (MinIO Client)","text":"<pre><code>curl -O https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\nsudo mv mc /usr/local/bin/\n</code></pre>"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/#common-mc-commands","title":"\u2705 Common <code>mc</code> Commands","text":"Command Description <code>mc alias set</code> Connects <code>mc</code> to a MinIO/S3 server <code>mc mb</code> Makes (creates) a new bucket <code>mc ls</code> Lists buckets or contents <code>mc cp</code> Copies files to/from bucket <code>mc rm</code> Removes files or buckets"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/#example-for-your-case","title":"\ud83d\udccc Example for Your Case","text":"<pre><code># Connect to your MinIO server\nmc alias set local http://172.18.0.1:9001 admin admin123\n\ngouse@gouse:~/DevOps/multipass_scripts$ mc alias set local http://172.18.0.1:9001 admin admin123 mc: &lt;ERROR&gt; Unable to initialize new alias from the provided credentials. S3 API Requests must be made to API port.\n\nThe error message indicates you are trying to connect to the **MinIO Console port (`9001`)**, but **`mc` requires the S3 API port**, which by default is **`9000`**, **not `9001`**\n\n# **Start/Verify MinIO server is running with API on port `9000`**:\nexport MINIO_ROOT_USER=admin\nexport MINIO_ROOT_PASSWORD=admin123\nminio server /data/minio --address \":9000\" --console-address \":9001\"\n\n# **Then run `mc` against port `9000`**:\nmc alias set local http://172.18.0.1:9000 admin admin123\n\n\n# Create a bucket named \"velero\"\nmc mb local/velero\n\n# List all buckets\nmc ls local\n</code></pre> <p>\u2705 Perfect! Your MinIO is now correctly connected, and the <code>velero</code> bucket is created.</p>"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/#now-proceed-with-velero-installation","title":"\u2705 Now proceed with Velero installation:","text":"<pre><code>cat &lt;&lt;EOF &gt; credentials-velero\n[default]\naws_access_key_id = admin\naws_secret_access_key = admin123\nEOF\n\nvelero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.8.0 \\\n  --bucket velero \\\n  --secret-file ./credentials-velero \\\n  --backup-location-config region=minio,s3ForcePathStyle=true,s3Url=http://172.18.0.1:9000 \\\n  --use-volume-snapshots=false\n</code></pre>"},{"location":"MinIO/MinIO%20cli%20with%20MinIO%20and%20velero/#after-installation","title":"\ud83d\udccc After installation","text":"<ul> <li>Check Velero pod:</li> </ul> <pre><code>kubectl get pods -n velero\nkubectl logs deployment/velero -n velero\n</code></pre>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/","title":"MinIO external for Velero backups","text":""},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#question","title":"\ud83d\udca1Question","text":"<p>Should I deploy MinIO inside K3s using Helm with Longhorn PVC, or install it externally and point Velero to it?</p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#short-answer","title":"\u2705 Short Answer:","text":"<p>You can do either, but for low-disk-space environments (like your 10Gi Multipass VMs), it\u2019s better to:</p> <p>\u2705 Run MinIO externally and use it with Velero.</p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#option-comparison","title":"\u2696\ufe0f Option Comparison","text":"Option Deploy MinIO Inside K3s Use External MinIO (outside cluster) Disk usage Consumes Longhorn storage (needs 10Gi PVC) \u274c Only writes backups remotely \u2705 Simplicity Easy to manage via Helm \u2705 Slightly more setup (binary or Docker) \u26a0\ufe0f Best for local K3s/Multipass testing \u274c Not recommended due to low space \u2705 Perfect match Performance Good, but competes with apps for disk Depends on network, usually good High availability Follows cluster HA rules Can be HA if MinIO is clustered externally ## \u2705 Recommended for You: Use External MinIO <p>Since your Multipass VMs only have ~10Gi, using an internal MinIO with <code>--set persistence.size=10Gi</code> is unschedulable due to Longhorn disk limits \u2014 this is why you're getting volumes stuck in <code>Fault</code>.</p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#what-you-should-do","title":"\u2705 What You Should Do","text":"<ol> <li>\ud83d\udcbe Set up MinIO externally (on your host Ubuntu machine)</li> <li>\ud83c\udfaf Point Velero to that MinIO instance using its IP and port</li> </ol>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#how-to-install-minio-on-ubuntu-external","title":"\ud83d\udee0\ufe0f How to Install MinIO on Ubuntu (External)","text":"<pre><code>wget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\nsudo mv minio /usr/local/bin/\n</code></pre> <p>Create data directory:</p> <pre><code>sudo mkdir -p /data/minio\n</code></pre> <p>Run MinIO:</p> <pre><code>export MINIO_ROOT_USER=minio\nexport MINIO_ROOT_PASSWORD=minio123\n\nminio server /data/minio --console-address \":9001\"\n</code></pre> <p>Access:</p> <ul> <li>Console: <code>http://&lt;your-host-ip&gt;:9001</code></li> <li>API/S3: <code>http://&lt;your-host-ip&gt;:9000</code></li> </ul>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#how-to-run-your-minio-server-as-a-systemd-service-that","title":"How to run your MinIO server as a systemd service that:","text":"<ul> <li>\u2705 Starts on boot</li> <li>\u2705 Auto-restarts if it crashes</li> <li>\u2705 Keeps your <code>MINIO_ROOT_USER</code> and <code>PASSWORD</code> securely defined</li> </ul>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#step-by-step-run-minio-as-a-systemd-service","title":"\u2705 Step-by-Step: Run MinIO as a systemd Service","text":""},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#1-create-a-minio-environment-file","title":"1\ufe0f\u20e3 Create a MinIO Environment File","text":"<pre><code>sudo vi /etc/default/minio\n</code></pre> <p>Paste: <pre><code>MINIO_ROOT_USER=minio\nMINIO_ROOT_PASSWORD=minio123\n</code></pre></p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#2-create-the-systemd-service-unit","title":"2\ufe0f\u20e3 Create the systemd Service Unit","text":"<pre><code>sudo vi /etc/systemd/system/minio.service\n</code></pre> <p>Paste:</p> <pre><code>[Unit]\nDescription=MinIO S3 Server\nAfter=network.target\n\n[Service]\nEnvironmentFile=/etc/default/minio\nExecStart=/usr/local/bin/minio server /data/minio --console-address \":9001\"\nWorkingDirectory=/data/minio\nRestart=always\nRestartSec=5\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#3-reload-systemd-enable-and-start-minio","title":"3\ufe0f\u20e3 Reload systemd, Enable and Start MinIO","text":"<pre><code>sudo systemctl daemon-reexec\nsudo systemctl daemon-reload\nsudo systemctl enable minio\nsudo systemctl start minio\nsudo systemctl status minio --no-page\n</code></pre>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#4-verify-status","title":"4\ufe0f\u20e3 Verify Status","text":"<pre><code>sudo systemctl status minio\n</code></pre> <p>You should see <code>active (running)</code></p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#5-access-urls","title":"5\ufe0f\u20e3 Access URLs","text":"Service URL MinIO Console http://:9001 S3 API Endpoint http://:9000"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#configure-velero-to-use-external-minio","title":"\ud83d\udd17 Configure Velero to Use External MinIO","text":"<p>Prepare <code>minio-credentials</code> file:</p> <pre><code>[default]\naws_access_key_id = admin\naws_secret_access_key = admin123\n</code></pre> <p>Install Velero:</p> <pre><code>velero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.9.0,velero/velero-plugin-for-csi:v0.7.0 \\\n  --bucket velero \\\n  --secret-file ./minio-credentials \\\n  --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://&lt;host-ip&gt;:9000 \\\n  --use-volume-snapshots=true \\\n  --snapshot-location-config region=minio\n</code></pre> <p>Replace <code>&lt;host-ip&gt;</code> with your Ubuntu host IP accessible from inside the K3s cluster (often <code>192.168.122.1</code> in Multipass).</p>"},{"location":"MinIO/MinIO%20external%20for%20Velero%20backups/#result","title":"\u2705 Result","text":"<ul> <li> <p>MinIO runs outside cluster</p> </li> <li> <p>Uses your host's disk (no Longhorn space used)</p> </li> <li> <p>Velero backups succeed via CSI snapshots</p> </li> <li> <p>You keep Longhorn space free for critical workloads</p> </li> </ul> <p>Let me know if you want:</p> <ul> <li> <p>A systemd unit to auto-run MinIO</p> </li> <li> <p>Script to test Velero backup + restore with external MinIO</p> </li> <li> <p>Full local S3-compatible backup architecture diagram</p> </li> </ul>"},{"location":"Namespace/Default%20namespaces/","title":"Default namespaces","text":"<p>Creates four default namespaces to manage cluster resources - default - kube-node-lease - kube-public - ku be-system</p> <ul> <li>Resource Isolate resources in a cluster, preventing interference between users or teams</li> <li>Resource Quotas Enforce limits on resource usage to avoid excessive consumption by any single team</li> <li>Resource Organization Helps in organizing and managing resources specific to application or environment</li> </ul>"},{"location":"Namespace/Delete%20all%20resources%20from%20namespace/","title":"Delete all resources from namespace","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Run the following command to delete all resources in a namespace:</p> <pre><code>kubectl delete all --all -n demo\n</code></pre> <p>To also delete configmaps, secrets, PVCs, etc., run:</p> <pre><code>kubectl delete all,cm,secret,pvc,ingress,role,rolebinding,serviceaccount --all -n demo\n</code></pre> <p>Or to delete the entire namespace (which deletes everything inside):</p> <pre><code>kubectl delete namespace demo\n</code></pre>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/","title":"Create a Docker image registry in Nexus","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Below are the steps to create a Docker image registry in Nexus (using Nexus OSS) and allow clients to push/pull using username/password authentication.</p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#1-install-nexus-repository-oss","title":"\u2705 1. Install Nexus Repository OSS","text":"<p>On a server (e.g., <code>10.14.14.17</code>):</p> <pre><code>docker run -d -p 8081:8081 --name nexus \\\n  -v nexus-data:/nexus-data \\\n  sonatype/nexus3\n</code></pre> <p>Access via: <code>http://10.14.14.17:8081</code></p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#2-login-setup","title":"\u2705 2. Login &amp; Setup","text":"<ul> <li> <p>Default credentials:</p> <ul> <li>User: <code>admin</code></li> <li> <p>Password: check with:</p> <pre><code>docker exec nexus cat /nexus-data/admin.password\n</code></pre> </li> </ul> </li> <li> <p>Login \u2192 Change password</p> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#3-create-a-docker-hosted-registry","title":"\u2705 3. Create a Docker Hosted Registry","text":"<ul> <li>Go to: \"Repositories\" \u2192 \"Create repository\"</li> <li> <p>Choose: docker (hosted)</p> </li> <li> <p>Fill:</p> <ul> <li>Name: <code>docker-hosted</code></li> <li>HTTP port: <code>5007</code></li> <li>Blob store: (default)</li> <li>Enable Allow anonymous access (optional)</li> </ul> </li> <li> <p>Click Create</p> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#4-create-a-user","title":"\u2705 4. Create a User","text":"<ul> <li>Go to: \"Security\" \u2192 \"Users\"</li> <li>Create a new user (e.g., <code>dockeruser</code>)</li> <li>Set roles:<ul> <li><code>nx-repository-view-docker-docker-hosted-*</code> (read + write)</li> </ul> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#5-allow-docker-client-access-insecure-or-secure","title":"\u2705 5. Allow Docker Client Access (Insecure or Secure)","text":""},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#option-a-insecure-registry","title":"Option A: Insecure Registry","text":"<p>Edit <code>/etc/containers/registries.conf</code> (Podman) or <code>/etc/docker/daemon.json</code> (Docker):</p> <pre><code>{\n  \"insecure-registries\": [\"10.14.14.17:5007\"]\n}\n</code></pre> <p>Then restart:</p> <pre><code>sudo systemctl restart docker\n# or\nsudo systemctl restart podman\n</code></pre>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#6-login-from-client-machine","title":"\u2705 6. Login from Client Machine","text":"<pre><code>podman login 10.14.14.17:5007\n# OR\ndocker login 10.14.14.17:5007\n</code></pre> <p>Enter: - Username: <code>dockeruser</code> - Password: (your created password)</p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#7-tag-and-push-image","title":"\u2705 7. Tag and Push Image","text":"<pre><code>podman tag localhost/ubi-unzip:latest 10.14.14.17:5007/cron-image/busybox:latest\npodman push 10.14.14.17:5007/cron-image/busybox:latest\n</code></pre> <p>Let me know if you want to add TLS/SSL or set up proxy/hosted group registries.</p>"},{"location":"Nexus/Nexus%20Repository%20Manager%20OSS%20and%20integrate%20it%20with%20your%20Kubernetes/","title":"Nexus Repository Manager OSS and integrate it with your Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Nexus/Nexus%20Repository%20Manager%20OSS%20and%20integrate%20it%20with%20your%20Kubernetes/#steps-overview","title":"\u2705 Steps Overview","text":"<ol> <li>Deploy Nexus on your cluster</li> <li>Expose Nexus (via NodePort or Ingress)</li> <li>Persistent storage (PVC)</li> <li>Access Nexus UI &amp; configure repos</li> <li>Use Nexus repo in CI/CD or package managers</li> </ol>"},{"location":"Nexus/Nexus%20Repository%20Manager%20OSS%20and%20integrate%20it%20with%20your%20Kubernetes/#nexus-deployment-yaml-basic","title":"\ud83d\udc33 Nexus Deployment YAML (Basic)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nexus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nexus\n  template:\n    metadata:\n      labels:\n        app: nexus\n    spec:\n      containers:\n      - name: nexus\n        image: sonatype/nexus3:latest\n        ports:\n        - containerPort: 8081\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n        resources:\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1\"\n      volumes:\n      - name: nexus-data\n        persistentVolumeClaim:\n          claimName: nexus-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nexus\nspec:\n  type: NodePort\n  ports:\n    - port: 8081\n      targetPort: 8081\n      nodePort: 30081\n  selector:\n    app: nexus\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nexus-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Nexus/Nexus%20Repository%20Manager%20OSS%20and%20integrate%20it%20with%20your%20Kubernetes/#access-nexus","title":"\ud83d\udca1 Access Nexus","text":"<ul> <li>Run: <code>kubectl port-forward svc/nexus 8081:8081</code></li> <li>Or: open <code>http://&lt;NODE_IP&gt;:30081</code></li> <li>Default credentials: <code>admin / &lt;get from admin.password file inside container&gt;</code></li> </ul>"},{"location":"Nexus/Nexus%20Repository%20Manager%20OSS%20and%20integrate%20it%20with%20your%20Kubernetes/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":"Tool Integration Point Docker Push/pull from Nexus Docker registry Maven/Gradle Set Nexus as artifact repo Helm Host your own Helm chart repo CI/CD (Jenkins/GitLab) Use Nexus as artifact/image/cache store"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/","title":"Nexus oss setup   On Prem","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/#use-this-correct-working-download-command","title":"\u2705 Use this correct working download command:","text":"<pre><code>wget https://sonatype-download.global.ssl.fastly.net/repository/downloads-prod-group/3/nexus-3.81.1-01-linux-x86_64.tar.gz\n</code></pre> <p>\ud83d\udd04 After downloading: <pre><code>tar -xvzf nexus-3.68.0-01-unix.tar.gz\nmv nexus-3.68.0-01 nexus\n</code></pre></p>"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/#steps-to-run-nexus-under-systemd-with-current-user","title":"\u2705 Steps to run Nexus under systemd with current user","text":"<p>Assuming Nexus is extracted to: <code>~/nexus</code> And data dir is: <code>~/sonatype-work</code></p> <ol> <li> <p>Edit the <code>nexus.rc</code> file     Set run_as_user empty so it runs as current user:     <pre><code>echo 'run_as_user=\"\"' &gt; ~/nexus/bin/nexus.rc\n</code></pre></p> </li> <li> <p>Create a systemd service file <pre><code>sudo tee /etc/systemd/system/nexus.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=Nexus Repository Manager\nAfter=network.target\n\n[Service]\nType=forking\nExecStart=${HOME}/nexus3/nexus/bin/nexus start\nExecStop=${HOME}/nexus3/nexus/bin/nexus stop\nUser=${USER}\nRestart=on-abort\nLimitNOFILE=65536\nTimeoutStartSec=120\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre></p> </li> <li> <p>Reload systemd and enable the service <pre><code>sudo systemctl daemon-reexec\nsudo systemctl daemon-reload\nsudo systemctl enable nexus\n</code></pre></p> </li> <li> <p>Start Nexus manually (first time) <pre><code>sudo systemctl start nexus\n</code></pre></p> </li> <li> <p>Check service status <pre><code>sudo systemctl status nexus\n</code></pre></p> </li> </ol> <p>Now Nexus will start automatically on boot under your user.</p> <pre><code>http://localhost:8081\nget the default password: \n/home/gouse/nexus3/sonatype-work/nexus3/admin.password\n</code></pre> <p>By default, Nexus Repository Manager OSS does not support external databases. It uses an embedded, file-based database (OrientDB) stored in:</p> <pre><code>&lt;sonatype-work&gt;/nexus3/db/\n</code></pre>"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/#no-support-for-external-databases-like","title":"\u274c No Support for External Databases Like:","text":"Database Supported? Notes PostgreSQL \u274c Not supported MySQL/MariaDB \u274c Not supported Oracle DB \u274c Not supported MongoDB \u274c Not supported ### \u2705 What Nexus Does Support: Component Type Notes Metadata &amp; config Embedded DB Uses embedded OrientDB Blob storage (artifacts) File system or S3 You can configure external blob stores LDAP/SSO External service Used for auth, not data ### \ud83d\udd10 Nexus Pro/Enterprise Edition Adds: <ul> <li>High Availability (HA)</li> <li>External object stores</li> <li>Smart proxy features     But still no external RDBMS support \u2014 everything remains file/embedded based.</li> </ul>"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/#conclusion","title":"\u2705 Conclusion:","text":"<p>You cannot use an external DB like PostgreSQL/MySQL with Nexus. All metadata and artifact info is stored internally, and backups are done by snapshotting the <code>sonatype-work/nexus3</code> directory.</p> <p>Let me know if you want steps to move blob storage to S3 or other persistent volume setup.</p>"},{"location":"Nexus/Nexus%20oss%20setup%20-%20On-Prem/#recommended-use-in-oss","title":"\u2699\ufe0f Recommended Use in OSS","text":"<ol> <li>Use MinIO/SeaweedFS/GlusterFS as a mounted local filesystem</li> <li>Point Nexus File Blob Store to that mount</li> <li>Use <code>rclone</code>/<code>restic</code> to backup to S3/GCS</li> </ol> <p>\u2705 Open Source Blob Storage Tools</p> Tool Type Compatible Use Case for Nexus OSS Notes MinIO S3-compatible Object Store \u274c* (not natively supported in OSS) Can be used with <code>rclone</code> or as NFS-like mount SeaweedFS Distributed File System \u2705 (mountable as local FS) Lightweight and good for blob storage CephFS Distributed File System \u2705 (via mount) Reliable at scale, mountable as local FS Nextcloud File Sync &amp; Share \u26a0\ufe0f (manual upload/backup) Useful for syncing artifacts externally rclone Sync tool (S3, GCS, etc.) \u2705 (for sync/backup) Can sync blobs from Nexus to cloud storage Restic Backup tool \u2705 (backup sonatype-work directory) Can back up to S3, GCS, etc. BorgBackup Backup tool \u2705 Fast deduplicated backup of blob data GlusterFS Distributed File System \u2705 Mount as blob store in Nexus OSS"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/","title":"Steps to uninstall Nexus it cleanly","text":"<p>If you installed Nexus 3 on Ubuntu manually (not via package manager), follow these steps to uninstall it cleanly:</p>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#1-stop-the-nexus-service","title":"\u2705 1. Stop the Nexus service","text":"<pre><code>sudo systemctl stop nexus\n</code></pre>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#2-disable-the-service","title":"\u2705 2. Disable the service","text":"<pre><code>sudo systemctl disable nexus\n</code></pre>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#3-remove-the-service-file","title":"\u2705 3. Remove the service file","text":"<pre><code>sudo rm /etc/systemd/system/nexus.service\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#4-remove-nexus-installation-directory","title":"\u2705 4. Remove Nexus installation directory","text":"<p>Replace path if you used a different install location <pre><code>sudo rm -rf /opt/nexus\nsudo rm -rf /opt/sonatype-work\n</code></pre></p>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#5-remove-nexus-user-if-created","title":"\u2705 5. Remove nexus user (if created)","text":"<pre><code>sudo userdel -r nexus\n</code></pre>"},{"location":"Nexus/Steps%20to%20uninstall%20Nexus%20it%20cleanly/#6-remove-logs-or-custom-configs-if-any","title":"\u2705 6. Remove logs or custom configs (if any)","text":"<pre><code>sudo rm -rf /var/log/nexus\nsudo rm -rf /etc/nexus\n</code></pre>"},{"location":"Nexus/nexus3-cli-deprecated%20not%20working/","title":"Nexus3 cli deprecated not working","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>there is an official Nexus CLI tool called <code>nexus3-cli</code>, which is open-source and supports basic operations for Nexus Repository Manager 3.</p>"},{"location":"Nexus/nexus3-cli-deprecated%20not%20working/#tool-nexus3-cli","title":"\ud83d\udce6 Tool: <code>nexus3-cli</code>","text":"Feature Description Tool Name <code>nexus3-cli</code> Language Python Maintained by Community (not Sonatype) Install via <code>pip install nexus3-cli</code> Supported operations Repositories, users, roles, blobstores, cleanup ### \u2705 Install it <pre><code>pip install nexus3-cli\n</code></pre>"},{"location":"Nexus/nexus3-cli-deprecated%20not%20working/#recommended-fix-use-a-virtual-environment","title":"\u2705 Recommended Fix: Use a virtual environment","text":"<pre><code>python3 -m venv ~/venvs/nexus3-cli\nsource ~/venvs/nexus3-cli/bin/activate\npip install nexus3-cli\npip install setuptools\n</code></pre> <p>\ud83e\uddea Optional: Install with <code>pipx</code> (preferred for CLI tools)</p> <pre><code># Install `pipx`:\n$ sudo apt install pipx\n$ pipx inject nexus3-cli setuptools\n# Install `nexus3-cli`:\n$ pipx install nexus3-cli\n$ pipx ensurepath\n$ source ~/.bashrc\n\n$ nexus3 --help\n</code></pre>"},{"location":"Nexus/nexus3-cli-deprecated%20not%20working/#basic-usage","title":"\ud83d\udd27 Basic Usage","text":"<pre><code>nexus3 --help\n</code></pre> <p>Example: list all hosted repos</p> <pre><code>nexus3 repository list --host http://localhost:8081 --user admin --password 'yourpass'\n</code></pre>"},{"location":"Nexus/nexus3-cli-deprecated%20not%20working/#key-features-supported","title":"\ud83d\udee0\ufe0f Key Features Supported","text":"Command What it does <code>repository list/create/delete</code> Manage repositories <code>user list/create/delete</code> Manage users <code>blobstore list/create</code> Manage blob stores <code>script list/run/delete</code> Manage Groovy scripts (REST automation) ### \ud83e\udde9 Alternatives Tool Type Notes <code>nexus3-cli</code> CLI tool Python-based, good for admin automation REST API HTTP-based Official API by Sonatype, full control via scripts Groovy scripts Embedded Run via UI/API, full server-side admin access"},{"location":"Nodejs/Installing%20Node.js%20with%20Apt%20from%20the%20Default%20Repositories/","title":"Installing Node.js with Apt from the Default Repositories","text":"<p>Ubuntu 20.04 contains a version of Node.js in its default repositories that can be used to provide a consistent experience across multiple systems. At the time of writing, the version in the repositories is 10.19. This will not be the latest version, but it should be stable and sufficient for quick experimentation with the language.</p> <p>Warning: the version of Node.js included with Ubuntu 20.04, version 10.19, is now unsupported and unmaintained. You should not use this version in production, and should refer to one of the other sections in this tutorial to install a more recent version of Node.</p> <p>To get this version, you can use the <code>apt</code> package manager. Refresh your local package index first:</p> <pre><code>sudo apt update\n</code></pre> <p>Then install Node.js:</p> <pre><code>sudo apt install nodejs\n</code></pre> <p>Check that the install was successful by querying <code>node</code> for its version number:</p> <pre><code>node -v\n</code></pre> <pre><code>Outputv10.19.0\n</code></pre> <p>If the package in the repositories suits your needs, this is all you need to do to get set up with Node.js. In most cases, you\u2019ll also want to also install <code>npm</code>, the Node.js package manager. You can do this by installing the <code>npm</code> package with <code>apt</code>:</p> <pre><code>sudo apt install npm\n</code></pre> <p>This allows you to install modules and packages to use with Node.js.</p> <p>At this point, you have successfully installed Node.js and <code>npm</code> using <code>apt</code> and the default Ubuntu software repositories. The next section will show how to use an alternate repository to install different versions of Node.js.</p>"},{"location":"Nodejs/Installing%20Node.js%20with%20Apt%20from%20the%20Default%20Repositories/#claude-code-overview","title":"Claude Code overview","text":"<p>https://docs.anthropic.com/en/docs/claude-code/overview Learn about Claude Code, Anthropic\u2019s agentic coding tool that lives in your terminal and helps you turn ideas into code faster than ever before.</p> <p>https://docs.anthropic.com/en/docs/claude-code/overview#get-started-in-30-seconds</p> <p>Get started in 30 seconds</p> <p>Prerequisites:\u00a0Node.js 18 or newer</p> <pre><code># Install Claude Code\nsudo npm install -g @anthropic-ai/claude-code\n\n# Navigate to your project\ncd your-awesome-project\n\n# Start coding with Claude\nclaude\n</code></pre> <p>That\u2019s it! You\u2019re ready to start coding with Claude.\u00a0Continue with Quickstart (5 mins) \u2192</p> <p>(Got specific setup needs or hit issues? See\u00a0advanced setup\u00a0or\u00a0troubleshooting.)</p>"},{"location":"OpenTofu/About%20OpenTofu/","title":"About OpenTofu","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>No, OpenTofu (a fork of Terraform) is not only for cloud infrastructure.</p> <p>https://opentofu.org/docs/intro/core-workflow/</p> What Open-source infrastructure-as-code (IaC) tool (Terraform fork) to manage infrastructure using <code>.tf</code> files. Why Automates provisioning, scaling, and managing infrastructure consistently and repeatably. Avoids manual errors. Where Can be used for cloud (AWS, Azure, GCP) and on-premises (VMware, OpenStack, bare metal, K8s, etc.) environments. When Use when you need to version, manage, or provision infrastructure automatically with code and apply the same config anywhere. How Write <code>.tf</code> configs \u2192 run <code>opentofu init</code> \u2192 <code>opentofu plan</code> \u2192 <code>opentofu apply</code>. Uses providers for target platforms. <p>\u2705 It supports cloud + hybrid + on-prem setups, depending on provider availability.</p> <p>Here\u2019s a categorized list of what you can do with OpenTofu:</p> Category Use Cases Cloud Infrastructure Provision VMs, networks, load balancers on AWS, Azure, GCP Kubernetes Deploy EKS, AKS, GKE, manage K8s resources via <code>kubernetes</code> provider On-Prem Infra Manage VMware, OpenStack, bare-metal servers Containers &amp; Clusters Create Docker containers, Podman setups, K3s/K8s clusters (via scripts) CI/CD Setup Provision Jenkins, GitLab Runners, Agents on cloud or VMs DNS Management Create/update DNS zones and records (Cloudflare, Route53, etc.) Load Balancers &amp; Firewalls Manage LB rules, WAFs, security groups IAM &amp; Security Create users, roles, policies in AWS, Azure, GCP Secrets Management Integrate with Vault, AWS Secrets Manager, SOPS Monitoring Tools Deploy Prometheus, Grafana, Loki stacks Database Setup Provision RDS, CloudSQL, MySQL/Postgres containers Serverless Manage Lambda, Azure Functions, GCP Cloud Functions Storage Create S3 buckets, disks, volumes, NFS mounts Networking VPCs, subnets, routes, VPN, NAT gateways Edge/IoT Provision edge nodes, IoT device config via custom providers or scripts Multi-Cloud Management Use single <code>.tf</code> to deploy infra across multiple clouds Automation Scripts Run shell scripts, Ansible roles, local-exec commands Version Control &amp; GitOps Manage infra via VCS triggers (GitHub Actions, GitLab CI, etc.) Infrastructure Testing Validate infra using Terratest, Checkov, or pre/post-hooks # Terraform vs OpenTofu Aspect Terraform OpenTofu License BSL (Business Source License) \u2014 restricted for large-scale use MPL (Mozilla Public License) \u2014 truly open-source Community Huge, well-established Growing fast with open-source supporters (e.g., Gruntwork, Spacelift) Enterprise Support Official support via HashiCorp No official enterprise from a single vendor (yet) State Management Terraform Cloud + CLI state backend options No Terraform Cloud, but supports same backends (S3, GCS, etc.) Feature Parity Leading with new features Follows Terraform v1.6+ features (frozen after the fork) Modules/Providers Vast ecosystem, official registry Uses same ecosystem; registry is mirrored and interchangeable Community Governance Controlled by HashiCorp Community-driven (by Linux Foundation, OpenTofu community) Tooling Support Supported by many tools: Atlantis, Spacelift, Env0, Terraform Cloud Most Terraform tools already compatible or adapting (e.g., Spacelift) Future Risk May restrict more under BSL, limited by HashiCorp's roadmap Open roadmap, safe from vendor lock-in Backward Compatibility Maintains it but subject to license terms Fully backward compatible with Terraform \u22641.6"},{"location":"OpenTofu/About%20OpenTofu/#tldr","title":"TL;DR","text":"<ul> <li> <p>\u2705 Use OpenTofu if you want open-source freedom, no vendor lock-in, and alignment with community-led governance.</p> </li> <li> <p>\u2705 Use Terraform if you need official enterprise support, and are OK with BSL and using Terraform Cloud.</p> </li> </ul> <p>Let me know if you want a command compatibility table too.</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/","title":"PV's and PVC's in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#what-are-persistentvolumes-pvs-and-persistentvolumeclaims-pvcs","title":"What are PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs)?","text":"<p>PersistentVolumes (PVs): - Cluster-wide storage resources provisioned by administrators - Represent physical storage (NFS, iSCSI, cloud storage, etc.) in the cluster - Have a lifecycle independent of any individual Pod - Can be statically provisioned (pre-created) or dynamically provisioned (on-demand)</p> <p>PersistentVolumeClaims (PVCs): - Requests for storage by users/application Pods - Bind to available PVs that match their requirements - Act as an abstraction layer between Pods and physical storage - Are namespaced resources (unlike PVs which are cluster-scoped)</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#why-use-pvs-and-pvcs","title":"Why use PVs and PVCs?","text":"<ol> <li>Data persistence: Survive Pod restarts and crashes</li> <li>Storage abstraction: Decouple storage configuration from Pod specs</li> <li>Dynamic provisioning: Automatically create storage when needed</li> <li>Resource management: Control storage allocation and access</li> <li>Portability: Same manifest can work across different storage backends</li> <li>Lifecycle management: Handle storage reclaim policies (retain, delete, recycle)</li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#when-to-use-pvs-and-pvcs","title":"When to use PVs and PVCs?","text":"<p>Use PV/PVC when: - Your application needs to persist data beyond a Pod's lifecycle - Multiple Pods need to share the same storage - You need different access modes (ReadWriteOnce, ReadOnlyMany, ReadWriteMany) - You want to manage storage separately from application deployments - You need storage with specific performance characteristics (SSD vs HDD) - You're running stateful applications (databases, message queues, etc.)</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#how-to-use-pvs-and-pvcs","title":"How to use PVs and PVCs?","text":""},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#basic-usage","title":"Basic Usage","text":"<ol> <li>Static Provisioning (Admin creates PV first)</li> </ol> <pre><code># PersistentVolume (cluster-admin creates)\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: manual\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre> <pre><code># PersistentVolumeClaim (user creates)\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n</code></pre> <ol> <li>Dynamic Provisioning (Automatic PV creation)</li> </ol> <pre><code># StorageClass (cluster-admin creates)\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp3\n</code></pre> <pre><code># PVC that triggers dynamic provisioning\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: dynamic-pvc\nspec:\n  storageClassName: fast\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n</code></pre> <ol> <li>Using PVC in a Pod</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: mycontainer\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/usr/share/nginx/html\"\n        name: my-storage\n  volumes:\n    - name: my-storage\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#advanced-usage","title":"Advanced Usage","text":"<ol> <li> <p>Volume Snapshots: <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: my-snapshot\nspec:\n  volumeSnapshotClassName: csi-aws-vsc\n  source:\n    persistentVolumeClaimName: my-pvc\n</code></pre></p> </li> <li> <p>Raw Block Volumes: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre></p> </li> <li> <p>Volume Expansion (requires StorageClass to allow it): <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: expandable\nprovisioner: kubernetes.io/aws-ebs\nallowVolumeExpansion: true\n</code></pre></p> </li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Use Dynamic Provisioning for most cases (simpler management)</li> <li>Set appropriate reclaim policies:</li> <li><code>Retain</code>: Keep data after PVC deletion (manual cleanup)</li> <li><code>Delete</code>: Automatically delete storage (be careful with production data)</li> <li>Use StorageClasses to abstract storage backend details</li> <li>Monitor storage usage to avoid unexpected costs</li> <li>Consider volume snapshots for backup strategies</li> <li>Use volume modes appropriately:</li> <li><code>Filesystem</code> (default) for most applications</li> <li><code>Block</code> for databases or performance-sensitive apps</li> <li>Set resource requests/limits for StatefulSets using PVCs</li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>PVC stuck in \"Pending\" state:</li> <li>Check if storage class exists</li> <li>Verify sufficient capacity is available</li> <li> <p>Check provisioner logs</p> </li> <li> <p>Mount errors:</p> </li> <li>Verify access modes match between PV and PVC</li> <li> <p>Check if multiple Pods are trying to use ReadWriteOnce</p> </li> <li> <p>Common commands:    <pre><code># Check PVs and PVCs\nkubectl get pv\nkubectl get pvc --all-namespaces\n\n# Describe resources for details\nkubectl describe pv my-pv\nkubectl describe pvc my-pvc\n\n# Check storage classes\nkubectl get storageclass\n\n# Check events for issues\nkubectl get events --sort-by=.metadata.creationTimestamp\n</code></pre></p> </li> <li> <p>Access modes:</p> </li> <li><code>ReadWriteOnce</code> (RWO): Read-write by one node</li> <li><code>ReadOnlyMany</code> (ROX): Read-only by many nodes</li> <li><code>ReadWriteMany</code> (RWX): Read-write by many nodes</li> </ol>"},{"location":"Pods/Debug%20pod%20%28ephermal%20container%29/","title":"Debug pod (ephermal container)","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes, you can access the pods, but not via external IP because it's a headless service (<code>ClusterIP: None</code>) \u2014 it only helps with pod discovery, not load balancing or external access.</p> <pre><code>gouse@gouse:~/DevOps/k3d$ kubectl run curlpod --rm -it --image=curlimages/curl -- /bin/sh\nIf you don't see a command prompt, try pressing enter.\n~ $ curl nginx-0.nginx-headless.default.svc.cluster.local\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n~ $ exit\nSession ended, resume using 'kubectl attach curlpod -c curlpod -i -t' command when the pod is running\npod \"curlpod\" deleted\n</code></pre>"},{"location":"Pods/Debug%20pod%20%28ephermal%20container%29/#ways-to-access","title":"\u2705 Ways to Access","text":""},{"location":"Pods/Debug%20pod%20%28ephermal%20container%29/#1-from-inside-the-cluster-eg-debug-pod","title":"1. From inside the cluster (e.g., debug pod):","text":"<pre><code>kubectl run curlpod --rm -it --image=curlimages/curl -- /bin/sh\n</code></pre> <p>Then try:</p> <pre><code>curl nginx-0.nginx-headless.default.svc.cluster.local\n</code></pre> <p>Or test all pods:</p> <pre><code>for i in 0 1 2; do curl nginx-$i.nginx-headless.default.svc.cluster.local; done\n</code></pre>"},{"location":"Pods/Debug%20pod%20%28ephermal%20container%29/#2-if-you-want-external-access-expose-with-a-regular-service","title":"2. If you want external access, expose with a regular service:","text":"<pre><code>kubectl expose statefulset nginx --name=nginx-service --port=80 --target-port=80 --type=NodePort\n</code></pre> <p>Then get:</p> <pre><code>kubectl get svc nginx-service\n</code></pre> <p>Access via:</p> <pre><code>http://&lt;NodeIP&gt;:&lt;NodePort&gt;\n</code></pre> <p>You can also do and access this <code>statefulset</code> with <code>headless applicaiton</code> to access, if you want an <code>Ingress</code> setup or <code>LoadBalancer</code> type instead.</p>"},{"location":"Pods/Kubernetes%20Pod%20Architecture%20%26%20Lifecycle/","title":"Kubernetes Pod Architecture & Lifecycle","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>![[K8s-Pod-Architecture-Lifecycle.png]]</p>"},{"location":"Pods/List%20all%20pods%20with%20their%20init%20containers%20and%20sidecar%20containers%20info/","title":"List all pods with their init containers and sidecar containers info","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># List all pods with their init containers and sidecar containers info\n\nkubectl get pods --all-namespaces -o json | jq -r '\n  .items[] | \n  {\n    namespace: .metadata.namespace,\n    pod: .metadata.name,\n    initContainers: (.spec.initContainers // [] | map(.name)),\n    containers: (.spec.containers // [] | map(.name))\n  } | \n  \"Namespace: \\(.namespace)\\nPod: \\(.pod)\\nInit Containers: \\(.initContainers | join(\", \"))\\nContainers: \\(.containers | join(\", \"))\\n\"\n'\n</code></pre> <p>If you don't have <code>jq</code>, you can do a simpler approach:</p> <pre><code>kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,POD:.metadata.name,INIT_CONTAINERS:.spec.initContainers[*].name,CONTAINERS:.spec.containers[*].name\n</code></pre> <p>This shows init containers and sidecar (all containers except main) by pod. Sidecars are normally just additional containers listed alongside main containers.</p>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/","title":"Types of Containers in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#types-of-containers-in-kubernetes-pods","title":"\ud83d\udce6 Types of Containers in Kubernetes Pods","text":"Type Purpose Example Use Case Main (App) Container The primary container that runs your application. nginx, flask app, database service Sidecar Container Provides helper services to the main container. Shared network/storage. log forwarder, proxy, config updater Init Container Runs before app containers, one-time tasks only. db migration, wait-for-service Ephemeral Container Temporary debugging container attached to a running pod. <code>kubectl debug</code> to troubleshoot issues"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#sidecar-pattern-explained","title":"\ud83d\udd04 Sidecar Pattern Explained","text":""},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#characteristics","title":"Characteristics:","text":"<ul> <li> <p>Runs alongside main app container in the same pod</p> </li> <li> <p>Shares:</p> <ul> <li>Network namespace (localhost communication)</li> <li>Volumes (for logs/configs etc.)</li> </ul> </li> </ul>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#typical-use-cases","title":"Typical Use Cases:","text":"<ul> <li>Log shippers (e.g., Fluentd)</li> <li>Service mesh proxies (e.g., Envoy in Istio)</li> <li>Auto-refreshing configuration agents</li> </ul>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#example-app-sidecar","title":"\ud83e\uddf1 Example: App + Sidecar","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-sidecar\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    ports:\n    - containerPort: 8080\n  - name: log-shipper\n    image: fluentd\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /logs\n  volumes:\n  - name: shared-logs\n    emptyDir: {}\n</code></pre>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/#summary-table","title":"\ud83e\udde0 Summary Table","text":"Container Type Runs When? Restarts with Pod? Use Case Main Container Always (normal flow) Yes Business logic Init Container Before main starts No Setup tasks Sidecar Container With main container Yes Logs, proxy, monitoring Ephemeral Container Injected at runtime No Debugging Type Description Use Cases Example Tools/Workloads Standard Containers OCI-compliant containers (Docker, containerd, CRI-O). Microservices, web apps. <code>nginx</code>, <code>redis</code>, custom apps. Windows Containers Containers running Windows OS (requires Windows nodes). .NET apps, legacy Windows services. IIS, SQL Server on Windows. Init Containers Run before main app containers in a pod for setup tasks. DB migrations, config preloading. <code>alpine</code> (for setup scripts). Sidecar Containers Auxiliary containers running alongside the main app in the same pod. Logging, monitoring, proxying. <code>Fluentd</code>, <code>Envoy</code>, <code>Prometheus exporter</code>. Ephemeral Containers Temporary containers for debugging (attached to running pods). Troubleshooting without restarting pods. <code>busybox</code>, <code>alpine</code> (for <code>kubectl debug</code>). Job/CronJob Runs a container to completion (Job) or on a schedule (CronJob). Batch processing, backups, reports. Database backups, data processing scripts. DaemonSet Runs one instance per node (cluster-wide). Log collectors, node monitoring. <code>kube-proxy</code>, <code>Node Exporter</code>. StatefulSet Manages stateful apps with stable network IDs and storage. Databases, distributed storage. <code>MySQL</code>, <code>MongoDB</code>, <code>Elasticsearch</code>. GPU-Accelerated Containers requiring GPUs/TPUs (needs node support). AI/ML workloads, video encoding. <code>TensorFlow</code>, <code>PyTorch</code>. Security-Hardened Containers with extra isolation (sandboxing/VMs). Multi-tenant clusters, untrusted workloads. <code>gVisor</code>, <code>Kata Containers</code>. Multi-Container Pods Pods with multiple containers sharing network/storage. Co-located helper services (e.g., log shipper + app). <code>app + Fluentd</code> sidecar. Serverless (FaaS) Containers deployed as serverless functions (scales to zero). Event-driven workloads. <code>Knative</code>, <code>OpenFaaS</code>, <code>AWS Lambda on EKS</code>. Wasm Containers WebAssembly-based workloads (experimental). Edge computing, lightweight plugins. <code>wasmtime</code>, <code>wasmedge</code>. ### Key Notes: 1. OCI Runtimes: Kubernetes now defaults to <code>containerd</code> or <code>CRI-O</code> (Docker is deprecated but still usable via shims). 2. Windows Support: Requires Windows nodes and compatible images. 3. Security: Use <code>gVisor</code>/<code>Kata</code> for sandboxing, <code>PodSecurityPolicy</code> (or Kyverno/OPA) for restrictions. 4. Debugging: Ephemeral containers allow live troubleshooting without pod restarts."},{"location":"Pods/Types%20of%20Pods/","title":"Types of Pods","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Type Description Single Container Pod Most basic pod, runs one container. Multi-Container Pod Pod with multiple containers sharing the same network/storage (sidecar pattern). Init Container Pod Pod that runs init containers before main containers start. Static Pod Managed by kubelet directly (not via API server), used in control plane nodes. DaemonSet Pod Runs one pod per node, ideal for logs/metrics collection (e.g., Fluentd, Prometheus node-exporter). Deployment Pod Created and managed by a Deployment object for rolling updates and scaling. StatefulSet Pod Used for stateful apps, gives stable hostname and storage (e.g., databases). Job/ CronJob Pod Runs to completion once (Job) or on schedule (CronJob). Ephemeral Container Pod Used for debugging, doesn't run app logic, added dynamically. Mirror Pod Clone of a static pod, visible to the API server for monitoring. ### 1. Single Container Pod <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>\ud83e\uddfe Use Case: Simple stateless app or test container.</p>"},{"location":"Pods/Types%20of%20Pods/#2-multi-container-pod","title":"2. Multi-Container Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n  - name: sidecar\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do echo Sidecar; sleep 30; done\"]\n</code></pre> <p>\ud83e\uddfe Use Case: Log agent or proxy sidecar with main app.</p>"},{"location":"Pods/Types%20of%20Pods/#3-init-container-pod","title":"3. Init Container Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: init-pod\nspec:\n  initContainers:\n  - name: init\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo initializing... &amp;&amp; sleep 10\"]\n  containers:\n  - name: app\n    image: nginx\n</code></pre> <p>\ud83e\uddfe Use Case: Pre-setup before app starts (e.g., config fetch, wait for DB).</p>"},{"location":"Pods/Types%20of%20Pods/#4-static-pod","title":"4. Static Pod","text":"<ul> <li>Save this to <code>/etc/kubernetes/manifests/static-nginx.yaml</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: static-nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>\ud83e\uddfe Use Case: Run core components (like etcd, controller) outside of scheduler.</p>"},{"location":"Pods/Types%20of%20Pods/#5-daemonset-pod","title":"5. DaemonSet Pod","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-logger\nspec:\n  selector:\n    matchLabels:\n      app: logger\n  template:\n    metadata:\n      labels:\n        app: logger\n    spec:\n      containers:\n      - name: log-agent\n        image: fluentd\n</code></pre> <p>\ud83e\uddfe Use Case: Run log collector or monitoring agent on every node.</p>"},{"location":"Pods/Types%20of%20Pods/#6-deployment-pod","title":"6. Deployment Pod","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre> <p>\ud83e\uddfe Use Case: Scalable, rolling update-ready web apps or APIs.</p>"},{"location":"Pods/Types%20of%20Pods/#7-statefulset-pod","title":"7. StatefulSet Pod","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: \"mysql\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: root\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre> <p>\ud83e\uddfe Use Case: Databases, message brokers needing persistent identity &amp; storage.</p>"},{"location":"Pods/Types%20of%20Pods/#8-job-pod","title":"8. Job Pod","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: one-time-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl\n        command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n</code></pre> <p>\ud83e\uddfe Use Case: One-time batch job (like data migration, backup).</p>"},{"location":"Pods/Types%20of%20Pods/#9-cronjob-pod","title":"9. CronJob Pod","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello-cron\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            command: [\"sh\", \"-c\", \"date; echo Hello\"]\n          restartPolicy: OnFailure\n</code></pre> <p>\ud83e\uddfe Use Case: Scheduled tasks (e.g., cleanup, backups, reports).</p>"},{"location":"Pods/Types%20of%20Pods/#10-ephemeral-container","title":"10. Ephemeral Container","text":"<pre><code>kubectl debug -it mypod --image=busybox --target=app -- /bin/sh\n</code></pre> <p>\ud83e\uddfe Use Case: On-demand debugging of a running pod (no restart needed).</p>"},{"location":"Pods/Types%20of%20Pods/#mirror-pod","title":"mirror pod","text":"<ul> <li>\ud83e\udde0 You create a static pod \u2705 Actual pod running on the node</li> <li>YAML Location: Saved under <code>--pod-manifest-path</code> (e.g., <code>/etc/kubernetes/manifests</code>)</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb Kubelet runs it</li> <li>\ud83d\udce1 Kubelet then informs the API server, which shows it as a mirror pod, </li> <li>\u274c Not actually running, it's just a representation of the static pod</li> </ul> <p>\u274c Static Pod \u2260 Mirror Pod \u2705 Static Pod \u2192 creates \u2192 Mirror Pod (API view only means, visible in <code>kubectl get pods</code>) \ud83d\udccc Every static pod gets a mirror pod automatically created in the API server.</p> <ol> <li> <p>Create the Static Pod YAML <pre><code>sudo tee /etc/kubernetes/manifests/mirror-nginx.yaml &gt; /dev/null &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mirror-nginx\n  labels:\n    app: mirror-nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\nEOF\n</code></pre></p> </li> <li> <p>Verify <pre><code>kubectl get pods -A | grep mirror-nginx\n</code></pre></p> </li> <li> <p>Optional Cleanup <pre><code>sudo rm /etc/kubernetes/manifests/mirror-nginx.yaml\n</code></pre></p> </li> </ol>"},{"location":"Pods/Types%20of%20Pods/#notes","title":"\u26a0\ufe0f Notes","text":"<ul> <li>Only works on nodes where <code>kubelet</code> is configured with <code>--pod-manifest-path=/etc/kubernetes/manifests</code>.</li> <li>Mainly used for core components (etcd, kube-apiserver, etc.) in kubeadm setups.</li> </ul>"},{"location":"Prometheus%20%2B%20Grafana/Expose%20Prometheus%20and%20Grafana%20using%20Ingress/","title":"Expose Prometheus and Grafana using Ingress","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Steps expose Prometheus and Grafana using Ingress in our k3d cluster with a custom <code>values.yaml</code>.</p>"},{"location":"Prometheus%20%2B%20Grafana/Expose%20Prometheus%20and%20Grafana%20using%20Ingress/#valuesyaml-save-this-as-kube-prometheus-valuesyaml","title":"\ud83d\udcc4 <code>values.yaml</code> (save this as <code>kube-prometheus-values.yaml</code>):","text":"<pre><code>grafana:\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n    hosts:\n      - grafana.local\n    path: /\n  adminPassword: \"admin\"\n\nprometheus:\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n    hosts:\n      - prometheus.local\n    path: /\n\nalertmanager:\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n    hosts:\n      - alertmanager.local\n    path: /\n</code></pre>"},{"location":"Prometheus%20%2B%20Grafana/Expose%20Prometheus%20and%20Grafana%20using%20Ingress/#setup-steps","title":"\u2699\ufe0f Setup Steps:","text":"<ol> <li>Install ingress controller (if not already):</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/kind/deploy.yaml\n</code></pre> <ol> <li>Wait for ingress controller to be ready:</li> </ol> <pre><code>kubectl get pods -n ingress-nginx -w\n</code></pre> <ol> <li>Install the stack with custom values:</li> </ol> <pre><code>helm install kube-prometheus prometheus-community/kube-prometheus-stack -n monitoring -f kube-prometheus-values.yaml\n</code></pre> <ol> <li>Add local DNS entries:</li> </ol> <p>Add the following lines to your <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 grafana.local prometheus.local alertmanager.local\n</code></pre> <ol> <li> <p>Access UIs:</p> </li> <li> <p>Grafana: http://grafana.local</p> </li> <li>Prometheus: http://prometheus.local</li> <li>Alertmanager: http://alertmanager.local     (Default Grafana login: <code>admin / admin</code>)</li> </ol>"},{"location":"Prometheus%20%2B%20Grafana/Set%20up%20Prometheus%20%2B%20Grafana%20for%20your%20Kubernetes%20K3d%20cluster/","title":"Set up Prometheus + Grafana for your Kubernetes K3d cluster","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here's the most efficient way to set up Prometheus + Grafana for our Kubernetes K3d cluster using <code>kube-prometheus-stack</code> Helm chart.</p>"},{"location":"Prometheus%20%2B%20Grafana/Set%20up%20Prometheus%20%2B%20Grafana%20for%20your%20Kubernetes%20K3d%20cluster/#prerequisites","title":"\u2705 Prerequisites:","text":"<ul> <li>k3d cluster running</li> <li>kubectl configured</li> <li>Helm installed</li> </ul>"},{"location":"Prometheus%20%2B%20Grafana/Set%20up%20Prometheus%20%2B%20Grafana%20for%20your%20Kubernetes%20K3d%20cluster/#setup-steps-one-liner-explanations","title":"\u2699\ufe0f Setup Steps (One-liner explanations):","text":"<ol> <li>Add Helm repo:</li> </ol> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts &amp;&amp; helm repo update\n</code></pre> <ol> <li>Create monitoring namespace:</li> </ol> <pre><code>kubectl create ns monitoring\n</code></pre> <ol> <li>Install kube-prometheus-stack:</li> </ol> <pre><code>helm install kube-prometheus prometheus-community/kube-prometheus-stack -n monitoring\n</code></pre> <ol> <li>Wait for all pods to be ready:</li> </ol> <pre><code>kubectl get pods -n monitoring -w\n</code></pre> <ol> <li>Port-forward Grafana to access UI:</li> </ol> <pre><code>kubectl port-forward svc/kube-prometheus-grafana -n monitoring 3000:80\n</code></pre> <ol> <li>Access Grafana at:</li> </ol> <pre><code>http://localhost:3000\n</code></pre> <ol> <li>Default login:</li> </ol> <pre><code>user: admin\npass: prom-operator\n</code></pre>"},{"location":"Prometheus%20%2B%20Grafana/Set%20up%20Prometheus%20%2B%20Grafana%20for%20your%20Kubernetes%20K3d%20cluster/#notes","title":"\ud83c\udfaf Notes:","text":"<ul> <li>Prometheus, Alertmanager, Grafana are all included.</li> <li>Predefined dashboards + service discovery work out of the box.</li> <li>You can customize values via <code>--set</code> or a <code>values.yaml</code> file if needed.</li> </ul>"},{"location":"RBAC/Kubernetes%20RBAC/","title":"Kubernetes RBAC","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Kubernetes RBAC(Role-Based Access Control) like you're explaining it to your junior.</p>"},{"location":"RBAC/Kubernetes%20RBAC/#imagine-this-simple-setup","title":"\ud83e\udde0 Imagine This Simple Setup","text":"<p>You are the admin of an office building (Kubernetes cluster). There are:</p> <ul> <li>People (users or service accounts)</li> <li>Rooms (resources like pods, configmaps, etc.)</li> <li>Keys (permissions)</li> </ul> <p>You decide:</p> <ul> <li>Who can enter which room</li> <li>What they can do inside</li> </ul>"},{"location":"RBAC/Kubernetes%20RBAC/#rbac-giving-right-keys-to-right-people","title":"\ud83d\udddd\ufe0f RBAC = Giving Right Keys to Right People","text":"<p>RBAC uses 4 main objects:</p> Object Meaning Example Role Defines \"what someone can do\" in one namespace \"Can read pods in <code>dev</code>\" ClusterRole Same as Role, but works across all namespaces \"Can delete pods everywhere\" RoleBinding Gives a Role to a person in a namespace \"Give read-pods Role to dev-user\" ClusterRoleBinding Like RoleBinding but across all namespaces \"Give admin Role to ops-team globally\""},{"location":"RBAC/Kubernetes%20RBAC/#simple-rbac-example","title":"\ud83e\udeaa Simple RBAC Example","text":"<p>You want to allow dev-user to read pods in <code>demo</code> namespace only.</p>"},{"location":"RBAC/Kubernetes%20RBAC/#step-1-create-the-role","title":"\ud83d\udd39 Step 1: Create the Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: read-pods\n  namespace: demo\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>\ud83d\udd0d This says: \u201cWhoever gets this Role can <code>get</code> and <code>list</code> pods in namespace <code>demo</code>.\u201d</p>"},{"location":"RBAC/Kubernetes%20RBAC/#step-2-bind-the-role-to-a-user","title":"\ud83d\udd39 Step 2: Bind the Role to a User","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: bind-read-pods\n  namespace: demo\nsubjects:\n- kind: User\n  name: dev-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: read-pods\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>\ud83d\udd0d This means: \"Give <code>read-pods</code> Role to <code>dev-user</code> in the <code>demo</code> namespace.\"</p>"},{"location":"RBAC/Kubernetes%20RBAC/#final-summary","title":"\ud83d\udc68\u200d\ud83c\udfeb Final Summary","text":"Thing Real-World Analogy Role Job description: \u201cCan read rooms A and B\u201d RoleBinding Giving that job to a person User or ServiceAccount The employee (or robot) doing the job ### \ud83e\uddea Test It <pre><code>kubectl auth can-i list pods --namespace=demo --as=dev-user\n</code></pre> <p>Let me know if you want examples for:</p> <ul> <li>writing permissions</li> <li>admin role</li> <li>service account RBAC</li> </ul> <p>Perfect \u2014 here\u2019s a simple set of examples with RBAC permissions that cover:</p> <ol> <li>\u2705 Read-only access (basic user)</li> <li>\u270d\ufe0f Read + Write access (developer)</li> <li>\ud83d\udee0\ufe0f Full access (admin)</li> <li>\ud83e\udd16 Service Account-based access (for CI/CD pods)</li> </ol>"},{"location":"RBAC/Kubernetes%20RBAC/#1-read-only-access-user","title":"\u2705 1. Read-Only Access (User)","text":""},{"location":"RBAC/Kubernetes%20RBAC/#role-read-only-for-pods","title":"Role (read-only for pods)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: demo\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#rolebinding","title":"RoleBinding","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-reader-binding\n  namespace: demo\nsubjects:\n- kind: User\n  name: dev-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#2-read-write-access-developer","title":"\u270d\ufe0f 2. Read + Write Access (Developer)","text":""},{"location":"RBAC/Kubernetes%20RBAC/#role-manage-pods-deployments","title":"Role (manage pods, deployments)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: dev-role\n  namespace: demo\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#rolebinding_1","title":"RoleBinding","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-role-binding\n  namespace: demo\nsubjects:\n- kind: User\n  name: dev-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#3-admin-access-full-rights-in-all-namespaces","title":"\ud83d\udee0\ufe0f 3. Admin Access (Full rights in all namespaces)","text":""},{"location":"RBAC/Kubernetes%20RBAC/#clusterrolebinding-use-built-in-cluster-admin-role","title":"ClusterRoleBinding (use built-in <code>cluster-admin</code> role)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: give-admin-to-user\nsubjects:\n- kind: User\n  name: admin-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#4-service-account-based-access-for-pods-cicd","title":"\ud83e\udd16 4. Service Account-Based Access (for pods, CI/CD)","text":""},{"location":"RBAC/Kubernetes%20RBAC/#role-read-secrets","title":"Role (read secrets)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: demo\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#rolebinding-to-serviceaccount","title":"RoleBinding to ServiceAccount","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: bind-secret-reader\n  namespace: demo\nsubjects:\n- kind: ServiceAccount\n  name: ci-job\n  namespace: demo\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"RBAC/Kubernetes%20RBAC/#apply-all","title":"\u2705 Apply All:","text":"<pre><code>kubectl apply -f role.yaml\nkubectl apply -f rolebinding.yaml\n</code></pre> <ul> <li>If you want all of them zipped into one YAML</li> <li>If your users are authenticated via OIDC/LDAP or using service accounts only</li> </ul> <p>Great question \u2014 here's a clear and simple explanation of authentication methods in Kubernetes, focusing on:</p> <ul> <li>\u2705 Service Account (default in pods)</li> <li>\ud83d\udd10 OIDC (e.g. Keycloak, Azure AD, Google)</li> <li>\ud83d\udc65 LDAP (via Dex, Keycloak, etc.)</li> </ul>"},{"location":"RBAC/Kubernetes%20RBAC/#1-service-account-authentication-default-in-pods","title":"\u2705 1. Service Account Authentication (default in pods)","text":"<ul> <li>Every pod gets a service account by default.</li> <li>Kubernetes uses JWT tokens from <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code></li> <li>Used for automated tools (CI/CD, controllers, etc.)</li> </ul>"},{"location":"RBAC/Kubernetes%20RBAC/#how-to-use","title":"How to use:","text":"<ol> <li>Create a service account:</li> </ol> <pre><code>kubectl create serviceaccount sa-ci --namespace demo\n</code></pre> <ol> <li>Bind roles to it (RBAC):</li> </ol> <pre><code>kind: RoleBinding\n...\nsubjects:\n- kind: ServiceAccount\n  name: sa-ci\n  namespace: demo\n</code></pre> <ol> <li>In a pod/deployment:</li> </ol> <pre><code>serviceAccountName: sa-ci\n</code></pre> <p>\u2705 Used internally, no login needed manually.</p>"},{"location":"RBAC/Kubernetes%20RBAC/#2-oidc-authentication-users-login-via-google-azure-ad-keycloak-etc","title":"\ud83d\udd10 2. OIDC Authentication (Users login via Google, Azure AD, Keycloak, etc.)","text":""},{"location":"RBAC/Kubernetes%20RBAC/#flow","title":"Flow:","text":"<ol> <li>You connect your Kubernetes API server to an OIDC identity provider.</li> <li>The user logs in via <code>kubectl</code> using <code>oidc</code> tokens.</li> <li>You create RBAC bindings based on their OIDC username or groups.</li> </ol>"},{"location":"RBAC/Kubernetes%20RBAC/#api-server-flags-example","title":"API Server Flags Example:","text":"<pre><code>--oidc-issuer-url=https://accounts.google.com\n--oidc-client-id=k8s-auth\n--oidc-username-claim=email\n--oidc-groups-claim=groups\n</code></pre> <p>Then bind roles like:</p> <pre><code>subjects:\n- kind: User\n  name: gowse@example.com\n</code></pre> <p>\u2705 Best for teams and real-user access.</p>"},{"location":"RBAC/Kubernetes%20RBAC/#3-ldap-authentication-via-dex-or-keycloak","title":"\ud83d\udc65 3. LDAP Authentication (via Dex or Keycloak)","text":"<p>Kubernetes does not natively support LDAP, so you use an identity proxy:</p> Tool Purpose Dex Lightweight OIDC provider that connects to LDAP Keycloak Identity provider that supports OIDC + LDAP Pinniped Modern OIDC provider with LDAP support Auth0 Cloud-based solution that supports LDAP backend"},{"location":"RBAC/Kubernetes%20RBAC/#example-flow","title":"Example Flow:","text":"<ol> <li>Set up Dex or Keycloak to connect to your LDAP</li> <li>Kubernetes API uses OIDC flags to integrate with them (same as above)</li> <li>You assign RBAC based on LDAP users/groups (via OIDC claims)</li> </ol>"},{"location":"RBAC/Kubernetes%20RBAC/#summary-table","title":"\u2705 Summary Table","text":"Method For Needs OIDC? Needs RBAC? Login Type ServiceAccount Pods, automation \u274c \u2705 Auto-attached in pods OIDC Real users \u2705 \u2705 <code>kubectl</code> login with token LDAP Real users (via OIDC proxy) \u2705 (via Dex/Keycloak) \u2705 LDAP login via OIDC token Let me know if: <ul> <li>You want a full OIDC setup with Google/Keycloak</li> <li>You want a working Dex+LDAP example</li> <li>You want to generate tokens manually for testing</li> </ul>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/","title":"Htpasswd   based access with basic auth for k3s cluster","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>To set up user-based access with basic auth (htpasswd) for your K3s cluster, follow this secure and simple approach using the Kubernetes basic-auth plugin via Ingress (e.g., Traefik) or directly via Kubernetes API with RBAC.</p>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#important-k3s-does-not-support-basic-auth-file","title":"\u2757 Important: K3s does not support <code>--basic-auth-file</code>","text":"<p>So you can\u2019t plug in an <code>htpasswd</code> file directly like in upstream <code>kube-apiserver</code>.</p>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#goal","title":"\u2705 Goal:","text":"<ul> <li>Create admin users with username/password</li> <li>Use <code>htpasswd</code> for basic auth</li> <li>Configure RBAC roles in Kubernetes</li> <li>Allow external <code>kubectl</code> access using their credentials</li> </ul>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#step-by-step-setup","title":"\ud83d\udee0\ufe0f Step-by-Step Setup","text":""},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#1-install-htpasswd-tool","title":"\ud83d\udd10 1. Install <code>htpasswd</code> Tool","text":"<p>On your local machine:</p> <pre><code>sudo apt install apache2-utils\n</code></pre>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#2-generate-user-credentials","title":"\ud83e\uddd1\u200d\ud83d\udcbb 2. Generate User Credentials","text":"<pre><code>sudo su # switch to root user and execute\n\nhtpasswd -c ./users.htpasswd admin1\n# Enter password when prompted\n\nhtpasswd ./users.htpasswd admin2\n</code></pre> <p>This creates <code>users.htpasswd</code> like:</p> <pre><code>admin1:$apr1$M9b1Z...     # hashed password\nadmin2:$apr1$Yx9ZL...\n</code></pre>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#3-create-kubernetes-secret","title":"\ud83d\ude80 3. Create Kubernetes Secret","text":"<p>Copy this to <code>cp1</code> if needed:</p> <pre><code>kubectl create secret generic basic-auth \\\n  --from-file=auth=users.htpasswd \\\n  -n kube-system\n</code></pre>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#4-enable-api-server-basic-auth-k3s-only","title":"\ud83d\udd0f 4. Enable API Server Basic Auth (K3s Only)","text":"<p>Edit <code>/etc/systemd/system/k3s.service</code> on all control-plane nodes:</p> <p>Find the <code>ExecStart</code> line, and append:</p> <pre><code>--basic-auth-file /var/lib/rancher/k3s/server/basic-auth.csv\n</code></pre> <p>Now create that CSV:</p> <pre><code>sudo tee /var/lib/rancher/k3s/server/basic-auth.csv &lt;&lt;EOF\nadmin1,$apr1$M9b1Z...,admin1,system:masters\nadmin2,$apr1$Yx9ZL...,admin2,system:masters\nEOF\n</code></pre> <p>Format: <code>username,password,user,group</code></p> <p>Restart K3s:</p> <pre><code>sudo systemctl daemon-reexec\nsudo systemctl restart k3s\n</code></pre> <p>Repeat for cp2 as well.</p>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#5-create-rbac-rolebinding-if-needed","title":"\ud83d\udee1\ufe0f 5. Create RBAC RoleBinding (if needed)","text":"<p>If not using <code>system:masters</code> (cluster admin), you can assign limited roles:</p> <p>\ud83d\udee0\ufe0f Working Alternative \u2013 Static kubeconfig per user <pre><code>sudo cat /etc/rancher/k3s/k3s.yaml &gt; kubeconfig\ncp ~/.kube/config admin1-config\n</code></pre></p> <p>Edit <code>admin1-config</code> like this: <pre><code>users:\n- name: admin1\n  user:\n    username: admin1\n    password: mysecurepassword\n</code></pre></p> <ol> <li>Create a new user kubeconfig </li> </ol> <pre><code># admin-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin1-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: User\n  name: admin1\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code>kubectl apply -f admin-role.yaml\n</code></pre>"},{"location":"RBAC/htpasswd%20-%20based%20access%20with%20basic%20auth%20for%20k3s%20cluster/#6-access-cluster-from-host-using-new-user","title":"\ud83e\uddea 6. Access Cluster from Host (using new user)","text":"<p>On the admin's host machine, create their kubeconfig:</p> <pre><code>cp kubeconfig admin1-config\n</code></pre> <p>Edit <code>admin1-config</code>, replace:</p> <pre><code>users:\n- name: default\n  user:\n    username: admin1\n    password: &lt;the password you set&gt;\n</code></pre> <p>Or dynamically using:</p> <pre><code>kubectl config set-credentials admin1 --username=admin1 --password='yourpass'\nkubectl config set-context admin1-context --user=admin1 --cluster=default\nkubectl config use-context admin1-context\n</code></pre> <p>Then test:</p> <pre><code>KUBECONFIG=admin1-config kubectl get pods --all-namespaces\n</code></pre>"},{"location":"RBAC/htpasswd%20based%20authentication/","title":"Htpasswd based authentication","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes \u2705, you can also use <code>htpasswd</code>-based basic authentication with Kubernetes \u2014 but it is not recommended for production and is deprecated in many setups.</p>"},{"location":"RBAC/htpasswd%20based%20authentication/#how-htpasswd-fits-with-rbac","title":"\ud83d\udd17 How htpasswd fits with RBAC","text":"<ol> <li>htpasswd file is used to authenticate users (who they are)</li> <li>RBAC is used to authorize what those users can do</li> <li>Both are needed:<ul> <li>AuthN (login): via <code>htpasswd</code></li> <li>AuthZ (access): via <code>ClusterRoleBinding</code>, etc.</li> </ul> </li> </ol>"},{"location":"RBAC/htpasswd%20based%20authentication/#important-k3s-does-not-support-basic-auth-file","title":"\u2757 Important: K3s does not support <code>--basic-auth-file</code>","text":"<p>So you can\u2019t plug in an <code>htpasswd</code> file directly like in upstream <code>kube-apiserver</code>.</p>"},{"location":"RBAC/htpasswd%20based%20authentication/#difference-at-a-glance","title":"\ud83d\udd0d Difference at a Glance","text":"Auth Method Description Recommended For OIDC (Keycloak) Secure, token-based, modern \u2705 Production-ready htpasswd Simple, file-based basic auth \ud83d\udeab Dev/Test only ServiceAccounts For pods and automation \u2705 Automation only ### \u2705 Using <code>htpasswd</code> Authentication (Step-by-Step)"},{"location":"RBAC/htpasswd%20based%20authentication/#1-generate-credentials-with-htpasswd","title":"1\ufe0f\u20e3 Generate Credentials with <code>htpasswd</code>","text":"<pre><code>sudo apt install apache2-utils  # if not installed\nhtpasswd -c /etc/kubernetes/htpasswd gowse\n\nhtpasswd -c ./users.htpasswd admin1\n# Enter password when prompted\n\nhtpasswd ./users.htpasswd admin2\n\nThis creates `users.htpasswd` like:\nadmin1:$apr1$M9b1Z...     # hashed password\nadmin2:$apr1$Yx9ZL...\n</code></pre> <p>You\u2019ll be prompted to enter a password. It stores a hashed version in the file.</p>"},{"location":"RBAC/htpasswd%20based%20authentication/#2-create-kubernetes-secret","title":"2\ufe0f\u20e3 Create Kubernetes Secret","text":"<pre><code>kubectl create secret generic basic-auth \\\n  --from-file=auth=users.htpasswd \\\n  -n kube-system\n</code></pre>"},{"location":"RBAC/htpasswd%20based%20authentication/#3-update-kube-apiserver-flags","title":"3 Update kube-apiserver Flags","text":"<p>Edit <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> and add:</p> <pre><code>- --basic-auth-file=/etc/kubernetes/htpasswd\n</code></pre> <p>Also ensure the file is mounted inside the static pod:</p> <pre><code>volumeMounts:\n- mountPath: /etc/kubernetes/htpasswd\n  name: htpasswd\n  readOnly: true\n\nvolumes:\n- name: htpasswd\n  hostPath:\n    path: /etc/kubernetes/htpasswd\n    type: File\n</code></pre> <p>\u2705 This enables basic HTTP authentication using that file.</p>"},{"location":"RBAC/htpasswd%20based%20authentication/#3-set-up-kubectl-access-for-user","title":"3\ufe0f\u20e3 Set Up <code>kubectl</code> Access for User","text":"<pre><code>kubectl config set-credentials gowse \\\n  --username=gowse \\\n  --password=&lt;your-password&gt;\n\nkubectl config set-context test-user \\\n  --cluster=&lt;your-cluster&gt; \\\n  --user=gowse\n\nkubectl config use-context test-user\n</code></pre>"},{"location":"RBAC/htpasswd%20based%20authentication/#4-bind-rbac-role","title":"4\ufe0f\u20e3 Bind RBAC Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: gowse-binding\nsubjects:\n- kind: User\n  name: gowse\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"RBAC/htpasswd%20based%20authentication/#why-htpasswd-is-not-recommended","title":"\ud83d\udeab Why <code>htpasswd</code> is Not Recommended","text":"Issue Explanation \u274c Password sent in base64 Not secure unless over HTTPS \u274c No session/token Every request uses raw user:pass \u274c No expiry, MFA, group claims Limited features \u274c Deprecated in kubeadm Not used in managed clusters (e.g., EKS, GKE)"},{"location":"RBAC/htpasswd%20based%20authentication/#use-it-only-when","title":"\u2705 Use It Only When:","text":"<ul> <li>Doing quick local testing (minikube, dev VMs)</li> <li>You can\u2019t set up Keycloak/OIDC yet</li> <li>You\u2019re experimenting with RBAC concepts</li> </ul>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/","title":"Step By Step  Using Sealed Secrets with GitOps","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#prerequisites","title":"Prerequisites","text":"<ul> <li> <ol> <li>Kubernetes cluster up and running</li> </ol> </li> <li> <ol> <li><code>kubectl</code> configured to access the cluster</li> </ol> </li> <li> <ol> <li><code>kubeseal</code> CLI installed locally (Install guide)</li> </ol> </li> <li> <ol> <li>Git repository for your manifests (GitHub, Gitea, etc.)</li> </ol> </li> <li> <ol> <li>Sealed Secrets controller installed in the cluster</li> </ol> </li> </ul>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-1-install-sealed-secrets-controller","title":"Step 1: Install Sealed Secrets Controller","text":"<p><pre><code>kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/latest/download/controller.yaml\n</code></pre> This installs the controller pod that will decrypt sealed secrets inside the cluster.</p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-2-create-plain-kubernetes-secret-and-configmap-files","title":"Step 2: Create Plain Kubernetes Secret and ConfigMap Files","text":"<p>Create files locally for your app configuration. configmap.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: default\ndata:\n  DB_HOST: \"db.example.com\"\n  DB_PORT: \"5432\"\n</code></pre></p> <p>secret.yaml <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\n  namespace: default\ntype: Opaque\ndata:\n  username: YWRtaW4=       # base64 encoded 'admin'\n  password: cGFzc3dvcmQ=   # base64 encoded 'password'\n</code></pre></p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-3-seal-the-configmap-and-secret-locally","title":"Step 3: Seal the ConfigMap and Secret Locally","text":"<p>Use <code>kubeseal</code> to encrypt your plain secrets before pushing to Git. <pre><code>kubectl create -f configmap.yaml --dry-run=client -o json | kubeseal --format yaml &gt; sealed-configmap.yaml\nkubectl create -f secret.yaml --dry-run=client -o json | kubeseal --format yaml &gt; sealed-secret.yaml\n</code></pre> Now you have sealed-configmap.yaml and sealed-secret.yaml with encrypted data safe for Git.</p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-4-add-sealed-secrets-manifests-to-git-repository","title":"Step 4: Add Sealed Secrets Manifests to Git Repository","text":"<pre><code>git clone git@github.com:youruser/yourrepo.git\ncd yourrepo\n\nmkdir -p k8s/secrets\ncp ../sealed-configmap.yaml k8s/secrets/\ncp ../sealed-secret.yaml k8s/secrets/\n\ngit add k8s/secrets/sealed-*.yaml\ngit commit -m \"Add sealed ConfigMap and Secret for app\"\ngit push origin main\n</code></pre>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-5-deployment-yaml-referring-to-secrets-and-configmaps","title":"Step 5: Deployment YAML Referring to Secrets and ConfigMaps","text":"<p>Create or update your deployment manifest to inject env vars from ConfigMap and Secret.</p> <p>deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: express-app\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: express-app\n  template:\n    metadata:\n      labels:\n        app: express-app\n    spec:\n      containers:\n      - name: express-app\n        image: your-image:latest\n        env:\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: DB_HOST\n        - name: DB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: DB_PORT\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: username\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: app-secret\n              key: password\n</code></pre></p> <p>Add this deployment manifest to Git repo alongside sealed secrets: <pre><code>cp ../deployment.yaml k8s/deployments/\ngit add k8s/deployments/deployment.yaml\ngit commit -m \"Add deployment for express app\"\ngit push origin main\n</code></pre></p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-6-use-gitops-tool-to-sync-repo-to-cluster","title":"Step 6: Use GitOps Tool to Sync Repo to Cluster","text":"<p>Use ArgoCD, Flux, or manual <code>kubectl apply -f</code> on the repo manifests: <pre><code>kubectl apply -f k8s/secrets/sealed-configmap.yaml\nkubectl apply -f k8s/secrets/sealed-secret.yaml\nkubectl apply -f k8s/deployments/deployment.yaml\n</code></pre> The Sealed Secrets controller will automatically decrypt sealed secrets and create normal Kubernetes Secret and ConfigMap objects.</p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#step-7-application-reads-environment-variables","title":"Step 7: Application Reads Environment Variables","text":"<p>Your Node.js app code reads env vars: <pre><code>const config = {\n  host: process.env.DB_HOST || 'localhost',\n  port: process.env.DB_PORT || '5432',\n  user: process.env.DB_USER || 'user',\n  pass: process.env.DB_PASS || 'pass',\n};\n</code></pre></p>"},{"location":"Sealed-Secrets/Step%20By%20Step-%20Using%20Sealed%20Secrets%20with%20GitOps/#summary","title":"Summary","text":"Step Description 1 Install Sealed Secrets controller in cluster 2 Create plain ConfigMap and Secret YAML files 3 Seal files with <code>kubeseal</code> CLI locally 4 Commit sealed secrets YAML to Git repo 5 Create deployment manifest using secrets/config 6 GitOps sync or manual apply to Kubernetes 7 App uses env variables from Kubernetes Secrets"},{"location":"Sealed-Secrets/Steps%20to%20push%20sealed%20secrets%20for%20an%20app/","title":"Steps to push sealed secrets for an app","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># Example: Practical GitOps flow to push sealed secrets for an app config\n\n# 1. Prepare plain secret YAML locally\ncat &gt; secret.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\n  namespace: default\ntype: Opaque\ndata:\n  db_user: YWRtaW4=        # base64 for 'admin'\n  db_password: c2VjdXJl   # base64 for 'secure'\nEOF\n\n# 2. Seal the secret with kubeseal using cluster public key\nkubectl create -f secret.yaml --dry-run=client -o json | kubeseal --format yaml &gt; sealed-secret.yaml\n\n# 3. Initialize Git repo or navigate to your GitOps repo directory\ngit init my-gitops-repo\ncd my-gitops-repo\n\n# 4. Copy sealed secret to the repo folder\ncp ../sealed-secret.yaml ./sealed-secret.yaml\n\n# 5. Commit and push to remote Git repo (GitHub/Gitea)\ngit add sealed-secret.yaml\ngit commit -m \"Add sealed secret for app\"\ngit remote add origin git@github.com:youruser/yourrepo.git\ngit push -u origin main\n\n# 6. Apply manifests from GitOps pipeline or manually trigger a sync (e.g. ArgoCD/Flux)\n# Cluster will automatically decrypt sealed-secret.yaml and create the actual secret.\n</code></pre> <p>Notes:</p> <ul> <li>You run <code>kubeseal</code> locally or in CI before pushing to Git.</li> <li>The Git repo stores only encrypted secrets.</li> <li>The cluster controller decrypts and creates Kubernetes secrets.</li> <li>Use this with GitOps tools like ArgoCD or Flux for automated syncing.</li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/","title":"Deep Dive into securing secrets in specific envs","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Fantastic! Let\u2019s dive deeper into securing secrets in specific environments, with real-world examples, tools, and actionable best practices.  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#1-kubernetes-secrets-management","title":"1. Kubernetes Secrets Management","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-teslas-kubernetes-crypto-mining-breach","title":"\u26a0\ufe0f Real-World Incident: Tesla\u2019s Kubernetes Crypto Mining Breach","text":"<ul> <li>What Happened? Tesla\u2019s Kubernetes dashboard was left unpassworded, allowing attackers to deploy crypto-mining pods.  </li> <li>Root Cause: Misconfigured RBAC + exposed secrets in environment variables.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-kubernetes-secrets","title":"\ud83d\udd12 How to Secure Kubernetes Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-avoid-default-secrets-base64-isnt-encryption","title":"A. Avoid Default Secrets (Base64 Isn\u2019t Encryption!)","text":"<p>\u274c Bad: Storing secrets in plaintext YAML (even if Base64-encoded). \u2705 Fix: Use: - Sealed Secrets (Mozilla SOPS, Kubeseal) - External Secrets Operator (syncs with AWS/Azure/HashiCorp Vault)  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-restrict-access","title":"B. Restrict Access","text":"<ul> <li>RBAC: Limit <code>get</code>/<code>list</code> access to secrets.  </li> <li>Pod Security Policies: Block pods from mounting host secrets.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-dynamic-secrets","title":"C. Dynamic Secrets","text":"<ul> <li>HashiCorp Vault + Kubernetes Auth: Automatically generates short-lived DB credentials.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose Kubeseal Encrypts secrets for GitOps Vault Agent Injects secrets into pods securely Kyverno Policies to block plaintext secrets ## 2. CI/CD Pipeline Secrets ### \u26a0\ufe0f Real-World Incident: CircleCI Breach (2023) - What Happened? Attackers stole CI/CD session tokens, granting access to customer code + secrets. - Root Cause: Engineers stored secrets in plaintext env vars."},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-cicd-secrets","title":"\ud83d\udd12 How to Secure CI/CD Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-store-secrets-in-plaintext","title":"A. Never Store Secrets in Plaintext","text":"<p>\u274c Bad: <pre><code># GitHub Actions (UNSAFE)  \nenv:  \n  AWS_KEY: \"AKIA...\"  \n</code></pre> \u2705 Fix: - Use encrypted secrets (GitHub Secrets, GitLab CI Variables). - For temporary credentials, use OIDC (e.g., AWS IAM Roles for GitHub Actions).  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-limit-secret-access","title":"B. Limit Secret Access","text":"<ul> <li>Scoped Secrets: Restrict by branch/environment (e.g., prod vs. staging).  </li> <li>Audit Logs: Monitor who accesses secrets (e.g., GitLab Audit Events).  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-ephemeral-secrets","title":"C. Ephemeral Secrets","text":"<ul> <li>Vault Dynamic Secrets: Generate a new DB password per pipeline run.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_1","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose GitHub Actions OIDC Temporary AWS creds via IAM Roles HashiCorp Vault Dynamic secrets for CI/CD Argo Workflows Secure secret injection in pipelines ## 3. Cloud Secrets (AWS/Azure/GCP) ### \u26a0\ufe0f Real-World Incident: Uber\u2019s AWS Key Leak (2022) - What Happened? An engineer committed an AWS key to a private repo; attackers found it and breached Uber\u2019s internal systems."},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-cloud-secrets","title":"\ud83d\udd12 How to Secure Cloud Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-use-long-term-keys","title":"A. Never Use Long-Term Keys","text":"<p>\u274c Bad: Hardcoded <code>~/.aws/credentials</code> files. \u2705 Fix: - IAM Roles (for EC2, Lambda, ECS). - AWS Session Manager (SSH-less server access).  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-rotate-secrets-automatically","title":"B. Rotate Secrets Automatically","text":"<ul> <li>AWS Secrets Manager: Auto-rotates RDS passwords.  </li> <li>Vault Dynamic AWS Creds: 1-hour expiry for CLI access.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-monitor-for-leaks","title":"C. Monitor for Leaks","text":"<ul> <li>AWS GuardDuty: Alerts on anomalous key usage.  </li> <li>GitGuardian: Scans GitHub/GitLab for leaked keys.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_2","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose AWS Secrets Manager Auto-rotation + audit logging GCP Secret Manager Versioned secrets for GCP Azure Key Vault Integrates with AAD for RBAC"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#4-developer-workstations-local-secrets","title":"4. Developer Workstations (Local Secrets)","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-codecov-hack-2021","title":"\u26a0\ufe0f Real-World Incident: Codecov Hack (2021)","text":"<ul> <li>What Happened? Attackers modified Codecov\u2019s Bash script to exfiltrate env vars (including secrets) from CI/CD systems.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-local-secrets","title":"\ud83d\udd12 How to Secure Local Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-store-in-plaintext","title":"A. Never Store in Plaintext","text":"<p>\u274c Bad: <pre><code># ~/.bashrc (UNSAFE)  \nexport DB_PASSWORD=\"123456\"  \n</code></pre> \u2705 Fix: - Use 1Password CLI or macOS Keychain. - For dev environments, use dotenv + .gitignore.  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-hardware-backed-secrets","title":"B. Hardware-Backed Secrets","text":"<ul> <li>YubiKey: Store GPG/SSH keys securely.  </li> <li>Temporary SSH Certificates (e.g., Teleport).  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-scan-for-accidental-leaks","title":"C. Scan for Accidental Leaks","text":"<pre><code># Install trufflehog &amp; scan your repo  \npip install trufflehog  \ntrufflehog git https://github.com/your/repo --fail  \n</code></pre>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_3","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose 1Password CLI Secure local secret storage TruffleHog Detect secrets in Git history gopass Team password management ## \ud83d\udd25 Pro Tip: Secret Zero Problem Even with perfect secrets management, how do you bootstrap the first secret? (e.g., the password to unlock Vault?) - Solutions: - Shamir\u2019s Secret Sharing: Split the root key among team leads. - Cloud KMS: AWS KMS/GCP Cloud HSM for auto-unsealing Vault."},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#checklist-immediate-actions","title":"\ud83d\ude80 Checklist: Immediate Actions","text":"<ol> <li>Scan your repos: <code>trufflehog git https://github.com/your/repo</code> </li> <li>Rotate long-lived keys: AWS/GitHub/DB passwords.  </li> <li>Enable MFA for all secrets managers (Vault, AWS, etc.).  </li> <li>Audit who has access: Principle of least privilege.  </li> </ol>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/","title":"Secrets management for sensitive information","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#1-what","title":"1. WHAT","text":"<ul> <li>Definition: Secrets are sensitive pieces of information that grant access to systems, data, or resources (e.g., passwords, API keys, encryption keys, tokens, SSH keys).  </li> <li>Examples:  </li> <li>Database credentials  </li> <li>Cloud service access keys  </li> <li>OAuth tokens  </li> <li>Private certificates  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#2-why-why-are-secrets-important","title":"2. WHY (Why are secrets important?)","text":"<ul> <li>Security Risks:  </li> <li>Unauthorized access if leaked  </li> <li>Data breaches, financial loss, reputational damage  </li> <li>Compliance: Many regulations (GDPR, HIPAA, PCI-DSS) require proper secrets management.  </li> <li>Operational Integrity: Prevents service disruptions due to compromised credentials.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#3-who-who-handles-secrets","title":"3. WHO (Who handles secrets?)","text":"<ul> <li>Developers: Need access for application integration.  </li> <li>DevOps/SREs: Manage secrets in CI/CD pipelines and infrastructure.  </li> <li>Security Teams: Enforce policies, monitor leaks, and rotate keys.  </li> <li>Attackers: Actively seek secrets via code leaks, phishing, or misconfigurations.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#4-how-how-to-manage-secrets-securely","title":"4. HOW (How to manage secrets securely?)","text":"<ul> <li>Best Practices:  </li> <li>Avoid Hardcoding: Never store secrets in plaintext in code/config files.  </li> <li>Use Secrets Managers: Tools like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault.  </li> <li>Environment Variables: Temporary storage (but not foolproof).  </li> <li>Access Controls: Principle of least privilege (restrict who can access secrets).  </li> <li>Rotation: Regularly update secrets to limit exposure.  </li> <li>Audit &amp; Monitoring: Track access and detect anomalies.  </li> <li>Encryption: Ensure secrets are encrypted at rest and in transit.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#additional-considerations","title":"Additional Considerations","text":"<ul> <li>Zero Trust: Assume breaches can happen; verify every access request.  </li> <li>Secret Scanning: Use tools like GitGuardian, TruffleHog to detect leaks in repositories.  </li> </ul> <p>Would you like a deeper dive into any specific aspect (e.g., tools, implementation)?</p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/","title":"Some real world examples of secrets mismanagement","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here are some real-world examples of secrets mismanagement leading to security incidents, along with best-practice solutions:  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#1-github-api-key-leak-unauthorized-cloud-access","title":"1. \ud83d\udea8 GitHub API Key Leak \u2192 Unauthorized Cloud Access**","text":"<p>What Happened? - A developer accidentally committed a <code>.env</code> file containing AWS credentials to a public GitHub repo. - Attackers scanned GitHub, found the keys, and spun up cryptocurrency mining servers, costing the company $50,000+ in cloud bills.  </p> <p>How to Prevent? \u2705 Use Git Secrets/Pre-commit Hooks: Tools like <code>git-secrets</code> block commits containing sensitive data. \u2705 Automated Scanning: GitHub\u2019s built-in secret scanning or GitGuardian alerts on leaked keys. \u2705 Short-Lived Credentials: Use AWS IAM roles instead of long-term access keys.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#2-hardcoded-database-password-data-breach","title":"2. \ud83d\udd11 Hardcoded Database Password \u2192 Data Breach**","text":"<p>What Happened? - A fintech app had a PostgreSQL password hardcoded in its Dockerfile. - Hackers accessed the DB, stealing 300,000+ user records (PII and payment info).  </p> <p>How to Prevent? \u2705 Secrets Manager: Store DB creds in HashiCorp Vault/Azure Key Vault. \u2705 Dynamic Secrets: Tools like Vault generate short-lived DB credentials per request. \u2705 Infra-as-Code Checks: Use Terraform/Snyk to detect hardcoded secrets in configs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#3-exposed-kubernetes-config-cryptojacking-attack","title":"3. \u2601\ufe0f Exposed Kubernetes Config \u2192 Cryptojacking Attack**","text":"<p>What Happened? - A misconfigured Kubernetes cluster had its <code>kubeconfig</code> file (admin access) exposed on a public S3 bucket. - Attackers deployed malicious pods to mine Monero, disrupting production workloads.  </p> <p>How to Prevent? \u2705 RBAC: Limit <code>kubeconfig</code> access with Kubernetes Role-Based Access Control. \u2705 Sealed Secrets: Encrypt Kubernetes secrets using tools like Mozilla SOPS. \u2705 Cloud CSPM: Tools like Prisma Cloud scan for open S3 buckets/misconfigs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#4-mobile-app-api-key-fake-user-impersonation","title":"4. \ud83d\udcf1 Mobile App API Key \u2192 Fake User Impersonation**","text":"<p>What Happened? - A food delivery app embedded its Firebase API key in the client-side code. - Attackers extracted the key, bypassed auth, and placed fake orders charged to legitimate users.  </p> <p>How to Prevent? \u2705 Backend Proxying: Never expose API keys in frontend/mobile apps; route calls via a backend service. \u2705 Restrict Key Usage: Firebase allows domain/IP restrictions for API keys. \u2705 Key Rotation: Automatically rotate keys monthly.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#5-stolen-ssh-key-ransomware-attack","title":"5. \ud83d\udcbe Stolen SSH Key \u2192 Ransomware Attack**","text":"<p>What Happened? - An employee\u2019s SSH private key (without a passphrase) was stolen from their laptop. - Attackers accessed servers, deployed ransomware, and demanded $1M to restore data.  </p> <p>How to Prevent? \u2705 Passphrase-Protect Keys: Always encrypt SSH keys with a password. \u2705 Ephemeral SSH: Use temporary certificates (e.g., Teleport, AWS Session Manager). \u2705 Network Segmentation: Limit SSH access to jump hosts/VPNs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#best-practices-summary","title":"Best Practices Summary","text":"Mistake Solution Tool Example Hardcoded secrets Use secrets managers HashiCorp Vault, AWS Secrets Manager Git leaks Pre-commit hooks + scanning GitGuardian, TruffleHog Long-lived keys Auto-rotation + short-lived creds AWS IAM, Vault dynamic secrets Misconfigured cloud CSPM + infrastructure checks Prisma Cloud, Checkov Exposed API keys Backend proxying + restrictions Firebase, API Gateway <p>Pro Tip: Run <code>trufflehog git https://github.com/your/repo</code> to scan your repos right now for leaked secrets!  </p> <p>Want a deep dive on securing secrets in a specific environment (e.g., Kubernetes, CI/CD)? Let me know! \ud83d\udd12</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/","title":"ServiceAccounts in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#what-are-serviceaccounts-in-kubernetes","title":"What are ServiceAccounts in Kubernetes?","text":"<p>ServiceAccounts are Kubernetes objects that provide an identity for processes running in Pods. They: - Are namespaced resources (exist within a specific namespace) - Are used to authenticate Pods with the Kubernetes API server - Are automatically mounted into Pods at <code>/var/run/secrets/kubernetes.io/serviceaccount</code> - Are different from UserAccounts which are for human users</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#why-use-serviceaccounts","title":"Why use ServiceAccounts?","text":"<p>ServiceAccounts are essential because: 1. Security: They enable least-privilege access control for Pods 2. Authentication: They allow Kubernetes to identify which Pod is making API requests 3. Authorization: They work with RBAC (Role-Based Access Control) to define what actions Pods can perform 4. Automation: They enable secure communication between Pods and the API server without manual credential management</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#when-to-use-serviceaccounts","title":"When to use ServiceAccounts?","text":"<p>You should use ServiceAccounts when: - A Pod needs to interact with the Kubernetes API - Different Pods need different levels of access to cluster resources - You need to restrict what certain workloads can do in your cluster - You're implementing CI/CD pipelines that interact with Kubernetes - You're running custom controllers or operators</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#how-to-use-serviceaccounts","title":"How to use ServiceAccounts?","text":""},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#basic-usage","title":"Basic Usage","text":"<ol> <li> <p>Default ServiceAccount: Every namespace has a default ServiceAccount automatically used by Pods    <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mycontainer\n    image: myimage\n  # Automatically uses the 'default' ServiceAccount\n</code></pre></p> </li> <li> <p>Creating a custom ServiceAccount <pre><code>kubectl create serviceaccount my-sa\n</code></pre></p> </li> <li> <p>Assigning to a Pod <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  serviceAccountName: my-sa  # custom ServiceAccount\n  containers:\n  - name: mycontainer\n    image: myimage\n</code></pre></p> </li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#advanced-usage","title":"Advanced Usage","text":"<ol> <li> <p>Creating RBAC bindings (ClusterRole and RoleBinding)    <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> </li> <li> <p>Using in Deployments <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 3\n  template:\n    spec:\n      serviceAccountName: my-sa\n      containers:\n      - name: mycontainer\n        image: myimage\n</code></pre></p> </li> <li> <p>Inspecting the token (from inside a Pod)    <pre><code># Inside a Pod:\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre></p> </li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Avoid using the default ServiceAccount for production workloads</li> <li>Create dedicated ServiceAccounts for different types of workloads</li> <li>Follow the principle of least privilege when assigning permissions</li> <li>Regularly audit ServiceAccount permissions</li> <li>Consider using <code>automountServiceAccountToken: false</code> for Pods that don't need API access</li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If a Pod can't access the API, check:</li> <li>Does the ServiceAccount exist?</li> <li>Are the RBAC permissions correct?</li> <li>Is the token properly mounted?</li> <li>Use <code>kubectl auth can-i</code> commands to verify permissions</li> </ul>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/","title":"Headless Services in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#headless-services-in-kubernetes-a-comprehensive-guide","title":"Headless Services in Kubernetes: A Comprehensive Guide","text":""},{"location":"Services/Headless%20Services%20in%20Kubernetes/#what-is-a-headless-service","title":"What is a Headless Service?","text":"<p>A headless service is a Kubernetes service that ==doesn't provide load balancing or a single IP address==. Instead, it returns the IP addresses of all pods matching the selector, allowing direct connections to individual pods.</p> <p><code>Note</code>: ==Not necessarily. While StatefulSets and Headless Services are commonly used together, they aren't strictly dependent on each other. Let me clarify when and why you might use a Headless Service with different types of workloads.==</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#why-use-a-headless-service","title":"Why Use a Headless Service?","text":"<p>You'd use a headless service when: - You need direct pod-to-pod communication - Your application handles load balancing itself - You're deploying stateful applications where each pod has unique network identity - You need DNS records for all pods (e.g., for databases with replication)</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#when-to-use-a-headless-service","title":"When to Use a Headless Service?","text":"<p>Common use cases: - StatefulSets (like databases: MongoDB, PostgreSQL, etc.) - Peer-to-peer applications - When pods need to discover and communicate with each other directly - Applications that require stable network identities</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#how-to-use-a-headless-service-with-example","title":"How to Use a Headless Service (with Example)","text":""},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-mongodb-replica-set","title":"Example: MongoDB Replica Set","text":"<p>Here's how to create a headless service for a MongoDB replica set:</p> <ol> <li>Create the Headless Service:</li> </ol> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\nspec:\n  clusterIP: None  # This makes it headless\n  ports:\n  - port: 27017\n    name: mongodb\n  selector:\n    app: mongodb\n</code></pre> <ol> <li>Create the StatefulSet:</li> </ol> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongodb\nspec:\n  serviceName: mongodb  # Matches the headless service name\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.4\n        ports:\n        - containerPort: 27017\n        command: [\"mongod\", \"--replSet\", \"rs0\", \"--bind_ip_all\"]\n</code></pre>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#how-it-works","title":"How It Works","text":"<ol> <li>When you create this setup:</li> <li>Kubernetes creates 3 MongoDB pods with names: mongodb-0, mongodb-1, mongodb-2</li> <li> <p>The headless service creates DNS entries for each pod:</p> <ul> <li><code>mongodb-0.mongodb.default.svc.cluster.local</code></li> <li><code>mongodb-1.mongodb.default.svc.cluster.local</code></li> <li><code>mongodb-2.mongodb.default.svc.cluster.local</code></li> </ul> </li> <li> <p>Inside the MongoDB configuration, you can initialize the replica set using these DNS names because they're stable (won't change even if pods restart).</p> </li> <li> <p>Applications can:</p> </li> <li>Connect to individual pods directly using their DNS names</li> <li>Discover all pods by querying the DNS (which returns all pod IPs)</li> </ol>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#verifying-the-dns","title":"Verifying the DNS","text":"<p>You can check the DNS records from another pod:</p> <pre><code># Run this in another pod in the same namespace\nnslookup mongodb\n</code></pre> <p>This will return all pod IPs rather than a single service IP.</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#key-differences-from-regular-services","title":"Key Differences from Regular Services","text":"Feature Regular Service Headless Service ClusterIP Has an IP None DNS Resolution Returns service IP Returns all pod IPs Load Balancing Yes No Use Case Stateless applications Stateful applications <p>Headless services give you more control over networking when you need direct access to pods rather than going through a service proxy.</p> <p>==Not necessarily. While StatefulSets and Headless Services are commonly used together, they aren't strictly dependent on each other. Let me clarify when and why you might use a Headless Service with different types of workloads.==</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#when-to-use-headless-services-with-statefulsets-vs-deployments","title":"When to Use Headless Services with StatefulSets vs Deployments","text":"Scenario Recommended Workload Why? Stateful applications (e.g., databases like MongoDB, Cassandra, PostgreSQL) StatefulSet + Headless Service StatefulSets provide stable pod names (<code>pod-0</code>, <code>pod-1</code>, etc.) and persistent storage. Headless Services allow direct pod-to-pod communication with stable DNS (<code>pod-0.svc.cluster.local</code>). Peer-to-peer or clustered apps (e.g., Kafka, Elasticsearch, RabbitMQ clusters) StatefulSet + Headless Service Each pod needs to discover others via DNS. StatefulSets ensure ordered scaling and stable identities. Direct pod access without load balancing (e.g., custom service discovery, gRPC with client-side LB) Deployment + Headless Service If you don\u2019t need stable pod identities but still want DNS records for all pods, a Deployment with a Headless Service works. Stateless apps needing pod DNS (e.g., monitoring agents that scrape all pods) Deployment + Headless Service A Headless Service lets you get all pod IPs via DNS, even if the app itself is stateless."},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-1-headless-service-with-statefulset-mongodb","title":"Example 1: Headless Service with StatefulSet (MongoDB)","text":"<p>This is the most common pairing: <pre><code># Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\nspec:\n  clusterIP: None  # Headless\n  ports:\n  - port: 27017\n  selector:\n    app: mongodb\n---\n# StatefulSet\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongodb\nspec:\n  serviceName: mongodb  # Matches the Headless Service\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo\n        command: [\"mongod\", \"--replSet\", \"rs0\"]\n</code></pre> Why? - Each MongoDB replica (<code>mongodb-0</code>, <code>mongodb-1</code>, etc.) gets a stable DNS name (<code>mongodb-0.mongodb.ns.svc.cluster.local</code>). - The replica set can be initialized using these DNS names.  </p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-2-headless-service-with-deployment-stateless-grpc-service","title":"Example 2: Headless Service with Deployment (Stateless gRPC Service)","text":"<p>You can use a Headless Service even with a Deployment if you need direct pod access: <pre><code># Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: grpc-service\nspec:\n  clusterIP: None  # Headless\n  ports:\n  - port: 50051\n  selector:\n    app: grpc-server\n---\n# Deployment (stateless)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grpc-server\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: grpc-server\n    spec:\n      containers:\n      - name: grpc-server\n        image: my-grpc-server\n</code></pre> Why? - Clients can discover all pods via DNS (<code>grpc-service.ns.svc.cluster.local</code> returns all pod IPs). - Useful for client-side load balancing (e.g., gRPC uses round-robin DNS).  </p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>StatefulSets + Headless Services are best for:</li> <li>Stateful apps (databases, queues).</li> <li>Pods that need stable DNS names (<code>pod-0.svc</code>, <code>pod-1.svc</code>).  </li> <li> <p>Ordered scaling (e.g., <code>pod-0</code> must start before <code>pod-1</code>).  </p> </li> <li> <p>Deployments + Headless Services work when:</p> </li> <li>You don\u2019t need stable pod identities but still want direct pod DNS resolution.  </li> <li> <p>Your app handles discovery/load balancing itself (e.g., gRPC, custom service mesh).  </p> </li> <li> <p>Avoid Headless Services if:</p> </li> <li>You just need a simple load-balanced service (use a normal <code>ClusterIP</code> instead).  </li> <li>Your app doesn\u2019t care about individual pod IPs.  </li> </ol> <p>Would you like a deeper dive into any specific scenario?</p>"},{"location":"Services/Types%20of%20Kubernetes%20Services/","title":"Types of Kubernetes Services","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Kubernetes offers several service types to expose your applications in different ways. Here's a comprehensive breakdown:</p>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#1-clusterip-default-service-type","title":"1. ClusterIP (Default Service Type)","text":"<ul> <li>Purpose: Exposes the service on an internal IP within the cluster</li> <li>Use Case: Communication between microservices within the cluster</li> <li>Characteristics:</li> <li>Only accessible from inside the cluster</li> <li>Provides load balancing across pods</li> <li>Default service type if none specified</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\nspec:\n  type: ClusterIP\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#2-nodeport","title":"2. NodePort","text":"<ul> <li>Purpose: Exposes the service on each Node's IP at a static port</li> <li>Use Case: When you need external access to your service</li> <li>Characteristics:</li> <li>Makes service accessible from outside the cluster</li> <li>Opens a specific port on all nodes</li> <li>Automatically creates a ClusterIP service to route to it</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 9376\n      nodePort: 30007  # Optional (default range: 30000-32767)\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#3-loadbalancer","title":"3. LoadBalancer","text":"<ul> <li>Purpose: Exposes the service externally using a cloud provider's load balancer</li> <li>Use Case: Production applications needing external access</li> <li>Characteristics:</li> <li>Creates an external load balancer</li> <li>Automatically creates NodePort and ClusterIP services</li> <li>Cloud provider specific (AWS ALB, GCP LB, Azure LB)</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#4-externalname","title":"4. ExternalName","text":"<ul> <li>Purpose: Maps a service to a DNS name</li> <li>Use Case: Accessing external services from within the cluster</li> <li>Characteristics:</li> <li>No selectors or endpoints</li> <li>Returns CNAME record of external service</li> <li>Useful for services outside Kubernetes</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#5-headless-service-special-case","title":"5. Headless Service (Special Case)","text":"<ul> <li>Purpose: For direct pod-to-pod communication without load balancing</li> <li>Use Case: Stateful applications, databases, peer discovery</li> <li>Characteristics:</li> <li>clusterIP: None</li> <li>Returns DNS A records for all matching pods</li> <li>No single service IP/proxy</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-headless-service\nspec:\n  clusterIP: None\n  selector:\n    app: my-stateful-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#comparison-table","title":"Comparison Table","text":"Type Cluster IP External Access Load Balancing DNS Resolution Use Case ClusterIP Yes No Yes Single IP Internal services NodePort Yes Via Node IP Yes Single IP Development/testing LoadBalancer Yes Yes Yes Single IP Production external access ExternalName No N/A No CNAME External services Headless None Depends No All Pod IPs Stateful apps, direct pod access"},{"location":"Services/Types%20of%20Kubernetes%20Services/#advanced-service-types-cloud-specific","title":"Advanced Service Types (Cloud-Specific)","text":"<p>Some cloud providers offer additional service types:</p> <ol> <li>Ingress: Not technically a service type but works with services to provide HTTP routing</li> <li>External IPs: Additional external IPs for services</li> <li>AWS NLB: AWS-specific Network Load Balancer</li> <li>GCP Internal LB: For internal load balancing in GCP</li> </ol>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#choosing-the-right-service-type","title":"Choosing the Right Service Type","text":"<ol> <li>Internal cluster communication: ClusterIP</li> <li>Development/testing external access: NodePort</li> <li>Production external access: LoadBalancer</li> <li>Stateful applications: Headless Service</li> <li>External services: ExternalName</li> </ol> <p>Would you like me to elaborate on any specific service type or provide more detailed examples?</p>"},{"location":"Software/Best%20practice%20to%20follow%20-%20software%20versioning/","title":"Best practice to follow   software versioning","text":"<p>When pushing software to GitHub (or any version control system), it's generally recommended to avoid using certain version numbers as the base version to prevent confusion or follow common conventions. Here are some guidelines:</p>"},{"location":"Software/Best%20practice%20to%20follow%20-%20software%20versioning/#versions-to-avoid-as-base-initial-versions","title":"Versions to Avoid as Base (Initial) Versions:","text":"<ol> <li><code>v0.0.0</code> </li> <li>Too ambiguous; suggests the project is not even in a pre-release state.</li> <li> <p>Often used as a placeholder in dependency systems.</p> </li> <li> <p><code>v1.0.0</code> (unless the software is truly stable) </p> </li> <li><code>1.0.0</code> typically signifies a stable, production-ready release.  </li> <li> <p>If your project is still experimental, starting with <code>0.x.x</code> (e.g., <code>0.1.0</code>) is better.</p> </li> <li> <p>Unconventional or Joke Versions (e.g., <code>v999.0.0</code>, <code>v0.0.1-alpha-snapshot</code>) </p> </li> <li>Can cause issues with dependency managers (npm, pip, Maven, etc.).  </li> <li> <p>Makes the project look unprofessional.</p> </li> <li> <p><code>v0.0.1</code> (Too Minimal) </p> </li> <li>While technically valid, it doesn\u2019t convey meaningful progress.  </li> <li>Better to start with <code>v0.1.0</code> (first pre-release with basic features).</li> </ol>"},{"location":"Software/Best%20practice%20to%20follow%20-%20software%20versioning/#recommended-starting-versions","title":"Recommended Starting Versions:","text":"Scenario Recommended Starting Version Experimental / Early Development <code>v0.1.0</code> (first testable release) Stable / Production-ready from Day 1 (rare) <code>v1.0.0</code> Following Semantic Versioning (SemVer) <code>v0.1.0</code> \u2192 <code>v1.0.0</code> when API stabilizes"},{"location":"Software/Best%20practice%20to%20follow%20-%20software%20versioning/#best-practices","title":"Best Practices:","text":"<ul> <li>Use Semantic Versioning (SemVer) (<code>MAJOR.MINOR.PATCH</code>).  </li> <li>For pre-releases, use suffixes like <code>-alpha</code>, <code>-beta</code>, or <code>-rc</code> (e.g., <code>v1.0.0-beta.1</code>).  </li> <li>Avoid skipping versions (e.g., jumping from <code>v0.1.0</code> to <code>v2.0.0</code> without reason).  </li> </ul> <p>Would you like recommendations for a specific programming language or package manager (npm, pip, etc.)?</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/","title":"Updating a MongoDB StatefulSet in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Updating a MongoDB StatefulSet in Kubernetes requires careful handling since StatefulSets manage stateful applications with persistent data. Below is a step-by-step guide on how to safely apply changes (e.g., configuration, image version, or resource limits) without causing downtime or data corruption.</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#steps-to-update-a-mongodb-statefulset","title":"\ud83d\udd27 Steps to Update a MongoDB StatefulSet","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#1-identify-the-change-needed","title":"1\ufe0f\u20e3 Identify the Change Needed","text":"<p>Possible changes: - Configuration (e.g., <code>mongod.conf</code> settings) - Docker Image Version (e.g., <code>mongo:4.4</code> \u2192 <code>mongo:5.0</code>) - Resource Limits (CPU/Memory) - Replica Count (Scaling up/down)</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#2-update-strategy-options","title":"2\ufe0f\u20e3 Update Strategy Options","text":"<p>StatefulSets support two update strategies:</p> Strategy Behavior Best For RollingUpdate (Default) Updates Pods one by one (in reverse order: <code>mongo-2</code> \u2192 <code>mongo-1</code> \u2192 <code>mongo-0</code>) Minor changes (e.g., configs, non-breaking image updates) OnDelete Requires manual deletion of Pods for updates Major changes (e.g., MongoDB major version upgrade)"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#3-example-updating-mongodb-configuration","title":"3\ufe0f\u20e3 Example: Updating MongoDB Configuration","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#step-1-modify-the-statefulset-yaml","title":"Step 1: Modify the StatefulSet YAML","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  updateStrategy:\n    type: RollingUpdate  # Default (Pod-by-Pod replacement)\n  template:\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:5.0  # Updated image\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          value: admin\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          value: password\n        volumeMounts:\n        - name: mongo-config\n          mountPath: /etc/mongod.conf  # Custom config file\n  volumeClaimTemplates: [...]\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#step-2-apply-the-changes","title":"Step 2: Apply the Changes","text":"<pre><code>kubectl apply -f mongo-statefulset.yaml\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#what-happens","title":"What Happens?","text":"<ul> <li>Kubernetes replaces Pods one by one (starting from the highest ordinal, <code>mongo-2</code> \u2192 <code>mongo-1</code> \u2192 <code>mongo-0</code>).</li> <li>Each Pod retains its PVC (data is preserved).</li> <li>The MongoDB replica set auto-reconfigures (if properly set up).</li> </ul>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#4-example-major-version-upgrade-mongodb-44-50","title":"4\ufe0f\u20e3 Example: Major Version Upgrade (MongoDB 4.4 \u2192 5.0)","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#warning-requires-manual-intervention","title":"\u26a0\ufe0f Warning: Requires Manual Intervention","text":"<ol> <li>Set <code>updateStrategy: OnDelete</code> <pre><code>spec:\n  updateStrategy:\n    type: OnDelete  # Prevents automatic updates\n</code></pre></li> <li>Apply the new YAML <pre><code>kubectl apply -f mongo-statefulset.yaml\n</code></pre></li> <li>Manually Delete Pods (One at a Time) <pre><code>kubectl delete pod mongo-2  # Starts with the highest ordinal\n</code></pre></li> <li>Wait for <code>mongo-2</code> to rejoin the replica set before deleting the next Pod.</li> </ol>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#5-verifying-the-update","title":"5\ufe0f\u20e3 Verifying the Update","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-pod-status","title":"Check Pod Status","text":"<pre><code>kubectl get pods -l app=mongo\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-mongodb-version","title":"Check MongoDB Version","text":"<pre><code>kubectl exec mongo-0 -- mongosh --eval \"db.version()\"\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-replica-set-health","title":"Check Replica Set Health","text":"<pre><code>kubectl exec mongo-0 -- mongosh --eval \"rs.status()\"\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#best-practices-for-updating-statefulsets","title":"\u2705 Best Practices for Updating StatefulSets","text":"<ol> <li>Backup Data Before Major Changes <pre><code>kubectl exec mongo-0 -- mongodump --out /backup\n</code></pre></li> <li>Test Changes in Staging First </li> <li>Use <code>RollingUpdate</code> for Minor Changes </li> <li>Use <code>OnDelete</code> for Major Upgrades </li> <li>Monitor Sync Status After Updates </li> </ol>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<ul> <li>\u26a0\ufe0f Breaking Replication: Changing auth credentials without updating all Pods can break sync.</li> <li>\u26a0\ufe0f Downtime Risk: If <code>mongo-0</code> (primary) restarts, a new leader election happens.</li> <li>\u26a0\ufe0f Storage Issues: Changing <code>volumeClaimTemplates</code> does not modify existing PVCs.</li> </ul>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#final-answer","title":"Final Answer","text":"<ul> <li>For minor changes (configs, patches): Use <code>RollingUpdate</code>.  </li> <li>For major upgrades (MongoDB 4 \u2192 5): Use <code>OnDelete</code> + manual Pod deletion.  </li> <li>Always verify replica set health after updates.  </li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/","title":"W3H Understanding StatefulSets in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#what-are-statefulsets","title":"What are StatefulSets?","text":"<p>StatefulSets are a Kubernetes workload API object used to manage stateful applications. They provide guarantees about the ordering and uniqueness of Pods, which is essential for applications that require:</p> <ul> <li>Stable, unique network identifiers</li> <li>Stable, persistent storage</li> <li>Ordered, graceful deployment and scaling</li> <li>Ordered, automated rolling updates</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#why-use-statefulsets","title":"Why Use StatefulSets?","text":"<p>You'd use StatefulSets when you need to run stateful applications in Kubernetes that require:</p> <ol> <li>Stable identities: Each Pod gets a persistent hostname that stays the same even after rescheduling</li> <li>Persistent storage: Volumes are created for each Pod and persist across rescheduling</li> <li>Ordered operations: Pods are created, updated, and deleted in a predictable order (ordinal index)</li> </ol>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#when-to-use-statefulsets","title":"When to Use StatefulSets?","text":"<p>Use StatefulSets for applications like: - Databases (MongoDB, PostgreSQL, MySQL clusters) - Message queues (Kafka, RabbitMQ) - Any application that requires stable network identity or persistent storage - Distributed systems that require ordered deployment/scaling</p>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#how-statefulsets-work-key-concepts","title":"How StatefulSets Work: Key Concepts","text":""},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#1-pod-identity","title":"1. Pod Identity","text":"<ul> <li>Each Pod gets a stable name: <code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code></li> <li>Example: <code>web-0</code>, <code>web-1</code>, <code>web-2</code> for a StatefulSet named \"web\"</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#2-persistent-storage","title":"2. Persistent Storage","text":"<ul> <li>Uses PersistentVolumeClaims (PVCs) that follow the Pod</li> <li>Each Pod gets its own PVC(s) bound to its ordinal index</li> <li>Storage persists even if the Pod is rescheduled</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#3-ordered-operations","title":"3. Ordered Operations","text":"<ul> <li>Scaling up: Pods are created sequentially (0, 1, 2...)</li> <li>Scaling down: Pods are terminated in reverse order (...2, 1, 0)</li> <li>Updates follow the same ordering</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#how-to-create-a-statefulset","title":"How to Create a StatefulSet","text":"<p>Here's a basic example for a MongoDB StatefulSet: <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  serviceName: \"mongo\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4.2\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-persistent-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre></p>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#key-components","title":"Key Components:","text":"<ul> <li><code>serviceName</code>: Headless Service that controls the network domain</li> <li><code>volumeClaimTemplates</code>: Defines PVCs that will be created for each Pod</li> <li>Ordered creation of Pods with stable names</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#statefulset-vs-deployment","title":"StatefulSet vs Deployment","text":"Feature Deployment StatefulSet Pod names Random hash Ordered (web-0, web-1) Storage Shared volumes Dedicated per Pod Scaling Any order Ordered Network identity Not stable Stable DNS name Use cases Stateless apps Stateful apps"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Always create a headless Service for your StatefulSet</li> <li>Design your application to handle the ordered operations</li> <li>Consider using Pod Management Policies (<code>parallel</code> or <code>ordered</code>) based on your needs</li> <li>Implement proper backup strategies for your persistent data</li> <li>Use readiness probes to ensure proper startup ordering</li> </ol> <p>StatefulSets bring the benefits of Kubernetes to stateful applications while providing the stability and predictability these applications require.</p>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#1-statefulsets-vs-deployments","title":"1. StatefulSets vs Deployments","text":"Feature Deployment (Stateless) StatefulSet (Stateful) Pod Naming Random (<code>web-7dfd6c8f5</code>) Ordered (<code>web-0</code>, <code>web-1</code>) Storage Shared volumes (if any) Dedicated PVC per Pod Scaling Order Parallel (no order) Sequential (0 \u2192 1 \u2192 2) Deletion Order Random Reverse order (2 \u2192 1 \u2192 0) Network Identity Unstable (changes on restart) Stable DNS (<code>web-0.mongo.default.svc.cluster.local</code>) Use Cases Stateless apps (frontend, APIs) Databases (MongoDB, MySQL), Message Queues (Kafka) Service Required Regular ClusterIP/LoadBalancer Headless Service (for stable DNS) ### 2. When to Use StatefulSets? Scenario Reason Databases (MySQL, MongoDB, PostgreSQL) Needs persistent storage &amp; stable hostnames Message Brokers (Kafka, RabbitMQ) Requires ordered scaling &amp; stable identities Distributed Systems with Leader Election Pods need to recognize each other via DNS Applications with Persistent Data Data must survive Pod restarts ### 3. Key Features of StatefulSets Feature Description Stable Pod Names Pods are named <code>&lt;statefulset&gt;-&lt;ordinal&gt;</code> (e.g., <code>mongo-0</code>, <code>mongo-1</code>) Persistent Volumes (PVCs) Each Pod gets its own storage (<code>volumeClaimTemplates</code>) Ordered Operations Pods start/stop in sequence (<code>0 \u2192 1 \u2192 2</code>) Stable Network IDs Each Pod gets a DNS record (<code>&lt;pod&gt;.&lt;svc&gt;.&lt;namespace&gt;.svc.cluster.local</code>) Graceful Scaling Scaling down removes the highest ordinal first ### 4. Example StatefulSet YAML Breakdown <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  serviceName: \"mongo\"  # Requires a Headless Service\n  replicas: 3           # Creates mongo-0, mongo-1, mongo-2\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4.2\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-storage\n          mountPath: /data/db\n  volumeClaimTemplates:  # Creates PVCs for each Pod\n  - metadata:\n      name: mongo-storage\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#summary-table-statefulset-key-points","title":"Summary Table: StatefulSet Key Points","text":"Aspect StatefulSet Behavior Pod Identity Fixed hostnames (<code>&lt;name&gt;-0</code>, <code>&lt;name&gt;-1</code>) Storage Each Pod gets its own PVC Scaling Up Sequential (<code>0 \u2192 1 \u2192 2</code>) Scaling Down Reverse order (<code>2 \u2192 1 \u2192 0</code>) Updates Ordered rolling updates DNS Records Stable (<code>&lt;pod&gt;.&lt;service&gt;.ns.svc.cluster.local</code>)"},{"location":"Storage/About%20StorageClass%20fields/","title":"About StorageClass fields","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Field What it Does Common Values reclaimPolicy Tells Kubernetes what to do with the actual disk when you delete a PVC \u2022 <code>Delete</code> \u2013 also deletes the PV and its data\u2022 <code>Retain</code> \u2013 keeps the PV and data for manual cleanup volumeBindingMode Controls when Kubernetes assigns a PV to a PVC \u2022 <code>Immediate</code> \u2013 binds as soon as PVC is created\u2022 <code>WaitForFirstConsumer</code> \u2013 waits until a pod uses the PVC, so PV lands on the right node allowVolumeExpansion Lets you grow the size of a volume after the PVC is already created \u2022 <code>true</code> \u2013 you can increase PVC\u2019s storage size\u2022 <code>false</code> \u2013 size is fixed once PVC is made - Use <code>Delete</code> reclaimPolicy if you want cleanup to happen automatically. - Use <code>WaitForFirstConsumer</code> bindingMode for StatefulSets or multi-zone clusters so the volume shows up on the pod\u2019s node. - Set allowVolumeExpansion to <code>true</code> if you expect to need more space later. <p>\u2705 Yes, you can attach and use multiple StorageClasses in your K3s cluster.</p> <p>Kubernetes (and K3s) fully support multiple StorageClasses, and you can pick the right one per PVC using the <code>storageClassName</code> field.</p>"},{"location":"Storage/About%20StorageClass%20fields/#how-it-works","title":"\ud83d\udd27 How It Works","text":"<ul> <li>You can install multiple CSI drivers (like Longhorn, NFS, Ceph, etc.).</li> <li>Each driver typically registers its own StorageClass.</li> <li>You choose which StorageClass to use per PVC.</li> </ul>"},{"location":"Storage/About%20StorageClass%20fields/#example-using-multiple-storageclasses","title":"\ud83e\udde0 Example: Using Multiple StorageClasses","text":""},{"location":"Storage/About%20StorageClass%20fields/#1-list-all-storageclasses","title":"1. List all StorageClasses","text":"<pre><code>kubectl get storageclass\n</code></pre> <p>Example output: <pre><code>NAME             PROVISIONER             DEFAULT\nlocal-path       rancher.io/local-path   yes\nlonghorn         driver.longhorn.io      no\nnfs-client       nfs.csi.k8s.io          no\n</code></pre></p>"},{"location":"Storage/About%20StorageClass%20fields/#2-use-specific-storageclass-in-your-pvc","title":"2. Use Specific StorageClass in Your PVC","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: longhorn\n</code></pre> <p>You can change <code>storageClassName</code> to <code>nfs-client</code>, <code>local-path</code>, etc.</p>"},{"location":"Storage/About%20StorageClass%20fields/#how-to-add-a-new-storageclass","title":"\u2699\ufe0f How to Add a New StorageClass","text":"Backend Install Guide Longhorn <code>helm install</code> or official YAML NFS Install NFS CSI driver and create StorageClass Ceph/Rook Deploy Rook operator and Ceph cluster HostPath (custom) Manually create PVs + StorageClass iSCSI, OpenEBS, etc. Install respective CSI drivers ## \ud83d\udccc Optional: Change the Default StorageClass <pre><code># Set Longhorn as default\nkubectl patch storageclass longhorn -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n# Remove default from local-path\nkubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n</code></pre>"},{"location":"Storage/About%20StorageClass%20fields/#best-use-case-examples","title":"\ud83e\udde0 Best Use Case Examples","text":"App Recommended StorageClass MySQL / PostgreSQL <code>longhorn</code> (replication, snapshot) Static website <code>nfs-client</code> (read-only shared) Temp processing <code>local-path</code> (fast, ephemeral) Mostly the same steps apply to Kubernetes (k8s) as well, but there are a few key differences between K3s and standard K8s you need to be aware of when working with StorageClasses and CSI drivers."},{"location":"Storage/About%20StorageClass%20fields/#whats-the-same-k3s-vs-k8s","title":"\u2705 What\u2019s the Same (K3s vs. K8s)","text":"Task Same in K3s &amp; K8s? Notes Multiple StorageClasses \u2705 Use <code>storageClassName</code> in PVC Setting default class \u2705 Use patch on annotation Installing CSI drivers (Longhorn, NFS, etc.) \u2705 Helm/YAML-based, works in both Creating/Binding PVCs \u2705 Same syntax in YAML Velero + CSI Snapshots \u2705 Requires CSI plugin + VolumeSnapshotClass ## \u26a0\ufe0f What\u2019s Different (K3s vs. Full K8s) Feature K3s K8s Default StorageClass <code>local-path</code> Often none (or GCE/AWS default) Pre-installed CSI drivers <code>local-path-provisioner</code> only None (must install manually) Kubelet root path <code>/var/lib/rancher/k3s</code> <code>/var/lib/kubelet</code> CRDs (CSI snapshot, etc.) Might not be present Needs to be installed manually too Component Management Bundled into single binary Modular (API, controller, etc.)"},{"location":"Storage/About%20StorageClass%20fields/#example-differences","title":"\ud83e\uddf1 Example Differences","text":""},{"location":"Storage/About%20StorageClass%20fields/#in-k3s","title":"\ud83d\udce6 In K3s:","text":"<ul> <li>You already have a <code>local-path</code> StorageClass</li> <li>You might already have embedded controller support</li> <li>Easier to deploy Longhorn with one Helm chart</li> </ul>"},{"location":"Storage/About%20StorageClass%20fields/#in-k8s-vanilla","title":"\ud83d\udce6 In K8s (Vanilla):","text":"<ul> <li>You must install a default StorageClass yourself</li> <li>Some CRDs (like <code>volumesnapshotclass</code>) might be missing</li> <li>CSI drivers require RBAC, webhook, snapshot-controller setup</li> </ul>"},{"location":"Storage/About%20StorageClass%20fields/#installation-nuances","title":"\ud83e\uddf0 Installation Nuances","text":"CSI Driver K3s K8s Longhorn \u2705 Helm with built-in defaults \u2705 Helm, but ensure <code>defaultStorageClass</code> is manually set NFS \u2705 Easy if NFS server reachable \u2705 Same Ceph/Rook \u2705 Works but heavy \u2705 Designed for full K8s clusters Velero \u2705 Works out of the box if CSI plugin used \u2705 Same, but might need manual snapshot-controller install ## \ud83d\udd1a Final Verdict Scenario Recommendation Want same behavior on both? Use CSI-based StorageClass and test PVC binding Need advanced features (e.g., replication, off-node backup)? Use Longhorn or Ceph with CSI Working across clusters? Use Velero with CSI snapshot for cross-cluster portability"},{"location":"Storage/Common%20Kubernetes%20storage%20types/","title":"Common Kubernetes storage types","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <ul> <li>Storage providers</li> <li>Container Storage Interface (CSI)</li> <li>The Kubernetes persistent volume subsystem</li> <li>Dynamic provisioning with Storage Classes</li> </ul> <p></p>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#kubernetes-storage-types-comparison","title":"\ud83d\udce6 Kubernetes Storage Types Comparison","text":"Type Access Mode Description Example Use Case Example emptyDir Pod-level only Ephemeral storage shared by containers in the same pod. Caching/temp files during pod lifetime <code>emptyDir: {}</code> in pod spec hostPath Node-level only Mounts a file/dir from host node into the pod. Accessing logs or shared folders on node <code>hostPath: /data/logs</code> NFS ReadWriteMany (RWX) Network File System \u2014 shared, file-based storage. Shared configs, data between multiple pods <code>nfs: { server: 10.0.0.1, path: \"/data\" }</code> Longhorn RWO, ROX, RWX Distributed block storage with replication, snapshot, backup. Stateful apps needing HA volumes <code>storageClassName: longhorn</code> in PVC CSI (Cloud) Varies Cloud-native block/file storage via Container Storage Interface. Cloud apps needing persistent volumes <code>storageClassName: gp2</code> (AWS), <code>premium</code> (Azure) Local PV Node-local Directly uses disks on the Kubernetes node. Performance-heavy workloads <code>local: { path: /mnt/disks/ssd1 }</code> GlusterFS RWX Distributed, scalable file storage system. Shared volumes across pods/nodes <code>glusterfs: { endpoints: ..., path: /data }</code> Ceph RBD/FS RWO, RWX Ceph RBD (block), CephFS (file) storage. Scalable &amp; reliable for prod workloads. DBs or shared file workloads <code>rbd: { monitors: [...], pool: ..., image: ... }</code> iSCSI RWO Raw block storage via iSCSI protocol. Legacy storage integration <code>iscsi: { targetPortal: ..., iqn: ..., lun: 0 }</code> Here\u2019s a clear comparison of Block, Network (File/Shared), Local, and CephFS storage types: Storage Type Description Use Case Access Type Examples Block Storage Raw storage volumes presented as disks to OS or containers. Databases, VMs, apps needing fast, low-latency storage ReadWriteOnce (usually) Longhorn, AWS EBS, iSCSI Network File Storage (NFS, GlusterFS) Shared storage accessed over network as file system. Shared configs, logs, or data across pods ReadWriteMany NFS, GlusterFS, CephFS Local Storage Storage physically attached to node (host). Fast access, node-specific data ReadWriteOnce hostPath, local PV CephFS Distributed POSIX-compliant file system on Ceph cluster. Shared storage with high availability ReadWriteMany CephFS ### Key Differences: <ul> <li> <p>Block Storage acts like a virtual disk, fast and low-level, but typically mounted by one pod at a time.</p> </li> <li> <p>Network File Storage allows multiple pods to read/write simultaneously over the network.</p> </li> <li> <p>Local Storage is node-specific, fast but tied to that node only.</p> </li> <li> <p>CephFS combines distributed file system features with scalability and high availability.</p> </li> </ul>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#pvc-pv-manifest-examples","title":"PVC + PV manifest examples","text":""},{"location":"Storage/Common%20Kubernetes%20storage%20types/#1-emptydir-ephemeral","title":"\ud83e\uddea 1. <code>emptyDir</code> (Ephemeral)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: emptydir-example\nspec:\n  containers:\n    - name: app\n      image: nginx\n      volumeMounts:\n        - name: cache\n          mountPath: /cache\n  volumes:\n    - name: cache\n      emptyDir: {}\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#2-hostpath","title":"\ud83c\udfe0 2. <code>hostPath</code>","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-example\nspec:\n  containers:\n    - name: app\n      image: nginx\n      volumeMounts:\n        - name: host-vol\n          mountPath: /data\n  volumes:\n    - name: host-vol\n      hostPath:\n        path: /mnt/data\n        type: Directory\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#3-nfs","title":"\ud83c\udf10 3. <code>NFS</code>","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: 10.0.0.100\n    path: /exports/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#4-longhorn","title":"\ud83d\udc02 4. <code>Longhorn</code>","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: longhorn-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 2Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#5-cloud-csi-aws-ebs-example","title":"\u2601\ufe0f 5. Cloud CSI (AWS EBS example)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: aws-ebs-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: gp2\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#6-local-pv","title":"\ud83d\udccd 6. Local PV","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: local-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  local:\n    path: /mnt/disks/ssd1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: kubernetes.io/hostname\n              operator: In\n              values:\n                - node-name\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: local-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#7-ceph-rbd-block-storage","title":"\ud83e\uddf1 7. Ceph RBD (Block Storage)","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ceph-secret\ntype: kubernetes.io/rbd\ndata:\n  key: &lt;base64-encoded-ceph-key&gt;\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ceph-rbd-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  rbd:\n    monitors:\n      - 10.0.0.1:6789\n    pool: kube\n    image: rbd-vol\n    user: admin\n    secretRef:\n      name: ceph-secret\n    fsType: ext4\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ceph-rbd-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#8-cephfs-file-storage","title":"\ud83d\udcc2 8. CephFS (File Storage)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: cephfs-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  cephfs:\n    monitors:\n      - 10.0.0.1:6789\n    path: /\n    user: admin\n    secretRef:\n      name: ceph-secret\n    readOnly: false\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#9-glusterfs","title":"\ud83d\udce1 9. GlusterFS","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: gluster-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  glusterfs:\n    endpoints: gluster-cluster\n    path: myvol\n    readOnly: false\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: gluster-cluster\nsubsets:\n  - addresses:\n      - ip: 10.0.0.2\n    ports:\n      - port: 1\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gluster-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Storage/Common%20Kubernetes%20storage%20types/#10-iscsi","title":"\ud83d\udcbf 10. iSCSI","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: iscsi-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  iscsi:\n    targetPortal: 10.0.0.10:3260\n    iqn: iqn.2023-04.com.example:storage.disk1\n    lun: 0\n    fsType: ext4\n    readOnly: false\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: iscsi-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/","title":"Setup NFS on Ubuntu Local Machine","text":"<p>\u2705 Yes, you can easily set up an NFS server on your Ubuntu machine, and then use it in K3s or Kubernetes with the NFS CSI driver for dynamic volume provisioning.</p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#can-you-use-longhorn-on-top-of-nfs","title":"\u2753 Can You Use Longhorn on top of NFS?","text":"<p>\ud83d\udd34 No \u2014 Longhorn does not support NFS as a backend. Longhorn requires direct block storage (local SSD/HDD attached to nodes). You cannot map NFS to Longhorn as storage.</p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#step-by-step-setup-nfs-on-ubuntu-local-machine","title":"\ud83d\udee0\ufe0f Step-by-Step: Setup NFS on Ubuntu Local Machine","text":""},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#1-install-nfs-server","title":"\u2705 1. Install NFS Server","text":"<pre><code>sudo apt update\nsudo apt install nfs-kernel-server -y\n</code></pre>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#2-create-shared-directory","title":"\u2705 2. Create Shared Directory","text":"<pre><code>sudo mkdir -p /srv/nfs/k3s-volumes\nsudo chown nobody:nogroup /srv/nfs/k3s-volumes\nsudo chmod 777 /srv/nfs/k3s-volumes\n</code></pre> <p>This will be your shared volume base.</p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#3-export-the-directory","title":"\u2705 3. Export the Directory","text":"<p>Edit exports file: <pre><code>sudo nano /etc/exports\n</code></pre> Add this line: <pre><code>/srv/nfs/k3s-volumes  *(rw,sync,no_subtree_check,no_root_squash)\n</code></pre> Apply the export: <pre><code>sudo exportfs -rav\n</code></pre> Restart the NFS service: <pre><code>sudo systemctl restart nfs-kernel-server\n</code></pre></p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#4-check-the-nfs-is-working","title":"\u2705 4. Check the NFS is Working","text":"<p><pre><code>showmount -e localhost\n</code></pre> Expected: <pre><code>Export list for localhost:\n/srv/nfs/k3s-volumes *\n</code></pre></p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#optional-open-firewall","title":"\ud83d\udd0c (Optional) Open Firewall","text":"<p>If running firewall: <pre><code>sudo ufw allow from &lt;your_k3s_node_ip&gt; to any port nfs\n</code></pre></p>"},{"location":"Storage/Setup%20NFS%20on%20Ubuntu%20Local%20Machine/#next-use-it-in-k3s-with-csi-driver","title":"\ud83d\ude80 Next: Use It in K3s with CSI Driver","text":"<p>Let me know and I\u2019ll give you:</p> <ul> <li>NFS Client Provisioner setup</li> <li>YAML for NFS <code>StorageClass</code></li> <li>Sample PVC &amp; Pod using the NFS backend</li> </ul>"},{"location":"Storage/Storage%20Types/","title":"Storage Types","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Storage/Storage%20Types/#types-of-storages","title":"Types of Storage's","text":"Storage Type Description Example Use Cases Object Storage Stores data as objects with metadata and unique IDs, accessed via APIs (like S3). Optimized for large amounts of unstructured data (files, images, backups). Not a traditional file system. AWS S3, MinIO, Azure Blob Storage File Storage Stores data as files in a hierarchical folder structure, accessible via file protocols (NFS, SMB). Network shares, local file systems Block Storage Provides raw storage volumes at the block level for use by OS/file systems (like hard drives). Longhorn, EBS, Persistent Volumes Feature Block Storage File Storage Object Storage Structure Raw blocks (like a disk) Hierarchical (files &amp; folders) Flat (objects with metadata) Access Low-level (via OS, formatted as FS) Via file systems (NFS, SMB) Via API (HTTP/REST) Use Cases Databases, VMs, OS disks Shared drives, home dirs, web servers Backups, media, logs, cloud apps Performance High (I/O intensive) Medium Variable (depends on use &amp; service) Scalability Limited to volume size Limited by FS &amp; protocol Highly scalable Examples AWS EBS, GCP PD, Azure Disk NFS, SMB, EFS, Azure Files S3, Azure Blob, GCP Cloud Storage"},{"location":"Storage/Storage%20Types/#storage-comparison-with-when-to-use","title":"Storage Comparison with When to Use ?","text":"Feature Block Storage File Storage Object Storage Structure Raw blocks, mount as disk Files &amp; folders (hierarchical) Objects with metadata (flat) Access Low-level, via OS (mount, format) NFS, SMB (file share protocols) REST API, HTTP-based Use Cases Databases, boot volumes, containers, VMs Shared directories, CMS, user home folders Backups, archives, logs, media, cloud-native apps Performance High IOPS, low latency Moderate Optimized for throughput, not IOPS Scalability Limited to disk/volume Limited by filesystem Highly scalable (petabytes+) Examples AWS EBS, GCP Persistent Disk, Azure Disk NFS, AWS EFS, Azure Files AWS S3, Azure Blob, GCP Cloud Storage When to Use - You need fast I/O (DBs) - Requires filesystem - Persistent volume for VMs or Pods - Shared access between multiple VMs - Classic apps using file systems - Store unstructured data (images, videos, logs) - Archive or backup with metadata - Internet-accessible content"},{"location":"Storage/Storage%20Types/#storage-types-with-tools","title":"Storage Types with Tools","text":"Storage Type Tools / Solutions Block Storage - AWS EBS- Azure Managed Disks- GCP Persistent Disks- OpenEBS- Longhorn- Ceph RBD- iSCSI- LVM File Storage - NFS- SMB/CIFS- AWS EFS- Azure Files- GCP Filestore- GlusterFS- CephFS Object Storage - AWS S3- Azure Blob Storage- GCP Cloud Storage- MinIO- Ceph Object Gateway (RGW)- OpenIO- Scality- IBM Cloud Object Storage # Kubernetes CSI Drivers (with CLI Tools) Storage Type Tools / CSI Driver CLI / Setup Tool Notes Block Storage - <code>ebs.csi.aws.com</code> - <code>pd.csi.storage.gke.io</code> - <code>disk.csi.azure.com</code> - <code>longhorn.io</code> - <code>openebs.io/local</code> - <code>aws eksctl</code>, <code>helm</code> - <code>gcloud container clusters</code> - <code>az aks</code>, <code>kubectl</code> - <code>helm</code>, <code>kubectl</code> Use <code>PersistentVolume</code> + <code>StorageClass</code> File Storage - <code>efs.csi.aws.com</code> - <code>file.csi.azure.com</code> - <code>nfs.csi.k8s.io</code> - <code>cephfs.csi.ceph.com</code> - <code>aws cli</code>, <code>helm</code> - <code>az cli</code>, <code>helm</code> - <code>kubectl</code>, custom NFS server - <code>rook-ceph</code>, <code>helm</code> Great for shared file systems Object Storage - <code>s3-csi-driver</code> (community) - <code>minio</code> - <code>ceph-rgw</code> - <code>mc</code> (MinIO client) - <code>rclone</code>, <code>s5cmd</code>, <code>aws cli</code> - <code>helm</code>, <code>rook</code> Object storage not used as native PV, used via apps # Example (EBS CSI - block storage) <pre><code># Install AWS EBS CSI Driver (EKS example)\nkubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\"\n\n# StorageClass example\nkubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-sc\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp2\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nEOF\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/","title":"Understanding Tolerations in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#what-are-tolerations","title":"What are Tolerations?","text":"<p>Tolerations are Kubernetes mechanisms that allow pods to be scheduled onto nodes with matching taints. They work in conjunction with taints to enable advanced scheduling scenarios where certain nodes are \"reserved\" for specific workloads.</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#why-use-tolerations","title":"Why Use Tolerations?","text":"<p>Tolerations are used when you need to: - Schedule pods on specialized nodes (like GPU nodes) - Allow critical workloads to run on reserved nodes - Handle node maintenance gracefully - Implement multi-tenant clusters with dedicated resources</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#when-to-use-tolerations","title":"When to Use Tolerations?","text":"<p>Common use cases: - Nodes with specialized hardware - Nodes reserved for specific teams/applications - Nodes that might be problematic for some workloads (e.g., spot instances) - Nodes undergoing maintenance</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#how-to-use-tolerations","title":"How to Use Tolerations?","text":""},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#basic-example-gpu-workload","title":"Basic Example: GPU Workload","text":"<ol> <li> <p>Taint the GPU node: <pre><code>kubectl taint nodes gpu-node-1 hardware=gpu:NoSchedule\n</code></pre></p> </li> <li> <p>Create a pod with matching toleration:</p> </li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:11.0-base\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n  tolerations:\n  - key: \"hardware\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#advanced-example-spot-instance-with-prefernoschedule","title":"Advanced Example: Spot Instance with PreferNoSchedule","text":"<ol> <li> <p>Taint the spot instance nodes: <pre><code>kubectl taint nodes spot-node-1 instance-type=spot:PreferNoSchedule\n</code></pre></p> </li> <li> <p>Pod with toleration and node selector:</p> </li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cost-optimized-app\nspec:\n  containers:\n  - name: app\n    image: my-app:latest\n  tolerations:\n  - key: \"instance-type\"\n    operator: \"Equal\"\n    value: \"spot\"\n    effect: \"PreferNoSchedule\"\n  nodeSelector:\n    instance-type: spot\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#maintenance-example-allow-pod-during-node-drain","title":"Maintenance Example: Allow pod during node drain","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-app\nspec:\n  containers:\n  - name: app\n    image: critical-app:latest\n  tolerations:\n  - key: \"node.kubernetes.io/unschedulable\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  - key: \"node.kubernetes.io/unreachable\"\n    operator: \"Exists\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 6000  # Keep running for 100 minutes if node becomes unreachable\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#key-points-to-remember","title":"Key Points to Remember","text":"<ol> <li>Tolerations don't guarantee pod scheduling - they just allow it if other conditions are met</li> <li>Use <code>operator: \"Exists\"</code> when you don't care about the value</li> <li><code>tolerationSeconds</code> controls how long a pod stays bound to an unreachable node</li> <li>Combine tolerations with nodeSelectors/affinity for precise scheduling</li> </ol> <p>Tolerations are powerful tools for advanced scheduling scenarios in Kubernetes, especially when working with heterogeneous node pools or specialized hardware.</p>"},{"location":"Taint-Tolerations/What%20is%20taint-toleration/","title":"What is taint toleration","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>In Kubernetes, a taint is like a \"warning label\" you put on a node to say:  </p> <p>\"Don\u2019t schedule pods here unless they explicitly allow it!\" </p>"},{"location":"Taint-Tolerations/What%20is%20taint-toleration/#simple-meaning","title":"Simple Meaning:","text":"<p>A taint marks a node as restricted\u2014pods will avoid it unless they have a matching toleration (special permission to run there).  </p>"},{"location":"Taint-Tolerations/What%20is%20taint-toleration/#real-world-example","title":"Real-World Example:","text":"<p>Imagine a \"Employees Only\" sign on a door: - Taint = The sign (\"Employees Only\") - Toleration = An employee badge (allows entry) - Pods without toleration = Regular people (can\u2019t enter)  </p>"},{"location":"Taint-Tolerations/What%20is%20taint-toleration/#why-use-it","title":"Why Use It?","text":"<ul> <li>Reserve nodes for special workloads (e.g., GPU nodes).  </li> <li>Prevent random pods from scheduling on sensitive nodes.  </li> <li>Handle maintenance or unreliable nodes (e.g., spot instances).  </li> </ul>"},{"location":"Taint-Tolerations/What%20is%20taint-toleration/#command-example","title":"Command Example:","text":"<p><pre><code>kubectl taint nodes my-node special=true:NoSchedule  \n</code></pre> This means: \"Only pods that tolerate <code>special=true</code> can run here.\" </p> <p>In short: Taints block pods, tolerations bypass the block. \ud83d\udea6</p>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/","title":"Securing Traefik Dashboard with Keycloak (OIDC or OAuth2)","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#securing-traefik-dashboard-with-keycloak-oidcoauth2","title":"Securing Traefik Dashboard with Keycloak (OIDC/OAuth2)","text":"<p>Yes, you can definitely use Keycloak instead of basic auth to secure your Traefik dashboard. This is a more secure and modern approach using OIDC/OAuth2. Here's how to set it up:</p>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running Keycloak instance</li> <li>Traefik deployed (preferably via Helm)</li> </ul>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-1-configure-keycloak","title":"Step 1: Configure Keycloak","text":"<ol> <li>Create a new client in Keycloak:</li> <li>Client ID: <code>traefik-dashboard</code></li> <li>Client Protocol: <code>openid-connect</code></li> <li>Root URL: <code>https://traefik.yourdomain.com</code></li> <li>Valid Redirect URIs: <code>https://traefik.yourdomain.com/oauth2/callback</code></li> <li>Web Origins: <code>+</code></li> <li>Access Type: <code>confidential</code></li> <li> <p>Standard Flow Enabled: ON</p> </li> <li> <p>Create roles (e.g., <code>traefik-admin</code>) and assign them to users</p> </li> <li> <p>Note down these values from the client's \"Credentials\" tab:</p> </li> <li>Client ID</li> <li>Client Secret</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-2-configure-traefik-with-keycloak-integration","title":"Step 2: Configure Traefik with Keycloak Integration","text":""},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#using-helm-values","title":"Using Helm Values","text":"<p>Update your <code>values.yaml</code>:</p> <pre><code>additionalArguments:\n  - --api.dashboard=true\n  - --providers.kubernetesingress\n  - --providers.kubernetescrd\n  - --entrypoints.web.address=:80\n  - --entrypoints.websecure.address=:443\n  - --certificatesresolvers.default.acme.tlschallenge=true\n  - --certificatesresolvers.default.acme.email=your@email.com\n  - --certificatesresolvers.default.acme.storage=/data/acme.json\n\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: Host(`traefik.yourdomain.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\n    entryPoints: [\"websecure\"]\n    middlewares:\n      - name: keycloak-auth\n    tls:\n      certResolver: default\n\nmiddlewares:\n  keycloak-auth:\n    enabled: true\n    forwardAuth:\n      address: \"https://keycloak.yourdomain.com/auth/realms/your-realm/protocol/openid-connect/auth\"\n      trustForwardHeader: true\n      authResponseHeaders:\n        - \"X-Forwarded-User\"\n        - \"Authorization\"\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#create-forwardauth-middleware-alternative","title":"Create ForwardAuth Middleware (Alternative)","text":"<p>For more control, create a separate ForwardAuth middleware:</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: keycloak-oauth\nspec:\n  forwardAuth:\n    address: \"https://keycloak.yourdomain.com/auth/realms/your-realm/protocol/openid-connect/auth\"\n    authResponseHeaders:\n      - \"X-Forwarded-User\"\n      - \"Authorization\"\n    tls:\n      insecureSkipVerify: false # Set to true if using self-signed certs\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-3-configure-keycloak-authentication-flow","title":"Step 3: Configure Keycloak Authentication Flow","text":"<ol> <li>In Keycloak, go to your client's settings</li> <li>Add these valid redirect URIs:</li> <li><code>https://traefik.yourdomain.com/oauth2/callback</code></li> <li><code>https://traefik.yourdomain.com/dashboard/*</code></li> <li>Configure roles/scopes as needed</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-4-deploy-the-configuration","title":"Step 4: Deploy the Configuration","text":"<pre><code>helm upgrade --install traefik traefik/traefik -f values.yaml -n traefik\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#verification","title":"Verification","text":"<ol> <li>Access <code>https://traefik.yourdomain.com/dashboard/</code></li> <li>You should be redirected to Keycloak for authentication</li> <li>After successful login, you'll be redirected back to the Traefik dashboard</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#additional-security-recommendations","title":"Additional Security Recommendations","text":"<ol> <li> <p>Add role-based access:    <pre><code># In your middleware configuration\nlabels:\n  traefik.forwardauth.authResponseHeadersRegex: \"^X-Forwarded-User$\"\n  traefik.forwardauth.authResponseHeaders: \"X-Forwarded-User, Authorization\"\n</code></pre></p> </li> <li> <p>Enable HTTPS everywhere:    <pre><code>additionalArguments:\n  - --entrypoints.web.http.redirections.entryPoint.to=websecure\n  - --entrypoints.web.http.redirections.entryPoint.scheme=https\n</code></pre></p> </li> <li> <p>Configure session timeout in Keycloak client settings</p> </li> </ol> <p>This setup provides much better security than basic auth, with features like: - Single Sign-On (SSO) - Multi-factor authentication - Session management - Fine-grained access control - Token expiration</p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/","title":"Traefik Dashboard only for Visibility not for Administration","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#traefik-dashboard-visibility-vs-administration","title":"Traefik Dashboard: Visibility vs. Administration","text":"<p>Yes, by default, the Traefik Dashboard is primarily for visibility and monitoring\u2014it displays routing configurations, services, middlewares, and HTTP traffic metrics. However, it does not allow direct administration (e.g., modifying routes, adding TLS, or changing load-balancing rules) unless explicitly configured for write access (which is not recommended for security reasons).</p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#key-points-about-traefik-dashboard","title":"Key Points About Traefik Dashboard","text":"Aspect Details Default Mode Read-only (just visualization) Admin Access Disabled by default (must explicitly enable API write access) Security Risk Enabling admin control exposes Traefik to potential misconfigurations or attacks Alternative Use Kubernetes <code>IngressRoute</code>/<code>Ingress</code> CRDs or Traefik's CLI/API for changes"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#how-to-expose-the-dashboard-safely","title":"How to Expose the Dashboard (Safely)","text":""},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#1-read-only-dashboard-recommended","title":"1. Read-Only Dashboard (Recommended)","text":"<pre><code># traefik-dashboard.yaml (IngressRoute for Kubernetes)\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.example.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\n      kind: Rule\n      services:\n        - name: api@internal  # Built-in Traefik service\n          kind: TraefikService\n</code></pre>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#2-enable-api-for-admin-access-use-with-caution","title":"2. Enable API (For Admin Access - Use with Caution!)","text":"<pre><code># traefik-config.yaml (Dangerous!)\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-auth\nspec:\n  basicAuth:\n    secret: traefik-auth-secret  # Requires a Kubernetes Secret\n\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard-admin\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`traefik-admin.example.com`) &amp;&amp; PathPrefix(`/api`)\n      kind: Rule\n      middlewares:\n        - name: traefik-auth\n      services:\n        - name: api@internal\n          kind: TraefikService\n  tls:\n    secretName: traefik-tls\n</code></pre> <p>\u26a0\ufe0f Warning: - The API allows full control (create/delete routes, TLS, middlewares). - Never expose <code>/api</code> without authentication (use <code>basicAuth</code> or IP whitelisting). - Prefer Kubernetes CRDs (<code>IngressRoute</code>) for safer, GitOps-friendly management.  </p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#best-practices","title":"Best Practices","text":"<ol> <li>Keep it read-only (only expose <code>/dashboard</code>).  </li> <li>Use auth (BasicAuth, OAuth, or network policies).  </li> <li>Avoid exposing <code>/api</code> unless absolutely necessary.  </li> <li>Prefer CLI/kubectl for changes instead of the dashboard.  </li> </ol> <p>Conclusion: The dashboard is great for debugging and monitoring, but not for administration unless explicitly (and securely) configured. Always manage Traefik via Kubernetes manifests or API with strict access controls. \ud83d\udd12</p>"},{"location":"Ubuntu/Check%20memory%20slots%20available/","title":"Check memory slots available","text":"<p>Created: 2025-07-08 | Updated: 2025-07-08 | Author: Gouse Shaik</p> <pre><code>$ sudo dmidecode -t memory | grep -E 'Size:|Locator:' | grep -B1 'Size:' | grep 'Locator\\|Size'\n$ sudo dmidecode -t memory | grep -E 'Size:|Locator:'\n        Size: 8 GB\n        Locator: ChannelA-DIMM0\n        Bank Locator: BANK 0\n        Size: 8 GB\n        Locator: ChannelB-DIMM0\n        Bank Locator: BANK 2\n</code></pre>"},{"location":"Ubuntu/list%20of%20good%20tools%20to%20extract%20rar%20part%20files/","title":"List of good tools to extract rar part files","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here's a list of good tools to extract <code>.part*.rar</code> files on Ubuntu:</p> Tool Command to Install Supports Multi-Part RAR Notes unrar <code>sudo apt install unrar</code> \u2705 Yes Official but non-free binary unar <code>sudo apt install unar</code> \u2705 Yes Free and easier syntax 7z (p7zip-full) <code>sudo apt install p7zip-full</code> \u2705 Yes Supports many formats, flexible rar <code>sudo apt install rar</code> \u2705 Yes Non-free, but official RAR tool <p>\ud83d\udd27 Recommended: Use <code>unar</code> for ease or <code>unrar</code> if you're okay with non-free software.</p> <p>Yes \u2014 just run <code>7z x filename.part1.rar</code>, and it will:</p> <ul> <li>\u2705 Automatically detect and extract all parts (<code>filename.part2.rar</code>, <code>part3.rar</code>, etc.)</li> <li>\u2757 All parts must be in the same directory</li> <li>\ud83d\udeab Do not run it on <code>part2.rar</code> or later \u2014 only <code>part1.rar</code> contains the header</li> </ul>"},{"location":"Ubuntu/list%20of%20good%20tools%20to%20extract%20rar%20part%20files/#example","title":"Example:","text":"<pre><code>7z x myarchive.part1.rar\n</code></pre> <p>Will extract the full content from: <pre><code>myarchive.part1.rar\nmyarchive.part2.rar\nmyarchive.part3.rar\n...\n</code></pre></p>"},{"location":"annotations/About%20annotations%20in%20k8s/","title":"About annotations in k8s","text":"<p>Created: 2025-06-23 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"annotations/About%20annotations%20in%20k8s/#what-are-annotations","title":"\u2705 What are annotations?","text":"<ul> <li>Annotations are key-value pairs used to attach non-identifying metadata to Kubernetes objects.</li> <li>Unlike labels (used for selection/filtering), annotations are used for descriptive or informational purposes.</li> </ul>"},{"location":"annotations/About%20annotations%20in%20k8s/#when-to-use-annotations","title":"\ud83d\udd52 When to use annotations?","text":"<p>Use annotations when:</p> <ul> <li>You need to store metadata that doesn't affect how Kubernetes manages the object.</li> <li>You want to document context, track changes, or pass info to tools/controllers.</li> </ul> <p>Examples: - Add build info, commit hash, owner, team - Store debugging or troubleshooting info - Track last edited by whom, when, and why - Set instructions for custom controllers or operators</p>"},{"location":"annotations/About%20annotations%20in%20k8s/#why-use-annotations","title":"\u2753Why use annotations?","text":"<ul> <li>They don't affect scheduling, selectors, or object behavior (unlike labels).</li> <li>They\u2019re perfect for audit trails, history, and external tools.</li> <li>Clean way to embed business context inside manifests.</li> <li>Helps for DevOps transparency and team collaboration.</li> </ul>"},{"location":"annotations/About%20annotations%20in%20k8s/#how-to-use-annotations","title":"\u2699\ufe0f How to use annotations?","text":""},{"location":"annotations/About%20annotations%20in%20k8s/#1-cli","title":"1. CLI","text":"<pre><code>kubectl annotate pod mypod key=value\nkubectl annotate pod mypod owner=\"gowse\" purpose=\"debugging\"\nkubectl annotate pod mypod purpose-\n</code></pre>"},{"location":"annotations/About%20annotations%20in%20k8s/#2-yaml","title":"2. YAML","text":"<pre><code>metadata:\n  annotations:\n    owner: \"gowse\"\n    purpose: \"troubleshooting\"\n</code></pre>"},{"location":"annotations/About%20annotations%20in%20k8s/#3-use-cases","title":"3. Use Cases","text":"Use Case Key Example Git metadata git.commit=abc123 Owner tracking owner=gowse Change reason w3h.why=\"updated image to fix bug\" Debug instructions w3h.how=\"enable debug logging in config\" Triggering CI/CD tools build.trigger=manual Custom controller hints sidecar-injector.k8s.io/inject: \"true\""},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/","title":"Install etcdctl tool with supported versions","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Learn etcd : \"https://etcd.io/docs/\" <pre><code>gouse@gouse:~$ k3d version\nk3d version v5.8.3\nk3s version v1.31.5-k3s1 (default)\n\ngouse@gouse:~$ kubectl version\nClient Version: v1.30.4+k3s1\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.31.5+k3s1\n</code></pre></p> <p>Since your k3s version is <code>v1.31.5-k3s1</code>, it uses etcd v3.5.x internally. \u2705 So, you should install <code>etcdctl v3.5.13</code>, which is fully compatible.</p>"},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/#install-etcdctl-v3513-on-host","title":"\ud83d\udce6 Install <code>etcdctl v3.5.13</code> on Host","text":"<pre><code>ETCD_VER=v3.5.13\n\nwget https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz\ntar xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz\nsudo mv etcd-${ETCD_VER}-linux-amd64/etcdctl /usr/local/bin/\nrm -rf etcd-${ETCD_VER}-linux-amd64*\n</code></pre> <p>Then confirm:</p> <pre><code>etcdctl version\n</code></pre>"},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/#etcd-compatibility-versions-list","title":"etcd compatibility versions list","text":"Kubernetes Version etcd Version Used (Bundled) v1.31.x v3.5.9+ v1.30.x v3.5.9 v1.29.x v3.5.9 v1.28.x v3.5.7 v1.27.x v3.5.6 v1.26.x v3.5.6 v1.25.x v3.5.4 v1.24.x v3.5.3 v1.23.x v3.5.0 v1.22.x v3.4.13 v1.21.x v3.4.13 v1.20.x v3.4.13"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/","title":"To take backup of etcd db with etcdctl tool","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Great \u2014 since you have a k3d HA cluster (3 control-plane nodes) you're running embedded etcd, and can safely perform etcd backups the k3s way.</p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#prerequisites","title":"\u2705 Prerequisites:","text":"<ul> <li>Embedded etcd is used (since you have 3 servers)</li> <li>Backup must be done from only one server node (typically <code>server-0</code>)</li> <li>You can exec into the container or mount volumes</li> </ul>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#method-1-manual-backup-via-etcdctl-inside-container","title":"\ud83d\udd27 Method 1: Manual Backup via <code>etcdctl</code> (inside container)","text":""},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#1-exec-into-server-0","title":"1. Exec into <code>server-0</code>","text":"<pre><code>docker exec -it k3d-ha-cluster-server-0 sh\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#2-install-etcdctl-inside-a-cluster-server-0-container","title":"2. Install etcdctl inside a cluster server-0 container","text":"<pre><code>ETCD_VER=v3.5.13\ncurl -L https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o etcd.tar.gz\ntar xzf etcd.tar.gz\nsudo mv etcd-${ETCD_VER}-linux-amd64/etcdctl /usr/local/bin/\n\ndocker cp etcdctl &lt;container_name&gt;:/usr/local/bin/etcdctl\ndocker cp etcdctl k3d-ha-cluster-server-0:/usr/local/bin/etcdctl\n\ndocker exec -it &lt;container_name&gt; chmod +x /usr/local/bin/etcdctl\ndocker exec -it k3d-ha-cluster-server-0 chmod +x /usr/local/bin/etcdctl\ninside docker container\nchmod 777 /usr/local/bin/etcdctl\n</code></pre> <p>or  <pre><code>ETCD_VER=v3.5.0\n\n# choose either URL\nGOOGLE_URL=https://storage.googleapis.com/etcd\nGITHUB_URL=https://github.com/etcd-io/etcd/releases/download\nDOWNLOAD_URL=${GOOGLE_URL}\n\nrm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\nrm -rf /tmp/etcd-download-test &amp;&amp; mkdir -p /tmp/etcd-download-test\n\ncurl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\ntar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1\nrm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n\n/tmp/etcd-download-test/etcd --version\n/tmp/etcd-download-test/etcdctl version\n/tmp/etcd-download-test/etcdutl version\n\nmv /tmp/etcd-download-test/etcd /tmp/etcd-download-test/etcdctl /tmp/etcd-download-test/etcdutl /usr/local/bin\n</code></pre></p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#3-set-etcd-env-vars","title":"3. Set etcd env vars:","text":"<pre><code>export ETCDCTL_API=3\nexport K3S_DATA_DIR=/var/lib/rancher/k3s\nexport ETCDCTL_CACERT=\"${K3S_DATA_DIR}/server/tls/etcd/peer/ca.crt\"\nexport ETCDCTL_CERT=\"${K3S_DATA_DIR}/server/tls/etcd/peer/server-client.crt\"\nexport ETCDCTL_KEY=\"${K3S_DATA_DIR}/server/tls/etcd/peer/server-client.key\"\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#3-run-backup-command","title":"3. Run backup command:","text":"<pre><code>ETCDCTL API=3 etcdctl \\\n--endpoints https://172.31.20.211:2379 \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt\n--cert=/etc/kubernetes/pki/etcd/server.crt\n- -key=/etc/kubernetes/pki/etcd/server. keym\nsnapshot save_snapshot-backup.db\n\nor\n\netcdctl snapshot save /tmp/etcd-backup.db\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#verify-backup","title":"Verify Backup","text":"<p>By the way if you not sure about values used in above command then you can check values from your /etc/kubernetes/manifests/etcd.yaml file. Let\u2019s confirm status of our backup using,</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints https://172.17.0.9:2379 snapshot status /tmp/snapshot-backup.db\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#4-exit-container-and-copy-file-out","title":"4. Exit container and copy file out:","text":"<pre><code>docker cp k3d-ha-cluster-server-0:/tmp/etcd-backup.db ./etcd-backup.db\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#restore-manually","title":"\ud83d\udd04 Restore (manually)","text":"<pre><code>etcdctl restore save /tmp/etcd-backup.db\n</code></pre> <p>Let me know if you want to simulate restore too \u2014 restoring etcd involves stopping k3s, wiping old data, restoring snapshot, and starting again.</p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20db%20with%20etcdctl%20tool/#optional-automate-backup-on-host","title":"\ud83d\udee0\ufe0f Optional: Automate backup on host","text":"<p>If you want to regularly backup etcd from outside using a script, I can prepare that too \u2014 let me know your storage preference (host folder, S3, etc.).</p> <p>Would you like a backup+restore shell script next?</p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/","title":"To take backup of etcd with minIO","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>No, MinIO is not mandatory \u2014 it's just a self-hosted S3-compatible storage. But it's an excellent choice for storing etcd backups on-prem or in air-gapped clusters.</p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#when-you-need-minio","title":"\u2705 When You Need MinIO","text":"<p>You need MinIO only if:</p> <ul> <li>You don\u2019t have AWS/GCS/S3 already.</li> <li>You want local S3-compatible storage inside Kubernetes.</li> <li>You use tools like Stash, Velero, kube-etcd-backup expecting S3.</li> </ul>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#how-to-setup-minio-on-kubernetes-simple-persistent","title":"\ud83d\udee0\ufe0f How to Setup MinIO on Kubernetes (Simple &amp; Persistent)","text":""},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#1-create-namespace","title":"1. Create Namespace","text":"<pre><code>kubectl create namespace minio\n</code></pre>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#2-apply-minio-deployment-pvc","title":"2. Apply MinIO Deployment &amp; PVC","text":"<p><pre><code># minio-deploy.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        image: quay.io/minio/minio\n        args:\n        - server\n        - /data\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: minioadmin\n        - name: MINIO_SECRET_KEY\n          value: minioadmin\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: /data\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: minio-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pvc\n  namespace: minio\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <pre><code>kubectl apply -f minio-deploy.yaml\n</code></pre></p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#3-expose-with-nodeport-or-ingress","title":"3. Expose with NodePort (or Ingress)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  type: NodePort\n  ports:\n    - port: 9000\n      targetPort: 9000\n      nodePort: 30900\n  selector:\n    app: minio\n</code></pre> <pre><code>kubectl apply -f minio-service.yaml\n</code></pre> <p>Access via: <pre><code>http://&lt;NodeIP&gt;:30900  \nUser: minioadmin  \nPass: minioadmin\n</code></pre></p>"},{"location":"etcdctl/To%20take%20backup%20of%20etcd%20with%20minIO/#how-to-integrate-with-backup-tools","title":"\u2705 How to Integrate with Backup Tools","text":"Tool Integration with MinIO Stash Add <code>Repository</code> CRD with endpoint <code>http://minio.minio.svc:9000</code>, and access keys Velero Use <code>velero install</code> with <code>--provider aws --bucket &lt;bucket&gt; --secret-file</code> kube-etcd-backup Set S3 config with MinIO endpoint &amp; creds"},{"location":"etcdctl/check%20etcd%20is%20running/","title":"Check etcd is running","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p><pre><code>gouse@gouse:~$ etcdctl put foo \"bar\" {\"level\":\"warn\",\"ts\":\"2025-06-13T18:12:52.857946+0300\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.13/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0005a2000/127.0.0.1:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\\\"\"} Error: context deadline exceeded\n</code></pre> The error means <code>etcdctl</code> cannot connect to your etcd server at <code>127.0.0.1:2379</code>. The connection is refused because etcd is either not running or not listening on that address/port.</p> <p>Check these:</p> <pre><code># Check if etcd is running\nps aux | grep etcd\n\n# Or if using systemd\nsudo systemctl status etcd\n\n# Check if port 2379 is open\nss -tlnp | grep 2379\n</code></pre> <p>If etcd is not running, start it:</p> <pre><code>sudo systemctl start etcd\n</code></pre> <p>If etcd is running but on a different address/port, set the endpoint explicitly:</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=&lt;your-etcd-ip&gt;:&lt;port&gt; put foo \"bar\"\n</code></pre>"},{"location":"etcdctl/check%20etcd%20is%20running/#for-a-k3d-cluster","title":"For a k3d cluster","text":"<p>For a k3d cluster, etcd runs inside the k3s server container and is not directly accessible on <code>127.0.0.1:2379</code> by default.</p> <p>To interact with etcd in k3d/k3s, you usually:</p> <ol> <li> <p>Exec into the k3s server container where etcd runs.</p> </li> <li> <p>Use etcdctl inside that container with the correct environment variables.</p> </li> </ol> <p>Example commands:</p> <pre><code># List k3d containers\ndocker ps | grep k3d\n\n# Exec into the k3s server container (replace container name)\ndocker exec -it &lt;k3d-server-container-name&gt; sh\n\n# Inside container, set etcdctl env variables and run command\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/client.key\n\netcdctl put foo \"bar\"\n</code></pre> <p>If you want, I can give you the exact command to get your k3d server container name and run etcdctl inside it.</p> <p>If you really need etcdctl, consider:</p> <ul> <li> <p>Running an etcdctl container inside k3d with mounted certs.</p> </li> <li> <p>Or use the k3s builtin ==<code>3s etcd-snapshot</code>== commands if your version supports it.</p> </li> </ul>"},{"location":"etcdctl/check%20etcd%20is%20running/#to-ensure-etcd-is-configured-for-ha-in-k3s-cluster","title":"To ensure etcd is configured for HA in K3s cluster","text":"<p>verify the following directly on the master nodes:</p>"},{"location":"etcdctl/check%20etcd%20is%20running/#checklist-confirm-etcd-is-ha-in-k3s","title":"\u2705 Checklist: Confirm etcd is HA in K3s","text":""},{"location":"etcdctl/check%20etcd%20is%20running/#1-check-etcd-members","title":"\ud83d\udd39 1. Check etcd members","text":"<p>Run on any master node:</p> <pre><code>sudo k3s etcd-member-list\n</code></pre> <p>\u2705 If you see 3 healthy members (one per master), it's HA</p>"},{"location":"etcdctl/check%20etcd%20is%20running/#2-check-etcd-data-directory","title":"\ud83d\udd39 2. Check etcd data directory","text":"<pre><code>ls -l /var/lib/rancher/k3s/server/db/etcd\n</code></pre> <p>\u2705 If the directory exists, K3s is using embedded etcd</p>"},{"location":"etcdctl/check%20etcd%20is%20running/#3-check-logs-for-etcd-cluster-formation","title":"\ud83d\udd39 3. Check logs for etcd cluster formation","text":"<pre><code>sudo cat /var/log/syslog | grep -i etcd | grep member\n</code></pre> <p>or</p> <pre><code>sudo cat /tmp/k3s.log | grep etcd\n</code></pre> <p>\u2705 Look for lines showing member joining, leader election, etc.</p>"},{"location":"etcdctl/check%20etcd%20is%20running/#4-check-k3s-process-flags","title":"\ud83d\udd39 4. Check K3s process flags","text":"<pre><code>ps aux | grep k3s\n</code></pre> <p>\u2705 You should see <code>--cluster-init</code> on the first node, and <code>--server https://...</code> on others.</p>"},{"location":"etcdctl/check%20etcd%20is%20running/#5-check-kubernetes-node-roles","title":"\ud83d\udd39 5. Check Kubernetes node roles","text":"<pre><code>kubectl get nodes -o wide\n</code></pre> <p>\u2705 You should see 3 control-plane nodes, not just one.</p>"},{"location":"etcdctl/etcdctl%20commands%20for%20get%2C%20put%2C%20snapshot%20save%20and%20restore/","title":"Etcdctl commands for get, put, snapshot save and restore","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code># Set a key\netcdctl put foo \"bar\"\n\n# Get a key\netcdctl get foo\n\n# Save a snapshot\netcdctl snapshot save /backup/etcd-snapshot.db\n\n# Restore a snapshot\netcdctl snapshot restore /backup/etcd-snapshot.db \\\n  --data-dir=/var/lib/etcd-restored\n</code></pre>"},{"location":"etcdctl/etcdctl%20commands%20for%20get%2C%20put%2C%20snapshot%20save%20and%20restore/#basic-setup","title":"\u2705 Basic Setup","text":"<ol> <li> <p>Install etcdctl (if not already available):     <pre><code>sudo apt install etcd-client\n</code></pre></p> </li> <li> <p>Access etcd running in Kubernetes (inside control-plane/master node):     <pre><code>kubectl exec -n kube-system etcd-&lt;node-name&gt; -c etcd -it -- sh\n</code></pre></p> </li> <li> <p>Set environment for etcdctl (inside the etcd pod or use with <code>ETCDCTL_API=3</code>):     <pre><code>export ETCDCTL_API=3\n</code></pre></p> </li> </ol>"},{"location":"etcdctl/etcdctl%20commands%20for%20get%2C%20put%2C%20snapshot%20save%20and%20restore/#common-etcdctl-commands","title":"\ud83d\udcd8 Common etcdctl Commands","text":"Task Command View cluster health <code>etcdctl --endpoints=&lt;ep&gt; endpoint health</code> View cluster members <code>etcdctl --endpoints=&lt;ep&gt; member list</code> Get a key <code>etcdctl --endpoints=&lt;ep&gt; get &lt;key&gt;</code> Put a key <code>etcdctl --endpoints=&lt;ep&gt; put &lt;key&gt; &lt;value&gt;</code> Delete a key <code>etcdctl --endpoints=&lt;ep&gt; del &lt;key&gt;</code> List all keys <code>etcdctl --endpoints=&lt;ep&gt; get \"\" --prefix --keys-only</code> Snapshot save <code>etcdctl --endpoints=&lt;ep&gt; snapshot save snapshot.db</code> Snapshot status <code>etcdctl snapshot status snapshot.db</code> Restore from snapshot <code>etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-restored</code> <p>Replace <code>&lt;ep&gt;</code> with <code>https://127.0.0.1:2379</code> or proper internal IP.</p>"},{"location":"etcdctl/etcdctl%20commands%20for%20get%2C%20put%2C%20snapshot%20save%20and%20restore/#if-tls-is-enabled-as-in-kubeadm-setup","title":"\ud83d\udd10 If TLS is Enabled (as in kubeadm setup):","text":"<pre><code>etcdctl \\\n  --cert=/etc/kubernetes/pki/etcd/peer.crt \\\n  --key=/etc/kubernetes/pki/etcd/peer.key \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --endpoints=https://127.0.0.1:2379 \\\n  endpoint health\n</code></pre> <p>Let me know if you want example output, script for backup/restore, or a real-time cluster key scan.</p>"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/","title":"set up Kind with external etcd and Longhorn","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>You can set up Kind with external etcd and Longhorn, but it's non-standard, requires manual steps, and is suitable only for advanced testing, not production.</p>"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/#whats-possible","title":"\u2705 What's Possible:","text":"Component Status Notes Kind cluster \u2705 Supported Single-node or multi-node in Docker containers External etcd \u2705 Possible Must run etcd outside Kind (container/VM) Longhorn \u2705 Possible (hacky) Needs hostPath or block device emulation ### \ud83d\udee0\ufe0f Steps (High-level):"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/#1-run-external-etcd","title":"1. Run external etcd","text":"<p>You can run etcd in a Docker container or VM outside Kind:</p> <pre><code>docker run -d --name etcd \\\n  -p 2379:2379 -p 2380:2380 \\\n  quay.io/coreos/etcd \\\n  etcd --name external-etcd \\\n       --data-dir /etcd-data \\\n       --initial-advertise-peer-urls http://0.0.0.0:2380 \\\n       --listen-peer-urls http://0.0.0.0:2380 \\\n       --advertise-client-urls http://0.0.0.0:2379 \\\n       --listen-client-urls http://0.0.0.0:2379 \\\n       --initial-cluster external-etcd=http://0.0.0.0:2380 \\\n       --initial-cluster-token etcd-cluster \\\n       --initial-cluster-state new\n</code></pre> <p>Replace <code>0.0.0.0</code> with host IP if needed.</p>"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/#2-point-control-plane-to-external-etcd","title":"2. Point control-plane to external etcd","text":"<p>Kind doesn\u2019t natively support external etcd, so you\u2019d need to:</p> <ul> <li>Start a Kind cluster without etcd (requires custom kubeadm config).</li> <li>Manually modify <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> to point to external etcd.</li> <li>Mount certs and configure connection.</li> </ul> <p>\ud83d\udfe5 Very complex and fragile.</p>"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/#3-deploy-longhorn","title":"3. Deploy Longhorn","text":"<p>Longhorn needs:</p> <ul> <li>Block storage or raw hostPath volume (fake disk).</li> <li>Running in a Kubernetes cluster with certain kernel modules.</li> </ul> <p>Kind nodes are Docker containers and don\u2019t have real disks.</p> <p>\u2705 Workaround:</p> <ul> <li>Use <code>loop</code> device + mount as <code>hostPath</code></li> <li>OR use Longhorn in a VM-based K3s cluster instead (easier)</li> </ul>"},{"location":"etcdctl/set%20up%20Kind%20with%20external%20etcd%20and%20Longhorn/#conclusion","title":"\ud83d\udd34 Conclusion:","text":"<ul> <li>You can do this, but it\u2019s unsupported, hacky, and brittle.</li> <li>Better: Use K3s or kubeadm with VMs + Longhorn + external etcd.</li> </ul> <p>Do you want me to give you a working setup example using <code>k3s</code> with external etcd and Longhorn?</p>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/","title":"Fluentd DaemonSet deployment YAML for Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here\u2019s a basic Fluentd DaemonSet deployment YAML for Kubernetes (output to stdout by default \u2014 you can change to Elasticsearch, Loki, etc.):</p> <p>Actually \u2014 \u2705 yes, we did deploy Fluentd in that YAML.</p>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#heres-how","title":"Here's how:","text":"<ul> <li> <p>ConfigMap: Defines Fluentd's configuration (<code>fluent.conf</code>)</p> </li> <li> <p>DaemonSet: Deploys Fluentd on every node</p> <ul> <li> <p>Pulls <code>fluent/fluentd:v1.14-1</code> image</p> </li> <li> <p>Mounts the logs from host (<code>/var/log/containers/</code>)</p> </li> <li> <p>Runs using the config from the ConfigMap</p> </li> </ul> </li> <li> <p>ServiceAccount: For access control (if needed)</p> </li> </ul> <p>\ud83d\udd04 <code>DaemonSet</code> ensures Fluentd runs on each node, reading logs and processing them as defined.</p> <p>You can verify deployment with:</p> <pre><code>kubectl get pods -n logging -l name=fluentd\n</code></pre> <p>Let me know if you want a Loki/Elasticsearch version or a Helm chart method.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: logging\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\n  namespace: logging\ndata:\n  fluent.conf: |\n    &lt;source&gt;\n      @type tail\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kube.*\n      format json\n    &lt;/source&gt;\n\n    &lt;match **&gt;\n      @type stdout\n    &lt;/match&gt;\n\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: logging\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      serviceAccountName: fluentd\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          operator: Exists\n          effect: NoSchedule\n      containers:\n        - name: fluentd\n          image: fluent/fluentd:v1.14-1\n          env:\n            - name: FLUENTD_CONF\n              value: fluent.conf\n          volumeMounts:\n            - name: varlog\n              mountPath: /var/log\n            - name: config-volume\n              mountPath: /fluentd/etc/fluent.conf\n              subPath: fluent.conf\n      volumes:\n        - name: varlog\n          hostPath:\n            path: /var/log\n        - name: config-volume\n          configMap:\n            name: fluentd-config\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: logging\n</code></pre> <p>We can change the output destination (e.g., Loki, Elasticsearch, S3, etc.), so modify the <code>ConfigMap</code> section accordingly.</p> <p>Fluentd itself does not have a built-in dashboard for logs or metrics.</p>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#common-practice","title":"Common practice:","text":"<ul> <li> <p>Fluentd collects and forwards logs to logging backends that do provide dashboards, such as:</p> <ul> <li> <p>Elasticsearch + Kibana (Kibana is the dashboard UI)</p> </li> <li> <p>Grafana Loki + Grafana (Grafana is the dashboard)</p> </li> <li> <p>Splunk, Datadog, Sumo Logic, etc.</p> </li> </ul> </li> </ul>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#fluentd-monitoring","title":"Fluentd monitoring:","text":"<ul> <li> <p>You can enable Fluentd metrics (via plugins) and export them to Prometheus.</p> </li> <li> <p>Then visualize Fluentd metrics in Grafana dashboards (for Fluentd health, throughput, errors).</p> </li> </ul>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#tldr","title":"TL;DR:","text":"Tool Role Dashboard Fluentd Log collector and forwarder No built-in UI Elasticsearch Log storage and search Kibana Loki Log storage Grafana Prometheus Metrics collection Grafana <p>If you want, you can set up Fluentd with Elasticsearch + Kibana or Loki + Grafana for dashboards.</p>"},{"location":"fluentd/Fluentd%20can%20run%20on%20standalone%20VMs/","title":"Fluentd can run on standalone VMs","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Yes, Fluentd can run on standalone VMs (including those running Siebel or any apps) to collect logs and forward them.</p>"},{"location":"fluentd/Fluentd%20can%20run%20on%20standalone%20VMs/#how-it-works-on-standalone-vms","title":"How it works on standalone VMs:","text":"<ul> <li>Install Fluentd agent on the VM.</li> <li>Configure Fluentd to tail application log files (e.g., Siebel logs).</li> <li>Fluentd forwards logs to your desired backend (Elasticsearch, Loki, remote syslog, etc.).</li> </ul> Setup Fluentd Role Use Case Kubernetes Cluster DaemonSet log collector Collect container logs Standalone VM Fluentd agent daemon/service Collect app logs from files <p>Fluentd is very flexible and works well outside Kubernetes for log collection and forwarding.</p>"},{"location":"fluentd/Fluentd%20in%20Kubernetes/","title":"Fluentd in Kubernetes","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> W3H Details What Fluentd is a log collector and forwarder that unifies logging across systems. Why To collect, process, filter, and send logs from containers to destinations like Elasticsearch, Loki, S3, etc. Where Runs as a DaemonSet on each Kubernetes node to access container logs via <code>/var/log/containers/</code> or CRI socket. How Deploy Fluentd as a DaemonSet + ConfigMap for parsing + output plugin for destination. Example: Fluentd \u2192 Elasticsearch or Fluentd \u2192 Loki."},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/","title":"Fluentd install and config steps for a Linux VM","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Here are the steps to install and configure Fluentd on a standalone Linux VM to collect and forward logs (e.g., Siebel logs):</p>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#1-install-fluentd","title":"1. Install Fluentd","text":"<pre><code># Install td-agent (Fluentd stable distribution) on Ubuntu/Debian:\ncurl -fsSL https://packages.treasuredata.com/GPG-KEY-td-agent | sudo apt-key add -\necho \"deb https://packages.treasuredata.com/4/ubuntu/$(lsb_release -cs)/ $(lsb_release -cs) contrib\" | sudo tee /etc/apt/sources.list.d/treasure-data.list\nsudo apt-get update\nsudo apt-get install -y td-agent\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#2-configure-fluentd","title":"2. Configure Fluentd","text":"<p>Edit the main config file <code>/etc/td-agent/td-agent.conf</code>: Example to tail Siebel logs (adjust <code>/path/to/siebel/logs/*.log</code>):</p> <pre><code>&lt;source&gt;\n  @type tail\n  path /path/to/siebel/logs/*.log\n  pos_file /var/log/td-agent/siebel.log.pos\n  tag siebel.logs\n  format none\n&lt;/source&gt;\n\n&lt;match siebel.logs&gt;\n  @type stdout\n  # For forwarding to Elasticsearch, replace above with:\n  # @type elasticsearch\n  # host localhost\n  # port 9200\n&lt;/match&gt;\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#3-restart-fluentd","title":"3. Restart Fluentd","text":"<pre><code>sudo systemctl restart td-agent\nsudo systemctl enable td-agent\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#4-verify-fluentd-is-running","title":"4. Verify Fluentd is running","text":"<pre><code>sudo systemctl status td-agent\ntail -f /var/log/td-agent/td-agent.log\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#optional-forward-logs-to-elasticsearch-or-other-destinations","title":"Optional: Forward logs to Elasticsearch or other destinations","text":"<p>Replace the <code>&lt;match&gt;</code> block with appropriate output plugin config, for example:</p> <pre><code>&lt;match siebel.logs&gt;\n  @type elasticsearch\n  host your-elasticsearch-host\n  port 9200\n  logstash_format true\n  index_name siebel-logs\n&lt;/match&gt;\n</code></pre>"},{"location":"git/Git%20file%20attributes/","title":"Git file attributes","text":"<p>Created: 2025-07-08 | Updated: 2025-07-08 | Author: Gouse Shaik</p>"},{"location":"git/Git%20file%20attributes/#1-handling-line-endings-windows-vs-unix","title":"1. Handling Line Endings (Windows vs. Unix)","text":"<p>Scenario: Your team uses both Windows (CRLF) and Unix (LF) systems, leading to inconsistent line endings.  </p>"},{"location":"git/Git%20file%20attributes/#solution-use-gitattributes-to-enforce-consistent-line-endings","title":"Solution: Use <code>.gitattributes</code> to enforce consistent line endings.","text":""},{"location":"git/Git%20file%20attributes/#steps","title":"Steps:","text":"<ol> <li>Create/Edit <code>.gitattributes</code>:    <pre><code>touch .gitattributes\n</code></pre></li> <li>Define rules:    <pre><code># Auto-detect line endings for text files\n* text=auto\n\n# Force LF for scripts (Unix-compatible)\n*.sh eol=lf\n\n# Force CRLF for Windows-specific files\n*.bat eol=crlf\n</code></pre></li> <li>Apply changes:    <pre><code>git add .gitattributes\ngit commit -m \"Normalize line endings\"\n</code></pre></li> <li>Verify:    <pre><code>git ls-files --eol  # Shows line endings per file\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#2-treating-files-as-binary-avoiding-merge-conflicts","title":"2. Treating Files as Binary (Avoiding Merge Conflicts)","text":"<p>Scenario: Your repo has PDFs, Word docs, or JAR files that should not be diffed/merged.  </p>"},{"location":"git/Git%20file%20attributes/#solution-mark-them-as-binary-in-gitattributes","title":"Solution: Mark them as <code>binary</code> in <code>.gitattributes</code>.","text":""},{"location":"git/Git%20file%20attributes/#steps_1","title":"Steps:","text":"<ol> <li>Edit <code>.gitattributes</code>:    <pre><code>*.pdf binary\n*.docx binary\n*.jar binary\n</code></pre></li> <li>Commit changes:    <pre><code>git add .gitattributes\ngit commit -m \"Treat PDF/DOCX/JAR as binary\"\n</code></pre></li> <li>Test:    <pre><code>git diff  # Should skip binary files\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#3-custom-diff-for-specific-files-eg-java-csv","title":"3. Custom Diff for Specific Files (e.g., Java, CSV)","text":"<p>Scenario: You want better <code>git diff</code> output for structured files (Java, CSV, JSON).  </p>"},{"location":"git/Git%20file%20attributes/#solution-use-a-custom-diff-driver","title":"Solution: Use a custom diff driver.","text":""},{"location":"git/Git%20file%20attributes/#steps_2","title":"Steps:","text":"<ol> <li>Define a diff driver in <code>.gitconfig</code>:    <pre><code>git config --global diff.java.textconv \"java -jar /path/to/javaprettifier.jar\"\n</code></pre></li> <li>Map it in <code>.gitattributes</code>:    <pre><code>*.java diff=java\n*.csv diff=csv\n</code></pre></li> <li>Test:    <pre><code>git diff MyFile.java  # Should use the custom formatter\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#4-git-lfs-handling-large-files","title":"4. Git LFS (Handling Large Files)","text":"<p>Scenario: Your repo has large files (videos, datasets, PSDs), bloating Git history.  </p>"},{"location":"git/Git%20file%20attributes/#solution-use-git-lfs-large-file-storage","title":"Solution: Use Git LFS (Large File Storage).","text":""},{"location":"git/Git%20file%20attributes/#steps_3","title":"Steps:","text":"<ol> <li>Install Git LFS:    <pre><code>git lfs install\n</code></pre></li> <li>Track large files (in <code>.gitattributes</code>):    <pre><code>*.psd filter=lfs diff=lfs merge=lfs -text\n*.mp4 filter=lfs diff=lfs merge=lfs -text\n</code></pre></li> <li>Commit &amp; push:    <pre><code>git add .gitattributes\ngit commit -m \"Track PSD/MP4 with LFS\"\ngit push\n</code></pre></li> <li>Verify:    <pre><code>git lfs ls-files  # Lists LFS-tracked files\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#5-export-ignore-exclude-files-from-archives","title":"5. Export-Ignore (Exclude Files from Archives)","text":"<p>Scenario: You generate <code>git archive</code> releases but want to exclude test files.  </p>"},{"location":"git/Git%20file%20attributes/#solution-use-export-ignore","title":"Solution: Use <code>export-ignore</code>.","text":""},{"location":"git/Git%20file%20attributes/#steps_4","title":"Steps:","text":"<ol> <li>Edit <code>.gitattributes</code>:    <pre><code>/tests/ export-ignore\n*.tmp export-ignore\n</code></pre></li> <li>Test:    <pre><code>git archive --format=zip -o release.zip HEAD\nunzip -l release.zip  # Should exclude /tests/ and *.tmp\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#6-filter-drivers-pre-process-files-on-commitcheckout","title":"6. Filter Drivers (Pre-Process Files on Commit/Checkout)","text":"<p>Scenario: You want to strip debug logs before committing.  </p>"},{"location":"git/Git%20file%20attributes/#solution-use-a-cleansmudge-filter","title":"Solution: Use a clean/smudge filter.","text":""},{"location":"git/Git%20file%20attributes/#steps_5","title":"Steps:","text":"<ol> <li>Define filter in <code>.gitconfig</code>:    <pre><code>git config --global filter.clean_logs.clean \"sed '/DEBUG/d'\"\n</code></pre></li> <li>Map it in <code>.gitattributes</code>:    <pre><code>*.log filter=clean_logs\n</code></pre></li> <li>Test:    <pre><code>echo \"INFO: OK\" &gt; test.log\necho \"DEBUG: Remove me\" &gt;&gt; test.log\ngit add test.log\ngit commit -m \"Test filter\"\ngit show :test.log  # Should exclude DEBUG lines\n</code></pre></li> </ol>"},{"location":"git/Git%20file%20attributes/#summary-table-key-scenarios-fixes","title":"Summary Table: Key Scenarios &amp; Fixes","text":"Scenario Solution <code>.gitattributes</code> Example Line endings <code>text=auto</code>, <code>eol=lf/crlf</code> <code>*.sh eol=lf</code> Binary files <code>binary</code>, <code>-diff -merge</code> <code>*.pdf binary</code> Custom diffs <code>diff=driver</code> <code>*.java diff=java</code> Large files <code>filter=lfs</code> <code>*.mp4 filter=lfs</code> Exclude from zip <code>export-ignore</code> <code>/tests/ export-ignore</code> Pre-process files <code>filter=clean/smudge</code> <code>*.log filter=clean_logs</code>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/","title":"How to Set Up Git Aliases","text":"<p>Created: 2025-07-08 | Updated: 2025-07-08 | Author: Gouse Shaik</p> <p>Git aliases are shortcuts that allow you to create custom Git commands or abbreviate existing ones. They can save you time and make your Git workflow more efficient. You can define them in your Git configuration file (<code>~/.gitconfig</code>) or by using the <code>git config</code> command.</p>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/#1-using-git-config","title":"1. Using <code>git config</code>","text":"<p>You can create aliases directly from the command line:</p> <pre><code>git config --global alias.&lt;shortcut&gt; \"&lt;full-command&gt;\"\n</code></pre> <p>Example: <pre><code>git config --global alias.co checkout  # Now `git co` = `git checkout`\ngit config --global alias.br branch   # Now `git br` = `git branch`\ngit config --global alias.ci commit   # Now `git ci` = `git commit`\n</code></pre></p>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/#2-editing-gitconfig-directly","title":"2. Editing <code>~/.gitconfig</code> Directly","text":"<p>You can manually add aliases to your Git config file (usually at <code>~/.gitconfig</code> or <code>~/.config/git/config</code>):</p> <pre><code>[alias]\n    co = checkout\n    br = branch\n    ci = commit\n    st = status\n    last = log -1 HEAD  # Show the last commit\n    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative\n</code></pre>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/#common-useful-git-aliases","title":"Common &amp; Useful Git Aliases","text":"<p>Here are some popular aliases:</p> Alias Command Description <code>git co</code> <code>checkout</code> Switch branches <code>git ci</code> <code>commit</code> Commit changes <code>git st</code> <code>status</code> Show working tree status <code>git br</code> <code>branch</code> List branches <code>git unstage</code> <code>reset HEAD --</code> Unstage changes <code>git last</code> <code>log -1 HEAD</code> Show the last commit <code>git lg</code> Custom log format Pretty commit history <code>git amend</code> <code>commit --amend</code> Amend the last commit <code>git fp</code> <code>push --force-with-lease</code> Safer force push ### Advanced Aliases You can even chain commands or use shell functions: <pre><code>[alias]\n    # Reset last commit while keeping changes\n    undo = reset --soft HEAD~1\n\n    # List all branches (local &amp; remote)\n    branches = !git branch -a\n\n    # Delete merged branches\n    cleanup = \"!git branch --merged | grep -v '\\\\*\\\\|main\\\\|master' | xargs -n 1 git branch -d\"\n\n    # Update &amp; rebase current branch\n    sync = !git pull --rebase origin $(git rev-parse --abbrev-ref HEAD)\n</code></pre>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/#listing-removing-aliases","title":"Listing &amp; Removing Aliases","text":"<ul> <li> <p>List all aliases: <pre><code>git config --get-regexp alias\n</code></pre>   or   <pre><code>git alias # (if you have an alias for this)\n</code></pre></p> </li> <li> <p>Remove an alias: <pre><code>git config --global --unset alias.&lt;name&gt;\n</code></pre></p> </li> </ul>"},{"location":"git/How%20to%20Set%20Up%20Git%20Aliases/#final-thoughts","title":"Final Thoughts","text":"<p>Git aliases can significantly speed up your workflow. Start with simple ones and gradually add more as you discover repetitive tasks.</p>"},{"location":"git/View%20your%20configured%20for%20Git/","title":"View your configured for Git","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Use the following command to view your configured Git for your username: <pre><code>git config user.name\n</code></pre></p>"},{"location":"git/View%20your%20configured%20for%20Git/#additional","title":"\ud83d\udd0d Additional:","text":"<ul> <li>To check global username:     <pre><code>git config --global user.name\n</code></pre></li> <li>To see all Git config values:     <pre><code>git config --list\n</code></pre></li> </ul> Command Purpose <code>git config --list</code> Show all Git config values (from all levels) <code>git config --global user.name \"Your Name\"</code> Set global username <code>git config --global user.email \"you@example.com\"</code> Set global user email <code>git config user.name \"Your Name\"</code> Set local (repo-level) username <code>git config user.email \"you@example.com\"</code> Set local (repo-level) user email <code>git config --global core.editor nano</code> Set default global text editor <code>git config core.editor code</code> Set default local text editor <code>git config --global merge.tool vimdiff</code> Set merge tool globally <code>git config --global diff.tool vimdiff</code> Set diff tool globally <code>git config --global color.ui auto</code> Enable colored output globally <code>git config --system core.autocrlf input</code> Set line ending handling (system-wide) <code>git config --global alias.co checkout</code> Create a shortcut alias (e.g., <code>git co</code> for <code>git checkout</code>) <code>git config --unset user.name</code> Remove local user.name setting <code>git config --unset --global user.email</code> Remove global user.email setting <code>git config --edit</code> Edit local Git config file in editor <code>git config --global --edit</code> Edit global Git config file in editor <code>git config --system --edit</code> Edit system-level Git config file"},{"location":"git/git%20commits%20-%20hacks/","title":"Git commits   hacks","text":""},{"location":"git/git%20commits%20-%20hacks/#to-get-the-list-of-commits-and-authors-for-the-specific-file","title":"to get the list of commits and authors for the specific file:","text":"<pre><code>git log --pretty=format:\"%h %an %ad %s\" --date=short -- src/AmytodoApp/my_index.thml\n</code></pre>"},{"location":"git/git%20commits%20-%20hacks/#to-get-only-the-most-recent-commit-that-modified-the-file-use","title":"To get only the most recent commit that modified the file, use:","text":"<pre><code>git log -1 --pretty=format:\"%ae\" -- src/AmytodoApp/my_index.thml\n\n### Breakdown:\n- `-1` \u2192 limits to **1 latest commit**\n- `--pretty=format:\"...\"` \u2192 custom output with hash, name, email, date, and message\n- `--date=short` \u2192 formats date as `YYYY-MM-DD`\n- `-- &lt;file&gt;` \u2192 restricts to that file only\n</code></pre>"},{"location":"git/git%20commits%20-%20hacks/#to-extract-only-the-author-email-from-that-latest-commit-use","title":"To extract only the author email from that latest commit, use:","text":"<pre><code>git log -1 --pretty=format:\"%ae\" -- src/AmytodoApp/my_index.thml\n</code></pre>"},{"location":"git/git%20commits%20-%20hacks/#output","title":"Output:","text":"<p>Just the email address, like: <pre><code>gowse_1232someemail@gmail.com\n</code></pre></p>"},{"location":"git/git%20commits%20-%20hacks/#automated-script","title":"Automated Script","text":"<pre><code>MODIFIED_FILES_CONTENT=\"\"\nEMAIL_LIST=\"\"\n\nwhile IFS= read -r file; do\n    # Get latest author email for each file\n    EMAIL=$(git log -1 --pretty=format:\"%ae\" -- \"$file\")\n    MODIFIED_FILES_CONTENT+=\"${file} (by ${EMAIL})&lt;br&gt;\"\ndone &lt; modified_files\n\n# Remove trailing &lt;br&gt;\nMODIFIED_FILES_CONTENT=\"${MODIFIED_FILES_CONTENT%&lt;br&gt;}\"\n\nif [[ -n \"$MODIFIED_FILES_CONTENT\" ]]; then\n    sed -i \"s|&gt;RepoChangesNA&lt;|&gt;${MODIFIED_FILES_CONTENT}&lt;|\" /home/gouse/Reports/report.html\nelse\n    sed -i \"s|&gt;RepoChangesNA&lt;|&gt;No Changes&lt;|\" /home/gouse/Reports/report.html\nfi\n</code></pre>"},{"location":"git/git%20errors/","title":"Git errors","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>(.venv) gouse@gouse:~/Documents/homelab$ git push -u origin main\nTo https://github.com/gowseshaik/homelab.git\n ! [rejected]        main -&gt; main (non-fast-forward)\nerror: failed to push some refs to 'https://github.com/gowseshaik/homelab.git'\nhint: Updates were rejected because the tip of your current branch is behind                                                                                                                                               \nhint: its remote counterpart. If you want to integrate the remote changes,\nhint: use 'git pull' before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n</code></pre> <p>You're getting this error because your local <code>main</code> branch is behind the remote <code>main</code> branch \u2014 probably due to new commits on GitHub that your local repo doesn't have.</p>"},{"location":"git/git%20errors/#run-this-to-fix","title":"Run this to fix:","text":"<pre><code>git pull --rebase origin main &amp;&amp; git push -u origin main\n</code></pre>"},{"location":"git/git%20errors/#explanation","title":"Explanation:","text":"<ul> <li><code>git pull --rebase origin main</code>: Fetches and applies remote changes before your local commits.</li> <li><code>git push -u origin main</code>: Pushes your changes after syncing with remote.</li> </ul> <p>\u26a0\ufe0f If you have uncommitted changes, stash or commit them before running this to avoid conflicts.</p> <pre><code>(.venv) gouse@gouse:~/Documents/homelab/docs$ git push -u origin main\nTo https://github.com/gowseshaik/homelab.git\n ! [rejected]        main -&gt; main (non-fast-forward)\nerror: failed to push some refs to 'https://github.com/gowseshaik/homelab.git'\nhint: Updates were rejected because a pushed branch tip is behind its remote                                                                                                                                               \nhint: counterpart. If you want to integrate the remote changes, use 'git pull'\nhint: before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n(.venv) gouse@gouse:~/Documents/homelab/docs$ git branch\n* (no branch, rebasing main)\n  gh-pages\n  main\n(.venv) gouse@gouse:~/Documents/homelab/docs$ cd ..\n(.venv) gouse@gouse:~/Documents/homelab$ git branch\n* (no branch, rebasing main)\n  gh-pages\n  main\n(.venv) gouse@gouse:~/Documents/homelab$ git checkout barnch main\nerror: pathspec 'barnch' did not match any file(s) known to git\nerror: pathspec 'main' did not match any file(s) known to git\n(.venv) gouse@gouse:~/Documents/homelab$ git branch\n* (no branch, rebasing main)\n  gh-pages\n  main\n(.venv) gouse@gouse:~/Documents/homelab$ \n</code></pre> <p>You're currently stuck in a rebase state, which is why: - Your current branch is shown as <code>* (no branch, rebasing main)</code> - You can\u2019t switch branches or push.</p>"},{"location":"git/git%20errors/#fix-it-step-by-step","title":"\ud83d\udd27 Fix it step-by-step:","text":""},{"location":"git/git%20errors/#1-abort-the-rebase","title":"\u2705 1. Abort the rebase","text":"<pre><code>git rebase --abort\n\n(.venv) gouse@gouse:~/Documents/homelab$ git rebase --abort\n(.venv) gouse@gouse:~/Documents/homelab$ git branch\n  gh-pages\n* main\n\n(.venv) gouse@gouse:~/Documents/homelab$ git status\nOn branch main\nYour branch and 'origin/main' have diverged,\nand have 2 and 8 different commits each, respectively.\n  (use \"git pull\" if you want to integrate the remote branch with yours)\n\nnothing to commit, working tree clean\n</code></pre>"},{"location":"git/git%20errors/#2-switch-to-main-branch","title":"\u2705 2. Switch to <code>main</code> branch","text":"<pre><code>git checkout main\n\n(.venv) gouse@gouse:~/Documents/homelab$ git checkout main\nAlready on 'main'\nYour branch and 'origin/main' have diverged,\nand have 2 and 8 different commits each, respectively.\n  (use \"git pull\" if you want to integrate the remote branch with yours)\n</code></pre>"},{"location":"git/git%20errors/#3-pull-remote-changes-and-rebase","title":"\u2705 3. Pull remote changes and rebase","text":"<pre><code>git pull --rebase origin main\n\n(.venv) gouse@gouse:~/Documents/homelab$ git pull --rebase origin main\nerror: cannot pull with rebase: You have unstaged changes.\nerror: Please commit or stash them.\n\n#If you want to **merge** remote changes:\ngit pull --no-rebase\n\n# If you want to **rebase** your local commits on top of remote:\ngit pull --rebase\n\nIf you want to make it permanent for this repo:\n# For **merge (default)**:\ngit config pull.rebase false\n\n# For **rebase (recommended for clean history)**:\ngit config pull.rebase true\n</code></pre>"},{"location":"git/git%20errors/#4-push-your-changes","title":"\u2705 4. Push your changes","text":"<pre><code>git push -u origin main\n</code></pre>"},{"location":"git/git%20errors/#5-pull-from-remote-to-sync-local-and-push-your-changes","title":"\u2705 5. Pull from remote to sync local and Push your changes","text":"<p>Run this single command to resolve it: <pre><code>git add . &amp;&amp; git stash &amp;&amp; git pull --rebase origin main &amp;&amp; git stash pop\n</code></pre></p>"},{"location":"git/git%20errors/#what-each-does","title":"\ud83d\udd0d What each does:","text":"<ul> <li><code>git add .</code> \u2192 stages all changes (to avoid stash errors on untracked files).</li> <li><code>git stash</code> \u2192 saves local changes temporarily.</li> <li><code>git pull --rebase origin main</code> \u2192 pulls remote changes cleanly.</li> <li><code>git stash pop</code> \u2192 restores your local changes back on top.</li> </ul>"},{"location":"git/git%20errors/#5-to-discard-all-local-unstaged-changes-and-pull","title":"\u2705 5. To discard all local unstaged changes and pull","text":"<p>If you want to discard all local unstaged changes and pull with rebase, run: <pre><code>git reset --hard &amp;&amp; git clean -fd &amp;&amp; git pull --rebase origin main\n</code></pre></p>"},{"location":"git/git%20errors/#explanation_1","title":"\ud83d\udd0d Explanation:","text":"<ul> <li><code>git reset --hard</code> \u2192 discards all tracked file changes.</li> <li><code>git clean -fd</code> \u2192 removes untracked files and directories.</li> <li><code>git pull --rebase origin main</code> \u2192 pulls latest changes with rebase.</li> </ul> <p>\u26a0\ufe0f Warning: This will delete all your local changes permanently.</p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/","title":"Steps to deploy MailHog with Ingress","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Below are the full steps to deploy MailHog with Ingress via Traefik on a k3d cluster using port 80.</p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-1-create-k3d-cluster-with-port-80-mapped","title":"\u2705 Step 1: Create <code>k3d</code> cluster with port 80 mapped","text":"<pre><code># file: k3d-mailhog.yaml\napiVersion: k3d.io/v1alpha4\nkind: Simple\nname: mailhog-cluster\nports:\n  - port: 80:80      # maps host port 80 to container port 80\n    nodeFilters:\n      - loadbalancer\n</code></pre> <pre><code>k3d cluster create --config k3d-mailhog.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-2-deploy-traefik-if-not-already","title":"\u2705 Step 2: Deploy Traefik (if not already)","text":"<p>Traefik is installed by default with <code>k3d</code>. Check with: <pre><code>kubectl get pods -n kube-system | grep traefik\n</code></pre> If not, install manually via Helm or manifest.</p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-3-create-mailhog-deployment-and-service","title":"\u2705 Step 3: Create MailHog Deployment and Service","text":"<pre><code># file: mailhog-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mailhog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mailhog\n  template:\n    metadata:\n      labels:\n        app: mailhog\n    spec:\n      containers:\n      - name: mailhog\n        image: mailhog/mailhog\n        ports:\n        - containerPort: 1025\n        - containerPort: 8025 \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mailhog\nspec:\n  selector:\n    app: mailhog\n  ports:\n  - name: smtp\n    port: 1025\n    targetPort: 1025\n  - name: http\n    port: 8025  # i used 80, becuase i used 80 in k3d config\n    targetPort: 8025\n</code></pre> <pre><code>kubectl apply -f mailhog-deployment.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-4-create-ingress-for-traefik","title":"\u2705 Step 4: Create Ingress for Traefik","text":"<pre><code># file: mailhog-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mailhog\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  rules:\n  - host: mailhog.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: mailhog\n            port:\n              number: 8025  # i used 80, becuase i used 80 in k3d config\n</code></pre> <pre><code>kubectl apply -f mailhog-ingress.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-5-add-local-dns-for-mailhoglocal","title":"\u2705 Step 5: Add local DNS for <code>mailhog.local</code>","text":"<p>Edit <code>/etc/hosts</code>: <pre><code>127.0.0.1 mailhog.local\n</code></pre></p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-6-access-mailhog","title":"\u2705 Step 6: Access MailHog","text":"<p>Open in browser: <code>http://mailhog.local</code> \u2192 Web UI SMTP will be available at <code>mailhog.local:1025</code> (for apps)</p>"},{"location":"mailhog/Usecase%20of%20mailhog/","title":"Usecase of mailhog","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>No, MailHog does not send emails to real recipients.</p> <p>It acts as a fake SMTP server that captures emails locally for testing and debugging.</p> <p>You can: - Send emails to MailHog from your app - View those emails in MailHog\u2019s web UI - Inspect content, headers, attachments - Resend emails within MailHog (to test again internally)</p> <p>But MailHog won\u2019t deliver emails to external addresses. It\u2019s only for local/dev use.</p> <p>If you want to send real emails, you must configure your app to use a real SMTP service (e.g., Gmail SMTP, SendGrid, AWS SES).</p>"},{"location":"mkdocs/mkdocs%20to%20github%20projects/","title":"Mkdocs to github projects","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>$ python3 -m venv handsondevops\n$ source ./handsondevops/bin/activate\n$ pip install mkdocs\n$ pip install mkdocs-material\n$ pip install mkdocs-exclude # to exclude .obsidian directory\n\u2705 Other files/folders will remain unaffected. Only `.obsidian` is ignored in the build.\n\n\n# Stop tracking the folder\ngit rm -r --cached .obsidian\n\n# Commit the change\ngit commit -m \"Remove .obsidian directory from repo\"\n\n# Push to GitHub\ngit push origin main\n\n$ echo \".obsidian/\" &gt;&gt; .gitignore\n$ git add .\n$ git commit -am \"add comment\"\n$ git push -u origin main\n$ mkdocs gh-deploy --clean\n</code></pre> <p>\ud83e\udde9 Popular Themes</p> Theme Use with <code>mkdocs</code> Default <code>readthedocs</code> Lightweight <code>material</code> <code>pip install mkdocs-material</code> <p>\ud83d\udce6 Common Commands</p> Command Description <code>mkdocs new my-project</code> Create new MkDocs project <code>mkdocs serve</code> Run dev server at http://127.0.0.1:8000 <code>mkdocs build</code> Build static site to <code>site/</code> dir <code>mkdocs gh-deploy</code> Deploy to GitHub Pages \ud83d\udcc1 Project Structure <pre><code>my-project/\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 about.md\n\u251c\u2500\u2500 mkdocs.yml\n</code></pre> <p>\ud83d\udcc4 mkdocs.yml (basic) <pre><code>site_name: My Docs\nnav:\n  - Home: index.md\n  - About: about.md\ntheme: readthedocs\n</code></pre></p> <p>example :  mkdocs.yml  <pre><code>site_name: Gouse Shaik  \nsite_url: https://github.com/username/repo  \nrepo_url: https://github.com/username/repo \nrepo_name: RepoName  \n\ntheme:  \n  name: material  \n  logo: images/logo-design-4.png  \n  favicon: images/favicon.ico  \n  features:  \n    - navigation.instant  \n    - navigation.top  \n    - navigation.sections  \n    - navigation.expand  \n    - content.code.copy  \n    - content.tabs.link  \n    - toc.integrate  \n    - content.action.edit  \n    - content.tooltips  \n  palette:  \n    - scheme: default  \n      primary: cyan  \n      accent: teal  \n      toggle:  \n        icon: material/weather-night  \n        name: Switch to dark mode  \n    - scheme: slate  \n      primary: cyan  \n      accent: teal  \n      toggle:  \n        icon: material/weather-sunny  \n        name: Switch to light mode  \n  font:  \n    text: \"Patrick Hand\"  \n    code: \"JetBrains Mono\"  \n  icon:  \n    repo: fontawesome/brands/github  \n  extra_css:  \n    - extra.css  \n\nmarkdown_extensions:  \n  - admonition  \n  - toc:  \n      permalink: true  \n  - codehilite  \n  - footnotes  \n  - tables  \n  - pymdownx.superfences  \n  - pymdownx.tabbed  \n  - pymdownx.details  \n  - pymdownx.emoji:  \n      emoji_generator: !!python/name:pymdownx.emoji.to_svg  \n\nplugins:  \n  - search  \n\nnav:  \n  - Home: index.md  \n  - About: about.md\n\nexclude:\n  - .obsidian/*\n</code></pre></p>"},{"location":"mkdocs/mkdocs%20to%20github%20projects/#python-script-to-generate-before-pushing-to-github","title":"python script to generate before pushing to Github","text":"<pre><code>import os  \nfrom pathlib import Path  \n\ndocs_dir = \"docs\"  \noutput_file = \"docs/index.md\"  \n\n\ndef generate_index():  \n    content = \"# Knowledge Base\\n\\n\"  \n\n    for topic in sorted(os.listdir(docs_dir)):  \n        topic_path = os.path.join(docs_dir, topic)  \n        if os.path.isdir(topic_path) and topic != \"assets\":  \n            content += f\"## {topic.capitalize()}\\n\"  \n            for file in sorted(os.listdir(topic_path)):  \n                if file.endswith(\".md\"):  \n                    file_name = file[:-3]  # Remove '.md'  \n                    content += f\"- [{file_name}]({topic}/{file})\\n\"  \n            content += \"\\n\"  \n\n    with open(output_file, \"w\") as f:  \n        f.write(content)  \n\n\nif __name__ == \"__main__\":  \n    generate_index()\n\n-----------\n# Updated for .obsidian\n\nimport os  \nfrom pathlib import Path  \n\ndocs_dir = \"docs\"  \noutput_file = \"docs/index.md\"  \n\n# Directories to exclude (add more if needed)  \nEXCLUDE_DIRS = {\".obsidian\", \"assets\"}  \n\n\ndef generate_index():  \n    content = \"# Knowledge Base\\n\\n\"  \n\n    for topic in sorted(os.listdir(docs_dir)):  \n        topic_path = os.path.join(docs_dir, topic)  \n\n        # Skip excluded directories and non-directories  \n        if not os.path.isdir(topic_path) or topic in EXCLUDE_DIRS:  \n            continue  \n\n        content += f\"## {topic.capitalize()}\\n\"  \n        for file in sorted(os.listdir(topic_path)):  \n            if file.endswith(\".md\"):  \n                file_name = file[:-3]  # Remove '.md'  \n                content += f\"- [{file_name}]({topic}/{file})\\n\"  \n        content += \"\\n\"  \n\n    # Ensure output directory exists  \n    Path(output_file).parent.mkdir(exist_ok=True)  \n\n    with open(output_file, \"w\") as f:  \n        f.write(content)  \n\n\nif __name__ == \"__main__\":  \n    generate_index()\n</code></pre>"},{"location":"mkdocs/plugins%20list%20used%20with%20MkDocs-material/","title":"plugins list used with MkDocs material","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> Plugin Name Install Command Purpose / Notes <code>search</code> (built-in) \u2014 Enables search functionality <code>minify</code> <code>pip install mkdocs-minify-plugin</code> Minifies HTML for faster loading <code>git-revision-date</code> <code>pip install mkdocs-git-revision-date-plugin</code> Shows last modified date per page <code>git-authors</code> <code>pip install mkdocs-git-authors-plugin</code> Adds Git contributors to pages <code>awesome-pages</code> <code>pip install mkdocs-awesome-pages-plugin</code> Allows sorting/nav customization via <code>.pages</code> files <code>macros</code> <code>pip install mkdocs-macros-plugin</code> Use Python macros in Markdown <code>include-markdown</code> <code>pip install mkdocs-include-markdown-plugin</code> Include .md or .txt content in your pages <code>redirects</code> <code>pip install mkdocs-redirects</code> Set up redirects between pages <code>i18n</code> <code>pip install mkdocs-static-i18n</code> Adds multilingual support <code>tags</code> <code>pip install mkdocs-plugin-tags</code> Adds tagging support for pages <code>section-index</code> <code>pip install mkdocs-section-index</code> Enables folder-based landing pages <code>rss</code> <code>pip install mkdocs-rss-plugin</code> Adds RSS feed generation <code>pdf-export</code> <code>pip install mkdocs-pdf-export-plugin</code> Allows exporting site as PDF <p>and you can include these plugins in <code>mkdocs.yml</code> file</p>"},{"location":"multipass/1.%20k8s%20hardway/","title":"1. k8s hardway","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>#!/bin/bash\n# Step 1: Create Multipass VMs\n\n# Master Nodes\nmultipass launch -n k8s-master-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch -n k8s-master-2 --cpus 2 --mem 2G --disk 10G\n\n# Worker Nodes\nmultipass launch -n k8s-worker-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch -n k8s-worker-2 --cpus 2 --mem 2G --disk 10G\n\n# Load Balancer\nmultipass launch -n k8s-lb --cpus 1 --mem 1G --disk 5G\n\n# Step 2: Get IPs\nMASTER1_IP=$(multipass info k8s-master-1 | grep IPv4 | awk '{print $2}')\nMASTER2_IP=$(multipass info k8s-master-2 | grep IPv4 | awk '{print $2}')\nWORKER1_IP=$(multipass info k8s-worker-1 | grep IPv4 | awk '{print $2}')\nWORKER2_IP=$(multipass info k8s-worker-2 | grep IPv4 | awk '{print $2}')\nLB_IP=$(multipass info k8s-lb | grep IPv4 | awk '{print $2}')\n\n# Step 3: Install Prerequisites on all nodes (Docker, curl, etc.)\nfor NODE in k8s-master-1 k8s-master-2 k8s-worker-1 k8s-worker-2 k8s-lb; do\n  multipass exec $NODE -- bash -c '\n    sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y docker.io curl wget vim\n  '\ndone\n\n# Step 4: Download Kubernetes Binaries (cfssl, kubectl, etc.) on host\nmkdir -p ./kthw-bin\ncd ./kthw-bin\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-apiserver\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-controller-manager\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-scheduler\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubelet\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-proxy\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubectl\ncurl -LO https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -LO https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x *\n\n# Step 5: Generate CA and TLS certs using cfssl\n# (Separate script will be generated)\n\n# Step 6: Configure etcd cluster on both master nodes\n# (Separate script will be generated)\n\n# Step 7: Configure Kubernetes control plane static pods\n# (Separate script will be generated)\n\n# Step 8: Configure kubelet, kube-proxy on worker nodes\n# (Separate script will be generated)\n\n# Step 9: Setup Load Balancer using HAProxy or nginx\n# (Separate script will be generated)\n\n# Step 10: Validate cluster using kubectl\n# (Separate script will be generated)\n\ncd ..\necho \"Initial VMs and base setup done. Next: generate certs and etcd config.\"\n</code></pre> <pre><code>#!/bin/bash\n# Step 1: Create Multipass VMs\n\n# Master Nodes\nmultipass launch -n k8s-master-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch -n k8s-master-2 --cpus 2 --mem 2G --disk 10G\n\n# Worker Nodes\nmultipass launch -n k8s-worker-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch -n k8s-worker-2 --cpus 2 --mem 2G --disk 10G\n\n# Load Balancer\nmultipass launch -n k8s-lb --cpus 1 --mem 1G --disk 5G\n\n# Step 2: Get IPs\nMASTER1_IP=$(multipass info k8s-master-1 | grep IPv4 | awk '{print $2}')\nMASTER2_IP=$(multipass info k8s-master-2 | grep IPv4 | awk '{print $2}')\nWORKER1_IP=$(multipass info k8s-worker-1 | grep IPv4 | awk '{print $2}')\nWORKER2_IP=$(multipass info k8s-worker-2 | grep IPv4 | awk '{print $2}')\nLB_IP=$(multipass info k8s-lb | grep IPv4 | awk '{print $2}')\n\n# Step 3: Install Prerequisites on all nodes (Docker, curl, etc.)\nfor NODE in k8s-master-1 k8s-master-2 k8s-worker-1 k8s-worker-2 k8s-lb; do\n  multipass exec $NODE -- bash -c '\n    sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y docker.io curl wget vim\n  '\ndone\n\n# Step 4: Download Kubernetes Binaries (cfssl, kubectl, etc.) on host\nmkdir -p ./kthw-bin\ncd ./kthw-bin\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-apiserver\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-controller-manager\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-scheduler\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubelet\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-proxy\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubectl\ncurl -LO https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -LO https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x *\n\ncd ..\n\n# Step 5: Generate CA and TLS certs using cfssl\n# Prepare cfssl JSON files and generate certs (to be executed on local machine)\n# cfssl gencert -initca ca-csr.json | cfssljson -bare ca\n# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem ... (repeat for each component)\n\n# Step 6: Configure etcd cluster on both master nodes\n# Copy etcd binaries and certs to each master, then start etcd using systemd or static pod\n# Configure etcd to listen on MASTER1_IP and MASTER2_IP with peer URLs\n\n# Step 7: Configure Kubernetes control plane static pods\n# Create manifests for kube-apiserver, controller-manager, scheduler\n# Set --advertise-address, --bind-address, --etcd-servers, --secure-port\n\n# Step 8: Configure kubelet, kube-proxy on worker nodes\n# Copy kubelet/kube-proxy certs and kubeconfig files to workers\n# Create kubelet systemd unit files and kube-proxy config files\n\n# Step 9: Setup Load Balancer using HAProxy or nginx\n# On k8s-lb node, install HAProxy and configure to forward port 6443 to both masters\n# Example:\n# frontend kubernetes\n#   bind *:6443\n#   default_backend k8s-masters\n# backend k8s-masters\n#   balance roundrobin\n#   server master1 MASTER1_IP:6443 check\n#   server master2 MASTER2_IP:6443 check\n\n# Step 10: Validate cluster using kubectl\n# Export kubeconfig and use kubectl get nodes, kubectl get cs\n# You should see 2 masters, 2 workers\n\necho \"Initial setup complete. Follow next steps to generate certs, configure etcd, and deploy static pods.\"\n</code></pre>"},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/","title":"Setup access your cluster from the host machine","text":"<p>Created: 2025-07-06 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>Follow these steps:</p>"},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/#steps-to-access-k3s-cluster-from-host","title":"\u2705 Steps to Access K3s Cluster from Host","text":""},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/#1-copy-kubeconfig-from-the-primary-node","title":"1. Copy kubeconfig from the primary node","text":"<p>You already did this in the script: <pre><code>multipass exec cp1 -- sudo cat /etc/rancher/k3s/k3s.yaml &gt; kubeconfig\n\n# for kubeadm\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u) $(id -g) $HOME/.kube/config I\n</code></pre></p>"},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/#2-update-server-ip-in-kubeconfig","title":"2. Update server IP in kubeconfig","text":"<p>Replace <code>127.0.0.1</code> with the actual IP of <code>cp1</code>: <pre><code>CP1_IP=$(multipass info cp1 --format json | jq -r '.info[\"cp1\"].ipv4[0]')\nsed -i \"s/127.0.0.1/${CP1_IP}/g\" kubeconfig\n</code></pre></p>"},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/#3-set-kubeconfig-environment-variable","title":"3. Set KUBECONFIG environment variable","text":"<pre><code>#its a customized kube config file\nexport KUBECONFIG=$PWD/kubeconfig\n\nif its in home directory, then use as below\nexport KUBECONFIG=\"$HOME/.kube/config\"\n</code></pre>"},{"location":"multipass/Setup%20access%20your%20cluster%20from%20the%20host%20machine/#4-test-access-from-host","title":"4. Test access from host","text":"<pre><code>kubectl get nodes\n</code></pre> <p>You should see: <pre><code>NAME      STATUS   ROLES                  AGE   VERSION\ncp1       Ready    control-plane,master   5m    v1.xx.x\ncp2       Ready    control-plane,master   4m    v1.xx.x\nworker1   Ready    &lt;none&gt;                 3m    v1.xx.x\nworker2   Ready    &lt;none&gt;                 3m    v1.xx.x\n</code></pre></p> <p>\u2705 Done \u2014 you're now accessing the HA cluster from your Ubuntu host directly.</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/","title":"To access ports from the host machine running multipass","text":"<p>Created: 2025-06-29 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>sudo apt update\nsudo apt install ufw\nIf UFW is showing as \"inactive,\" you need to enable it before allowing any ports. Follow these steps:\nsudo ufw status\nsudo ufw enable\n\nsudo ufw allow 3389/tcp\nsudo ufw allow 8080/tcp\nsudo ufw allow 80/tcp\nsudo ufw allow 4000/tcp\nsudo ufw allow 5601/tcp # for kibana\nsudo ufw allow 9200 # for elasticsearch\n\nIf you don't want to use UFW and prefer using `iptables`, you can allow the port with:\n\nsudo iptables -A INPUT -p tcp --dport 3389 -j ACCEPT\n\nDon't forget to save the `iptables` rules to make sure they persist after a reboot:\n\nsudo apt install iptables-persistent\nsudo netfilter-persistent save\n</code></pre>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#now-port-forwarding-to-multipass-vms-with-nginx","title":"Now port forwarding to Multipass VMs with nginx","text":"<p>Yes \u2014 if you don\u2019t want to use <code>socat</code> with <code>nohup</code>, here are cleaner and better alternatives to forward port 5601 to <code>localhost</code>:</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#option-1-use-systemd-service-for-socat-cleaner-than-nohup","title":"\u2705 Option 1: Use <code>systemd</code> service for socat (cleaner than <code>nohup</code>)","text":""},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#create-systemd-service-on-host","title":"\ud83d\udd27 Create systemd service on host:","text":"<pre><code>sudo tee /etc/systemd/system/socat-kibana.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=Port forward Kibana from Multipass VM to localhost:5601\nAfter=network.target\n\n[Service]\nExecStart=/usr/bin/socat TCP-LISTEN:5601,fork TCP:$(multipass info elk-vm | grep IPv4 | awk '{print $2}'):5601\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <pre><code># Enable and start\nsudo systemctl daemon-reexec\nsudo systemctl daemon-reload\nsudo systemctl enable --now socat-kibana.service\n</code></pre> <p>Now, Kibana is always available at <code>localhost:5601</code> and runs cleanly as a background service.</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#option-2-use-nginx-reverse-proxy-on-host-no-socat","title":"\u2705 Option 2: Use <code>nginx</code> reverse proxy on host (No socat)","text":"<ol> <li> <p>Install nginx on host: <pre><code>sudo apt install nginx\n</code></pre></p> </li> <li> <p>Create reverse proxy config: <pre><code>sudo tee /etc/nginx/sites-available/kibana &gt; /dev/null &lt;&lt;EOF\nserver {\n    listen 5601;\n    location / {\n        proxy_pass http://$(multipass info elk-vm | grep IPv4 | awk '{print $2}'):5601;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade \\$http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host \\$host;\n        proxy_cache_bypass \\$http_upgrade;\n    }\n}\nEOF\n</code></pre></p> </li> </ol> <pre><code>sudo tee /etc/nginx/sites-available/elasticsearch &gt; /dev/null &lt;&lt;EOF\nserver {\n    listen 9200;\n    location / {\n        proxy_pass http://192.168.64.5:9200;\n        proxy_http_version 1.1;\n        proxy_set_header Host \\$host;\n        proxy_set_header X-Real-IP \\$remote_addr;\n    }\n}\nEOF\n</code></pre> <ol> <li>Enable site and reload nginx: <pre><code>sudo ln -s /etc/nginx/sites-available/kibana /etc/nginx/sites-enabled/\nsudo ln -s /etc/nginx/sites-available/elasticsearch /etc/nginx/sites-enabled/\nsudo nginx -t &amp;&amp; sudo systemctl reload nginx\n</code></pre></li> </ol> <p>\u2705 Now open <code>http://localhost:5601</code> \u2014 it will reverse proxy to the Kibana inside the VM.</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#recommendation","title":"\u2705 Recommendation","text":"Method Use Case Notes <code>systemd + socat</code> You want clean auto-starting service Better than <code>nohup</code> <code>nginx</code> You prefer proxy and load balancing No <code>socat</code> needed <code>ssh -L</code> You want temporary port tunnel Manual each time <p>Let me know which one you want to keep \u2014 I can give cleanup steps for the others.</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#port-foward-with-socat","title":"Port foward with socat","text":""},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#basic-port-forward-host-vm-kibana","title":"\u2705 Basic Port Forward: Host \u2192 VM (Kibana)","text":"<pre><code>sudo socat TCP-LISTEN:5601,fork TCP:&lt;vm-ip&gt;:5601\n</code></pre> <p>Forwards localhost:5601 on host to <code>elk-vm:5601</code></p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#run-in-background-useful-for-long-running","title":"\u23f3 Run in background (useful for long running)","text":"<pre><code>sudo nohup socat TCP-LISTEN:5601,fork TCP:&lt;vm-ip&gt;:5601 &amp;\n</code></pre> <p>Runs <code>socat</code> in background using <code>nohup</code></p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#kill-existing-socat-port-forward-if-stuck","title":"\ud83e\uddfc Kill existing socat port forward (if stuck)","text":"<pre><code>sudo pkill -f \"socat.*5601\"\n</code></pre> <p>Stops any <code>socat</code> process handling port 5601</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#test-port-forward-locally","title":"\ud83e\uddea Test port forward locally","text":"<pre><code>curl http://localhost:5601\n</code></pre> <p>This will now hit the VM's Kibana</p>"},{"location":"multipass/To%20access%20ports%20from%20the%20host%20machine%20running%20multipass/#replace-vm-ip-using-command","title":"\ud83d\udcdd Replace <code>&lt;vm-ip&gt;</code> using command:","text":"<pre><code>multipass info elk-vm | grep IPv4 | awk '{print $2}'\n</code></pre>"},{"location":"multipass/k3s%20with%20HA%20in%20multipass/","title":"k3s with HA in multipass","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>A complete Multipass script to set up a Kubernetes HA cluster using <code>k3s</code>, with:</p> <ul> <li>2 control plane nodes</li> <li>2 worker nodes </li> <li>Embedded etcd (HA enabled)</li> </ul> <p>All inside Multipass VMs, suitable for your system (15\u202fGiB RAM).</p>"},{"location":"multipass/k3s%20with%20HA%20in%20multipass/#script-k3s_ha_multipasssh","title":"\u2705 Script: <code>k3s_ha_multipass.sh</code>","text":"<pre><code>#!/bin/bash\n\n# Node specs\nCPUS=2\nDISK=10G\nMEM_CP=2G\nMEM_WORKER=2G\n\n# Node names\nCONTROL_NODES=(cp1 cp2)\nWORKER_NODES=(worker1 worker2)\nALL_NODES=(\"${CONTROL_NODES[@]}\" \"${WORKER_NODES[@]}\")\n\n# 1. Launch VMs\nfor NODE in \"${ALL_NODES[@]}\"; do\n  multipass launch --name \"$NODE\" --cpus $CPUS --mem ${MEM_CP} --disk $DISK\ndone\n\n# 2. Get IPs\nfor NODE in \"${ALL_NODES[@]}\"; do\n  IP=$(multipass info \"$NODE\" --format json | jq -r \".info[\\\"$NODE\\\"].ipv4[0]\")\n  echo \"$NODE IP: $IP\"\ndone\n\n# 3. Install k3s on first control-plane (cp1)\nCP1_IP=$(multipass info cp1 --format json | jq -r '.info[\"cp1\"].ipv4[0]')\nmultipass exec cp1 -- bash -c \"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='server --cluster-init' sh -\"\n\n# 4. Extract token from cp1\nTOKEN=$(multipass exec cp1 sudo cat /var/lib/rancher/k3s/server/node-token)\n\n# 5. Join cp2 to HA cluster\nmultipass exec cp2 -- bash -c \"curl -sfL https://get.k3s.io | K3S_URL=https://${CP1_IP}:6443 K3S_TOKEN=${TOKEN} INSTALL_K3S_EXEC='server' sh -\"\n\n# 6. Join worker1 and worker2\nfor WORKER in \"${WORKER_NODES[@]}\"; do\n  multipass exec $WORKER -- bash -c \"curl -sfL https://get.k3s.io | K3S_URL=https://${CP1_IP}:6443 K3S_TOKEN=${TOKEN} sh -\"\ndone\n\n# 7. Fetch kubeconfig to local machine\nmultipass exec cp1 -- sudo cat /etc/rancher/k3s/k3s.yaml &gt; kubeconfig\nsed -i \"s/127.0.0.1/${CP1_IP}/g\" kubeconfig\necho \"Kubeconfig saved to ./kubeconfig\"\n\n# 8. Done\necho -e \"\\n\u2705 K3s HA cluster setup complete!\"\n</code></pre>"},{"location":"multipass/k3s%20with%20HA%20in%20multipass/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li>Assumes you have <code>multipass</code>, <code>jq</code>, and <code>curl</code> installed.</li> <li>Exposes kubeconfig to access the cluster from your local host.</li> <li>Uses embedded etcd (HA mode).</li> <li> <p>Avoids external load balancer for simplicity (multi-master access via IP).</p> </li> <li> <p>MetalLB for load balancing</p> </li> <li>ArgoCD / Prometheus on top</li> <li>NGINX ingress setup</li> </ul>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/","title":"K8s with hard way   overview","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#why-hardway","title":"Why Hardway","text":"<p>Hard disagree because it depends on your goals. Kubernetes the hard way will help you troubleshoot some weird problems with k8s.</p> <p>\ud83d\udd0d Core Focus Areas</p> Area Why It Matters TLS &amp; Certificate Management Learn how security is handled (e.g., etcd, kubelet, apiserver) etcd Cluster Setup Understand how Kubernetes stores all its state and how consensus works Control Plane Setup Learn how to manually start and configure <code>kube-apiserver</code>, <code>controller-manager</code>, <code>scheduler</code> Kubelet + kube-proxy Setup Understand how worker nodes connect to the control plane Kubeconfig Files Learn how each component authenticates and communicates RBAC Configuration Learn how access control and API authz work in K8s Networking (Pod CIDRs, Services, DNS) Understand pod-to-pod networking and kube-proxy routing Static Pod Manifest Usage Learn how to run pods without a scheduler on control plane Manual Cluster Bootstrapping Learn how each component is brought up and talks to the rest to install"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#goal-of-kubernetes-the-hard-way","title":"\ud83d\udca1 Goal of Kubernetes the Hard Way","text":"Goal Description \u2705 Learn by Doing You manually install and configure every component \u2705 Understand Internals See what tools like <code>kubeadm</code>, <code>k3s</code>, or <code>kind</code> actually do behind the scenes \u2705 Debugging Skills You\u2019ll learn how to debug TLS, kubelet, or API failures manually"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#what-not-to-focus-on","title":"\u274c What Not to Focus On","text":"<ul> <li>No need to use <code>kubeadm</code>, Helm, or Operators</li> <li>Avoid dashboards, metrics, or cloud integrations</li> <li>Don\u2019t try to productionize it \u2014 this is a learning/lab setup</li> </ul> <p>Yes, you can install Kubernetes the Hard Way using Multipass, and it works well for local lab setups.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#why-use-multipass","title":"\u2705 Why use Multipass?","text":"<ul> <li>Lightweight Ubuntu VMs</li> <li>Easy to create/clone/destroy nodes</li> <li>Simulates cloud VMs (Ubuntu LTS)</li> <li>No nested virtualization needed (unlike Vagrant/VMware)</li> </ul>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#sample-setup-multipass-vm-layout","title":"\ud83e\uddf1 Sample Setup (Multipass VM Layout)","text":"VM Name Role CPU RAM <code>k8s-lb</code> Load Balancer 1 1GB <code>k8s-master-1</code> Controller 2 2GB <code>k8s-master-2</code> Controller 2 2GB <code>k8s-worker-1</code> Worker 2 2GB <code>k8s-worker-2</code> Worker 2 2GB"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#step-by-step-with-multipass","title":"\u2699\ufe0f Step-by-Step with Multipass","text":""},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#1-launch-vms","title":"1. Launch VMs","text":"<pre><code>multipass launch --name k8s-lb       --cpus 1 --mem 1G --disk 5G\nmultipass launch --name k8s-master-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch --name k8s-master-2 --cpus 2 --mem 2G --disk 10G\nmultipass launch --name k8s-worker-1 --cpus 2 --mem 2G --disk 10G\nmultipass launch --name k8s-worker-2 --cpus 2 --mem 2G --disk 10G\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#2-get-ips","title":"2. Get IPs","text":"<pre><code>multipass list\n</code></pre> <p>Save the IPs for <code>/etc/hosts</code> and for load balancer config.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>SSH into each node:</p> <pre><code>multipass shell k8s-master-1\n</code></pre> <p>Install:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y socat conntrack ipset iptables curl\n</code></pre> <p>Repeat for all nodes.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#4-clone-and-follow-kubernetes-the-hard-way","title":"4. Clone and Follow Kubernetes the Hard Way","text":"<p>From your host:</p> <pre><code>git clone https://github.com/kelseyhightower/kubernetes-the-hard-way\ncd kubernetes-the-hard-way\n</code></pre> <p>Then manually execute the steps from that repo, using <code>multipass exec &lt;vm-name&gt; -- &lt;command&gt;</code> or <code>multipass shell</code>.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#tip","title":"\ud83d\udcdd Tip","text":"<p>You can create a <code>cloud-init</code> YAML or small setup scripts to automate cert generation or configs. Let me know if you want an example script for this.</p> <p>Here's a shell script to auto-create and prep VMs for Kubernetes The Hard Way using Multipass:</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#kthw_multipass_setupsh","title":"\ud83d\udd27 <code>kthw_multipass_setup.sh</code>","text":"<pre><code>#!/bin/bash\n\n# Node specs\nMASTER_COUNT=2\nWORKER_COUNT=2\n\n# Create load balancer node\nmultipass launch --name k8s-lb --cpus 1 --mem 1G --disk 5G\n\n# Create controller nodes\nfor i in $(seq 1 $MASTER_COUNT); do\n  multipass launch --name k8s-master-${i} --cpus 2 --mem 2G --disk 10G\ndone\n\n# Create worker nodes\nfor i in $(seq 1 $WORKER_COUNT); do\n  multipass launch --name k8s-worker-${i} --cpus 2 --mem 2G --disk 10G\ndone\n\n# Update all nodes and install dependencies\nALL_NODES=(k8s-lb)\nfor i in $(seq 1 $MASTER_COUNT); do\n  ALL_NODES+=(k8s-master-${i})\ndone\nfor i in $(seq 1 $WORKER_COUNT); do\n  ALL_NODES+=(k8s-worker-${i})\ndone\n\necho \"\ud83d\udce6 Installing dependencies on all nodes...\"\nfor NODE in \"${ALL_NODES[@]}\"; do\n  echo \"\u27a1\ufe0f  ${NODE}\"\n  multipass exec $NODE -- bash -c \"sudo apt update &amp;&amp; sudo apt install -y socat conntrack ipset iptables curl\"\ndone\n\n# Get IPs and print them\necho \"\"\necho \"\ud83c\udf10 IP Addresses:\"\nfor NODE in \"${ALL_NODES[@]}\"; do\n  IP=$(multipass info $NODE | grep IPv4 | awk '{print $2}')\n  printf \"%-15s : %s\\n\" \"$NODE\" \"$IP\"\ndone\n\necho \"\"\necho \"\u2705 Multipass nodes are ready. Proceed with Kubernetes the Hard Way steps.\"\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#run","title":"\u2705 Run","text":"<pre><code>chmod +x kthw_multipass_setup.sh\n./kthw_multipass_setup.sh\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#here-are-the-steps-to-start-with-kubernetes-the-hard-way","title":"Here are the steps to start with Kubernetes The Hard Way","text":""},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#steps-to-start-with-kelsey-hightowers-repo","title":"\ud83e\ude9c Steps to Start with Kelsey Hightower's Repo","text":"Step Description 1\ufe0f\u20e3 Clone the Repo 2\ufe0f\u20e3 Read the Prerequisites 3\ufe0f\u20e3 Provision Infrastructure 4\ufe0f\u20e3 Set Env Variables 5\ufe0f\u20e3 Generate Certificates 6\ufe0f\u20e3 Configure etcd 7\ufe0f\u20e3 Setup Control Plane 8\ufe0f\u20e3 Setup Worker Nodes 9\ufe0f\u20e3 Configure Networking \ud83d\udd1f Access with kubectl and verify"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#step-by-step-commands","title":"\ud83e\uddf1 Step-by-Step Commands","text":""},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#1-clone-the-repo","title":"1. Clone the repo","text":"<pre><code>git clone https://github.com/kelseyhightower/kubernetes-the-hard-way.git\ncd kubernetes-the-hard-way\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#2-read-prerequisites","title":"2. Read prerequisites","text":"<p>Install on your host:</p> <pre><code>sudo apt install -y curl unzip\ncurl -s https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl\ncurl -s https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson\nchmod +x cfssl cfssljson\nsudo mv cfssl cfssljson /usr/local/bin/\n</code></pre> <p>Install <code>kubectl</code> on host:</p> <pre><code>curl -LO https://dl.k8s.io/release/$(curl -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#3-setup-multipass-nodes-if-not-done-already","title":"3. Setup Multipass nodes (if not done already)","text":"<p>Use the script I gave you earlier.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#4-set-environment-variables","title":"4. Set environment variables","text":"<p>Edit and export:</p> <pre><code>export KUBERNETES_PUBLIC_ADDRESS=\"&lt;Your-LB-IP&gt;\"\nexport KUBERNETES_CLUSTER_NAME=\"kubernetes-the-hard-way\"\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#5-follow-the-repo-section-by-section","title":"5. Follow the repo section-by-section","text":"<p>Inside the repo:</p> Section Dir What It Does <code>01-prerequisites.md</code> Confirm your system and tools <code>02-certificate-authority.md</code> Create CA and TLS certs <code>03-compute-resources.md</code> Setup worker kubelet + kube-proxy <code>04-certificate-generation.md</code> Generate certs for all components <code>05-kubeconfig-files.md</code> Create kubeconfig files <code>06-data-encryption.md</code> Setup encryption config <code>07-bootstrapping-etcd.md</code> Deploy etcd to master nodes <code>08-bootstrapping-control-plane.md</code> Deploy apiserver, controller, scheduler <code>09-rbac.md</code> Configure RBAC and cluster role bindings <code>10-worker-setup.md</code> Deploy kubelet, kube-proxy to workers <code>11-networking.md</code> Setup pod networking (e.g., Calico, Flannel) <code>12-dns.md</code> Deploy CoreDNS <code>13-smoke-test.md</code> Validate cluster is working"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#tip_1","title":"\u2705 Tip","text":"<p>Use <code>multipass shell &lt;node&gt;</code> to enter VM and run the commands as-is.</p> <p>Yes \u2705 \u2014 you can automate Kubernetes The Hard Way setup using bash scripts based on the Kelsey Hightower repo. The repo is intentionally manual for learning, but it\u2019s scriptable.</p>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#what-can-be-automated","title":"\ud83e\uddf0 What Can Be Automated?","text":"Area Can be Scripted? Notes VM Provisioning (Multipass) \u2705 Yes Already done in earlier script Certificate Generation \u2705 Yes Use <code>cfssl</code> and scripts Kubeconfig Files \u2705 Yes Reusable with <code>kubectl config set-*</code> etcd + Control Plane Setup \u2705 Yes Use SSH + remote commands Worker Node Bootstrap \u2705 Yes Script <code>kubelet</code>, <code>kube-proxy</code> installs Networking (Flannel/Calico) \u2705 Yes Apply YAML manifest Testing (kubectl access) \u2705 Yes Final smoke tests via script"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#example-structure-for-automation","title":"\ud83e\uddea Example Structure for Automation","text":"<pre><code>kthw-automation/\n\u251c\u2500\u2500 00-setup-multipass.sh         # Creates all VMs\n\u251c\u2500\u2500 01-gen-certs.sh               # Certs via cfssl\n\u251c\u2500\u2500 02-gen-kubeconfigs.sh         # Creates kubeconfig files\n\u251c\u2500\u2500 03-setup-etcd.sh              # Installs etcd on master nodes\n\u251c\u2500\u2500 04-setup-control-plane.sh     # Installs kube-apiserver, etc.\n\u251c\u2500\u2500 05-rbac.sh                    # Admin role binding\n\u251c\u2500\u2500 06-setup-workers.sh           # Installs kubelet &amp; kube-proxy\n\u251c\u2500\u2500 07-deploy-networking.sh       # Deploys Flannel/Calico\n\u251c\u2500\u2500 08-verify-cluster.sh          # Run `kubectl` and test\n</code></pre>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#automation-style","title":"\ud83d\udfe9 Automation Style","text":"<p>You can either:</p> <ul> <li> <p>\ud83d\udd01 Use <code>multipass exec &lt;node&gt; -- bash -c \"&lt;commands&gt;\"</code>, or</p> </li> <li> <p>\ud83d\ude80 Use <code>scp</code> + <code>ssh</code> to copy and run scripts inside VMs</p> </li> </ul>"},{"location":"multipass/k8s%20with%20hard-way%20-%20overview/#next-step","title":"\u23ed\ufe0f Next Step","text":"<p>Do you want me to generate the <code>01-gen-certs.sh</code> and <code>02-gen-kubeconfigs.sh</code> to start with? Or a full end-to-end automation bundle?</p> <p>Let me know the style: bash only or Ansible, or Python fabric etc.</p>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/","title":"Configuring Multiple Services on Common Ports","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <p>When you need to expose multiple services through the same ports (80 for HTTP or 443 for HTTPS), you'll use name-based virtual hosting in Nginx. Here's how to set this up properly:</p>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#solution-name-based-virtual-hosting","title":"Solution: Name-Based Virtual Hosting","text":"<p>All services will share ports 80/443, with Nginx routing traffic based on the hostname.</p>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#1-base-configuration-structure","title":"1. Base Configuration Structure","text":"<p>Create separate files in <code>/etc/nginx/sites-available/</code> for each service:</p> <pre><code>sudo nano /etc/nginx/sites-available/service1.example.com\nsudo nano /etc/nginx/sites-available/service2.example.com\n\nsudo vi /etc/nginx/sites-available/argocd.local\n</code></pre>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#2-sample-configurations","title":"2. Sample Configurations","text":"<p>For HTTP (port 80): <pre><code># /etc/nginx/sites-available/service1.example.com\nserver {\n    listen 80;\n    server_name service1.example.com;\n\n    location / {\n        proxy_pass http://localhost:3000;  # Your service's actual port\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n\n\n\n# /etc/nginx/sites-available/argocd.local\nserver {\n    listen 80;\n    server_name argocd.local;\n\n    location / {\n        proxy_pass http://localhost:3000;  # Your service's actual port\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n</code></pre></p> <p>For HTTPS (port 443): <pre><code># /etc/nginx/sites-available/service2.example.com\nserver {\n    listen 443 ssl;\n    server_name service2.example.com;\n\n    ssl_certificate /etc/ssl/certs/service2.pem;\n    ssl_certificate_key /etc/ssl/private/service2.key;\n\n    location / {\n        proxy_pass http://localhost:4000;  # Your service's actual port\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto https;\n    }\n}\n</code></pre></p>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#3-enable-the-configurations","title":"3. Enable the Configurations","text":"<pre><code>sudo ln -s /etc/nginx/sites-available/service1.example.com /etc/nginx/sites-enabled/\nsudo ln -s /etc/nginx/sites-available/service2.example.com /etc/nginx/sites-enabled/\n\n\nsudo ln -s /etc/nginx/sites-available/argocd.local /etc/nginx/sites-enabled/\n</code></pre>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#4-default-server-handling","title":"4. Default Server Handling","text":"<p>Add a default catch-all server to handle unmatched requests:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <pre><code>server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name _;\n    return 444;  # Close connection silently\n}\n\nserver {\n    listen 443 ssl default_server;\n    listen [::]:443 ssl default_server;\n    server_name _;\n    ssl_certificate /etc/ssl/certs/default.pem;\n    ssl_certificate_key /etc/ssl/private/default.key;\n    return 444;\n}\n</code></pre>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#tailscale-specific-configuration","title":"Tailscale-Specific Configuration","text":"<p>For Tailscale access, you have two options:</p>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#option-a-use-tailscale-magicdns","title":"Option A: Use Tailscale MagicDNS","text":"<pre><code>server {\n    listen 80;\n    server_name service1.your-tailnet-name.ts.net;\n    # ... rest of config\n}\n</code></pre>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#option-b-use-tailscale-ip-with-host-header","title":"Option B: Use Tailscale IP with Host Header","text":"<pre><code>server {\n    listen 80;\n    server_name 100.xx.yy.zz;  # Your Tailscale IP\n\n    location /service1 {\n        proxy_pass http://localhost:3000;\n        # ... headers\n    }\n\n    location /service2 {\n        proxy_pass http://localhost:4000;\n        # ... headers\n    }\n}\n</code></pre>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#verification-and-maintenance","title":"Verification and Maintenance","text":"<ol> <li> <p>Test your configuration:    <pre><code>sudo nginx -t\n</code></pre></p> </li> <li> <p>Reload Nginx:    <pre><code>sudo systemctl reload nginx\n</code></pre></p> </li> <li> <p>Check active connections:    <pre><code>sudo tail -f /var/log/nginx/access.log\n</code></pre></p> </li> <li> <p>Add new services:</p> </li> <li>Create new config file in <code>/etc/nginx/sites-available/</code></li> <li>Link to <code>/etc/nginx/sites-enabled/</code></li> <li>Reload Nginx</li> </ol>"},{"location":"nginx/Configuring%20Multiple%20Services%20on%20Common%20Ports/#important-notes","title":"Important Notes","text":"<ol> <li>All hostnames must resolve to your server's IP (via DNS or Tailscale MagicDNS)</li> <li>For HTTPS, you'll need certificates for each hostname</li> <li>Consider using a wildcard certificate if you have many subdomains</li> <li>Tailscale can automatically provision certificates for <code>*.ts.net</code> domains</li> </ol>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/","title":"Installing and Configuring Nginx on Ubuntu","text":"<p>Created: 2025-07-01 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#step-1-install-nginx","title":"Step 1: Install Nginx","text":"<ol> <li> <p>Update your package index:    <pre><code>sudo apt update\n</code></pre></p> </li> <li> <p>Install Nginx:    <pre><code>sudo apt install nginx -y\n</code></pre></p> </li> <li> <p>Check if Nginx is running:    <pre><code>sudo systemctl status nginx\n</code></pre>    You should see \"active (running)\" status.</p> </li> </ol>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#step-2-configure-firewall","title":"Step 2: Configure Firewall","text":"<p>Allow Nginx through the firewall: <pre><code>sudo ufw allow 'Nginx HTTP'\n</code></pre></p>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#step-3-create-a-sample-application","title":"Step 3: Create a Sample Application","text":"<ol> <li> <p>Create a directory for your sample application:    <pre><code>sudo mkdir -p /var/www/sample\n</code></pre></p> </li> <li> <p>Create a simple HTML file:    <pre><code>sudo nano /var/www/sample/index.html\n</code></pre></p> </li> <li> <p>Add this content:    <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Sample Application&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to the Sample Application!&lt;/h1&gt;\n    &lt;p&gt;Nginx is successfully serving this page.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> </li> <li> <p>Set proper permissions:    <pre><code>sudo chown -R www-data:www-data /var/www/sample\nsudo chmod -R 755 /var/www/sample\n</code></pre></p> </li> </ol>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#step-4-configure-nginx-server-block","title":"Step 4: Configure Nginx Server Block","text":"<ol> <li> <p>Create a new server block configuration:    <pre><code>sudo nano /etc/nginx/sites-available/sample\n</code></pre></p> </li> <li> <p>Add this configuration (replace <code>your_domain_or_IP</code> with your server's IP or domain):    <pre><code>server {\n    listen 80;\n    server_name your_domain_or_IP;\n\n    root /var/www/sample;\n    index index.html;\n\n    location / {\n        try_files $uri $uri/ =404;\n    }\n}\n</code></pre></p> </li> <li> <p>Enable the configuration by creating a symbolic link:    <pre><code>sudo ln -s /etc/nginx/sites-available/sample /etc/nginx/sites-enabled/\n</code></pre></p> </li> <li> <p>Test the configuration for syntax errors:    <pre><code>sudo nginx -t\n</code></pre></p> </li> <li> <p>Restart Nginx to apply changes:    <pre><code>sudo systemctl restart nginx\n</code></pre></p> </li> </ol>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#step-5-verify-the-installation","title":"Step 5: Verify the Installation","text":"<p>Open your web browser and navigate to your server's IP address or domain name. You should see your sample application page.</p>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#optional-additional-configurations","title":"Optional: Additional Configurations","text":""},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#set-up-a-domain-name","title":"Set Up a Domain Name","text":"<p>If you have a domain name, update your DNS records to point to your server's IP, then update the <code>server_name</code> in the Nginx configuration.</p>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#enable-https-with-lets-encrypt","title":"Enable HTTPS with Let's Encrypt","text":"<ol> <li> <p>Install Certbot:    <pre><code>sudo apt install certbot python3-certbot-nginx -y\n</code></pre></p> </li> <li> <p>Obtain and install certificate:    <pre><code>sudo certbot --nginx -d your_domain\n</code></pre></p> </li> <li> <p>Certbot will automatically update your Nginx configuration to use HTTPS.</p> </li> </ol>"},{"location":"nginx/Installing%20and%20Configuring%20Nginx%20on%20Ubuntu/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you get a 403 Forbidden error, check permissions on your web directory.</li> <li>Check Nginx error logs: <code>/var/log/nginx/error.log</code></li> <li>If changes don't appear, try clearing your browser cache or use incognito mode.</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/","title":"Microservice for Amazon affiliate marketing data scraping","text":""},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#project-structure","title":"Project Structure","text":"<pre><code>amazon-affiliate-scraper/\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 scraper-service/\n\u2502   \u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scrapers/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 amazon_scraper.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 base_scraper.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 product.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 rate_limiter.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 proxy_manager.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 data-service/\n\u2502   \u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 storage/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 csv_handler.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 image_downloader.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 api-gateway/\n\u2502       \u251c\u2500\u2500 src/\n\u2502       \u2502   \u251c\u2500\u2500 routes/\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 scraper_routes.py\n\u2502       \u2502   \u2514\u2500\u2500 main.py\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 settings.py\n\u2502   \u2514\u2500\u2500 messaging/\n\u2502       \u2514\u2500\u2500 queue_manager.py\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 csv/\n\u2502   \u2514\u2500\u2500 images/\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#critical-analysis-first-principles","title":"Critical Analysis &amp; First Principles","text":"<p>Legal Considerations: - Amazon's robots.txt and Terms of Service restrict automated scraping - You'll need to use Amazon's Product Advertising API instead for compliance - Direct scraping violates their terms and can result in IP bans</p> <p>Technical Challenges: - Anti-bot measures (CAPTCHAs, rate limiting, IP blocking) - Dynamic content loading requiring browser automation - Affiliate link generation requires API access - Image copyright considerations</p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#implementation-steps","title":"Implementation Steps","text":""},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-1-set-up-amazon-product-advertising-api","title":"Step 1: Set Up Amazon Product Advertising API","text":"<pre><code># services/scraper-service/src/scrapers/amazon_api.py\nimport boto3\nfrom paapi5_python_sdk.api.default_api import DefaultApi\nfrom paapi5_python_sdk.models.search_items_request import SearchItemsRequest\n\nclass AmazonAPIClient:\n    def __init__(self, access_key, secret_key, partner_tag, host, region):\n        self.access_key = access_key\n        self.secret_key = secret_key\n        self.partner_tag = partner_tag\n        self.host = host\n        self.region = region\n\n    def get_bestsellers(self, category, max_items=50):\n        # Implementation using Product Advertising API\n        pass\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-2-product-data-model","title":"Step 2: Product Data Model","text":"<pre><code># shared/models/product.py\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass Product:\n    asin: str\n    title: str\n    price: float\n    rating: float\n    review_count: int\n    image_url: str\n    affiliate_url: str\n    category: str\n    rank: int\n    description: Optional[str] = None\n    features: List[str] = None\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-3-data-processing-service","title":"Step 3: Data Processing Service","text":"<pre><code># services/data-service/src/storage/csv_handler.py\nimport csv\nimport pandas as pd\nfrom typing import List\nfrom shared.models.product import Product\n\nclass CSVHandler:\n    def __init__(self, output_path: str):\n        self.output_path = output_path\n\n    def save_products(self, products: List[Product]):\n        df = pd.DataFrame([product.__dict__ for product in products])\n        df.to_csv(self.output_path, index=False)\n\n    def append_products(self, products: List[Product]):\n        # Append to existing CSV\n        pass\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-4-image-download-service","title":"Step 4: Image Download Service","text":"<pre><code># services/data-service/src/storage/image_downloader.py\nimport aiohttp\nimport asyncio\nfrom pathlib import Path\n\nclass ImageDownloader:\n    def __init__(self, download_path: str):\n        self.download_path = Path(download_path)\n        self.download_path.mkdir(exist_ok=True)\n\n    async def download_image(self, url: str, filename: str):\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                if response.status == 200:\n                    content = await response.read()\n                    with open(self.download_path / filename, 'wb') as f:\n                        f.write(content)\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-5-microservice-communication","title":"Step 5: Microservice Communication","text":"<pre><code># shared/messaging/queue_manager.py\nimport redis\nimport json\nfrom typing import Dict, Any\n\nclass MessageQueue:\n    def __init__(self, redis_url: str):\n        self.redis_client = redis.from_url(redis_url)\n\n    def publish(self, channel: str, message: Dict[Any, Any]):\n        self.redis_client.publish(channel, json.dumps(message))\n\n    def subscribe(self, channel: str):\n        pubsub = self.redis_client.pubsub()\n        pubsub.subscribe(channel)\n        return pubsub\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#step-6-docker-configuration","title":"Step 6: Docker Configuration","text":"<pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n\n  scraper-service:\n    build: ./services/scraper-service\n    environment:\n      - REDIS_URL=redis://redis:6379\n      - AMAZON_ACCESS_KEY=${AMAZON_ACCESS_KEY}\n      - AMAZON_SECRET_KEY=${AMAZON_SECRET_KEY}\n    depends_on:\n      - redis\n\n  data-service:\n    build: ./services/data-service\n    environment:\n      - REDIS_URL=redis://redis:6379\n    volumes:\n      - ./data:/app/data\n    depends_on:\n      - redis\n\n  api-gateway:\n    build: ./services/api-gateway\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - scraper-service\n      - data-service\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#alternative-approaches","title":"Alternative Approaches","text":"<p>Option 1: Use Amazon's Official APIs - Product Advertising API for product data - Associates Program for affiliate links - Compliant but requires approval</p> <p>Option 2: Third-party Data Providers - Services like Keepa, Jungle Scout APIs - More expensive but legally compliant - Better data quality and reliability</p> <p>Option 3: Manual Curation - Manually select bestselling products - Use Amazon's bestseller lists as reference - Slower but completely compliant</p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#recommended-implementation-path","title":"Recommended Implementation Path","text":"<ol> <li>Start with Amazon Product Advertising API - Apply for API access through Amazon Associates</li> <li>Build data pipeline - Focus on data processing and storage first</li> <li>Add rate limiting and error handling - Essential for production use</li> <li>Implement monitoring - Track API usage, success rates, errors</li> <li>Add data validation - Ensure data quality before storage</li> </ol> <p>The direct scraping approach has significant legal and technical risks. The API-based approach, while requiring approval, provides a sustainable and compliant solution for affiliate marketing data collection.</p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#project-structure_1","title":"Project Structure","text":"<p>Here's a detailed explanation of each line in the project structure with comments:</p> <pre><code>amazon-affiliate-scraper/                   # Root project directory\n\u251c\u2500\u2500 src/                                    # Main source code directory\n\u2502   \u251c\u2500\u2500 __init__.py                         # Makes 'src' a Python package\n\u2502   \u251c\u2500\u2500 config/                             # Configuration management\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py                     # Makes 'config' a Python package\n\u2502   \u2502   \u2514\u2500\u2500 settings.py                     # Application settings (delays, URLs, API keys)\n\u2502   \u251c\u2500\u2500 scrapers/                           # Web scraping logic\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py                     # Makes 'scrapers' a Python package\n\u2502   \u2502   \u251c\u2500\u2500 base_scraper.py                 # Abstract base class for all scrapers\n\u2502   \u2502   \u2514\u2500\u2500 amazon_scraper.py               # Amazon-specific scraping implementation\n\u2502   \u251c\u2500\u2500 data_handlers/                      # Data processing and storage\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py                     # Makes 'data_handlers' a Python package\n\u2502   \u2502   \u251c\u2500\u2500 csv_handler.py                  # CSV file operations (read/write/update)\n\u2502   \u2502   \u2514\u2500\u2500 image_downloader.py             # Downloads and saves product images\n\u2502   \u251c\u2500\u2500 api/                                # REST API layer\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py                     # Makes 'api' a Python package\n\u2502   \u2502   \u251c\u2500\u2500 app.py                          # FastAPI application setup and configuration\n\u2502   \u2502   \u2514\u2500\u2500 routes.py                       # API endpoints and request handlers\n\u2502   \u2514\u2500\u2500 utils/                              # Utility functions and helpers\n\u2502       \u251c\u2500\u2500 __init__.py                     # Makes 'utils' a Python package\n\u2502       \u251c\u2500\u2500 logger.py                       # Logging configuration and setup\n\u2502       \u2514\u2500\u2500 helpers.py                      # Common utility functions\n\u251c\u2500\u2500 data/                                   # Data storage directory\n\u2502   \u251c\u2500\u2500 products.csv                        # CSV file storing scraped product data\n\u2502   \u2514\u2500\u2500 images/                             # Directory for downloaded product images\n\u251c\u2500\u2500 logs/                                   # Application logs directory\n\u251c\u2500\u2500 tests/                                  # Unit and integration tests\n\u2502   \u251c\u2500\u2500 __init__.py                         # Makes 'tests' a Python package\n\u2502   \u2514\u2500\u2500 test_scraper.py                     # Test cases for scraper functionality\n\u251c\u2500\u2500 requirements.txt                        # Python dependencies list\n\u251c\u2500\u2500 docker-compose.yml                      # Multi-container Docker application definition\n\u251c\u2500\u2500 Dockerfile                              # Docker image build instructions\n\u2514\u2500\u2500 README.md                               # Project documentation and setup guide\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#detailed-breakdown-by-directory","title":"Detailed Breakdown by Directory:","text":""},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#root-level-files","title":"Root Level Files:","text":"<ul> <li><code>requirements.txt</code>: Lists all Python packages needed (FastAPI, BeautifulSoup, etc.)</li> <li><code>docker-compose.yml</code>: Defines how to run the app in containers with volumes and ports</li> <li><code>Dockerfile</code>: Instructions to build a Docker image of the application</li> <li><code>README.md</code>: Documentation explaining how to install, configure, and use the project</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#src-directory-main-application-code","title":"<code>src/</code> Directory (Main Application Code):","text":"<ul> <li><code>__init__.py</code>: Empty file that tells Python this directory contains importable modules</li> <li>Purpose: Contains all the business logic and application code</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#srcconfig-directory-configuration-management","title":"<code>src/config/</code> Directory (Configuration Management):","text":"<ul> <li><code>settings.py</code>: Centralized configuration file containing:</li> <li>Scraping delays and timeouts</li> <li>File paths for data storage</li> <li>API server settings</li> <li>User agent strings for web requests</li> <li>Amazon categories to scrape</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#srcscrapers-directory-web-scraping-logic","title":"<code>src/scrapers/</code> Directory (Web Scraping Logic):","text":"<ul> <li><code>base_scraper.py</code>: Abstract class defining common scraping functionality:</li> <li>HTTP request handling with retries</li> <li>User agent rotation</li> <li>Rate limiting between requests</li> <li>Error handling and logging</li> <li><code>amazon_scraper.py</code>: Amazon-specific implementation:</li> <li>Parses Amazon search result pages</li> <li>Extracts product titles, prices, ratings, images</li> <li>Generates affiliate links</li> <li>Handles Amazon's specific HTML structure</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#srcdata_handlers-directory-data-processing","title":"<code>src/data_handlers/</code> Directory (Data Processing):","text":"<ul> <li><code>csv_handler.py</code>: Manages CSV file operations:</li> <li>Saves scraped products to CSV</li> <li>Loads existing products from CSV</li> <li>Handles duplicate removal</li> <li>Manages data formatting</li> <li><code>image_downloader.py</code>: Downloads product images:</li> <li>Downloads images from URLs</li> <li>Saves images with sanitized filenames</li> <li>Handles image format conversion</li> <li>Manages local image storage</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#srcapi-directory-rest-api-layer","title":"<code>src/api/</code> Directory (REST API Layer):","text":"<ul> <li><code>app.py</code>: FastAPI application setup:</li> <li>Creates the main FastAPI instance</li> <li>Configures API metadata (title, description)</li> <li>Sets up startup/shutdown events</li> <li>Includes route handlers</li> <li><code>routes.py</code>: API endpoint definitions:</li> <li><code>/scrape/{category}</code> - Triggers scraping for a category</li> <li><code>/products</code> - Returns all scraped products</li> <li><code>/products/{category}</code> - Returns products by category</li> <li>Background task management</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#srcutils-directory-utility-functions","title":"<code>src/utils/</code> Directory (Utility Functions):","text":"<ul> <li><code>logger.py</code>: Logging configuration:</li> <li>Sets up file and console logging</li> <li>Configures log formats and levels</li> <li>Creates daily log files</li> <li>Manages log rotation</li> <li><code>helpers.py</code>: Common utility functions:</li> <li>String sanitization</li> <li>URL validation</li> <li>Data transformation helpers</li> <li>Common validation functions</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#data-directory-data-storage","title":"<code>data/</code> Directory (Data Storage):","text":"<ul> <li><code>products.csv</code>: Main data file storing:</li> <li>Product titles and descriptions</li> <li>Prices and ratings</li> <li>Image URLs and local paths</li> <li>Affiliate links</li> <li>Category information</li> <li><code>images/</code>: Directory containing:</li> <li>Downloaded product images</li> <li>Organized by product or category</li> <li>Various image formats (JPG, PNG, etc.)</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#logs-directory-application-logs","title":"<code>logs/</code> Directory (Application Logs):","text":"<ul> <li>Contains daily log files with:</li> <li>Scraping activity records</li> <li>Error messages and stack traces</li> <li>API request logs</li> <li>Performance metrics</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#tests-directory-testing","title":"<code>tests/</code> Directory (Testing):","text":"<ul> <li><code>test_scraper.py</code>: Test cases for:</li> <li>Scraper functionality</li> <li>Data validation</li> <li>API endpoints</li> <li>Error handling scenarios</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#file-naming-conventions","title":"File Naming Conventions:","text":"<ul> <li>Snake_case: Used for Python files (<code>amazon_scraper.py</code>)</li> <li>Lowercase: Used for directories (<code>data_handlers/</code>)</li> <li>Descriptive names: Each file clearly indicates its purpose</li> <li>Separation of concerns: Each file has a single responsibility</li> </ul>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#architecture-benefits","title":"Architecture Benefits:","text":"<ul> <li>Modularity: Each component can be developed and tested independently</li> <li>Scalability: Easy to add new scrapers or data handlers</li> <li>Maintainability: Clear separation makes debugging easier</li> <li>Testability: Each module can be unit tested separately</li> <li>Deployment: Docker setup allows easy deployment anywhere</li> </ul> <p>This structure follows Python best practices and microservice architecture principles, making the codebase professional and maintainable.</p> <p>Here's a one-line command to create the entire project structure:</p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#for-linuxmacos-bash","title":"For Linux/macOS (Bash):","text":"<pre><code>mkdir -p amazon-affiliate-scraper/{src/{config,scrapers,data_handlers,api,utils},data/images,logs,tests} &amp;&amp; cd amazon-affiliate-scraper &amp;&amp; touch src/__init__.py src/config/{__init__.py,settings.py} src/scrapers/{__init__.py,base_scraper.py,amazon_scraper.py} src/data_handlers/{__init__.py,csv_handler.py,image_downloader.py} src/api/{__init__.py,app.py,routes.py} src/utils/{__init__.py,logger.py,helpers.py} tests/{__init__.py,test_scraper.py} data/products.csv requirements.txt docker-compose.yml Dockerfile README.md\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#for-windows-powershell","title":"For Windows (PowerShell):","text":"<pre><code>New-Item -ItemType Directory -Path \"amazon-affiliate-scraper\\src\\config\", \"amazon-affiliate-scraper\\src\\scrapers\", \"amazon-affiliate-scraper\\src\\data_handlers\", \"amazon-affiliate-scraper\\src\\api\", \"amazon-affiliate-scraper\\src\\utils\", \"amazon-affiliate-scraper\\data\\images\", \"amazon-affiliate-scraper\\logs\", \"amazon-affiliate-scraper\\tests\" -Force; Set-Location amazon-affiliate-scraper; New-Item -ItemType File -Path \"src\\__init__.py\", \"src\\config\\__init__.py\", \"src\\config\\settings.py\", \"src\\scrapers\\__init__.py\", \"src\\scrapers\\base_scraper.py\", \"src\\scrapers\\amazon_scraper.py\", \"src\\data_handlers\\__init__.py\", \"src\\data_handlers\\csv_handler.py\", \"src\\data_handlers\\image_downloader.py\", \"src\\api\\__init__.py\", \"src\\api\\app.py\", \"src\\api\\routes.py\", \"src\\utils\\__init__.py\", \"src\\utils\\logger.py\", \"src\\utils\\helpers.py\", \"tests\\__init__.py\", \"tests\\test_scraper.py\", \"data\\products.csv\", \"requirements.txt\", \"docker-compose.yml\", \"Dockerfile\", \"README.md\" -Force\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#for-windows-command-prompt","title":"For Windows (Command Prompt):","text":"<pre><code>mkdir amazon-affiliate-scraper\\src\\config amazon-affiliate-scraper\\src\\scrapers amazon-affiliate-scraper\\src\\data_handlers amazon-affiliate-scraper\\src\\api amazon-affiliate-scraper\\src\\utils amazon-affiliate-scraper\\data\\images amazon-affiliate-scraper\\logs amazon-affiliate-scraper\\tests &amp;&amp; cd amazon-affiliate-scraper &amp;&amp; type nul &gt; src\\__init__.py &amp;&amp; type nul &gt; src\\config\\__init__.py &amp;&amp; type nul &gt; src\\config\\settings.py &amp;&amp; type nul &gt; src\\scrapers\\__init__.py &amp;&amp; type nul &gt; src\\scrapers\\base_scraper.py &amp;&amp; type nul &gt; src\\scrapers\\amazon_scraper.py &amp;&amp; type nul &gt; src\\data_handlers\\__init__.py &amp;&amp; type nul &gt; src\\data_handlers\\csv_handler.py &amp;&amp; type nul &gt; src\\data_handlers\\image_downloader.py &amp;&amp; type nul &gt; src\\api\\__init__.py &amp;&amp; type nul &gt; src\\api\\app.py &amp;&amp; type nul &gt; src\\api\\routes.py &amp;&amp; type nul &gt; src\\utils\\__init__.py &amp;&amp; type nul &gt; src\\utils\\logger.py &amp;&amp; type nul &gt; src\\utils\\helpers.py &amp;&amp; type nul &gt; tests\\__init__.py &amp;&amp; type nul &gt; tests\\test_scraper.py &amp;&amp; type nul &gt; data\\products.csv &amp;&amp; type nul &gt; requirements.txt &amp;&amp; type nul &gt; docker-compose.yml &amp;&amp; type nul &gt; Dockerfile &amp;&amp; type nul &gt; README.md\n</code></pre>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#alternative-python-script-cross-platform","title":"Alternative: Python Script (Cross-platform)","text":"<p>Create a file called <code>create_structure.py</code> and run it:</p> <pre><code>import os\n\nstructure = {\n    'amazon-affiliate-scraper': {\n        'src': {\n            '__init__.py': '',\n            'config': {\n                '__init__.py': '',\n                'settings.py': ''\n            },\n            'scrapers': {\n                '__init__.py': '',\n                'base_scraper.py': '',\n                'amazon_scraper.py': ''\n            },\n            'data_handlers': {\n                '__init__.py': '',\n                'csv_handler.py': '',\n                'image_downloader.py': ''\n            },\n            'api': {\n                '__init__.py': '',\n                'app.py': '',\n                'routes.py': ''\n            },\n            'utils': {\n                '__init__.py': '',\n                'logger.py': '',\n                'helpers.py': ''\n            }\n        },\n        'data': {\n            'products.csv': '',\n            'images': {}\n        },\n        'logs': {},\n        'tests': {\n            '__init__.py': '',\n            'test_scraper.py': ''\n        },\n        'requirements.txt': '',\n        'docker-compose.yml': '',\n        'Dockerfile': '',\n        'README.md': ''\n    }\n}\n\ndef create_structure(base_path, structure):\n    for name, content in structure.items():\n        path = os.path.join(base_path, name)\n        if isinstance(content, dict):\n            os.makedirs(path, exist_ok=True)\n            create_structure(path, content)\n        else:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, 'w') as f:\n                f.write(content)\n\ncreate_structure('.', structure)\nprint(\"Project structure created successfully!\")\n</code></pre> <p>Then run: <pre><code>python create_structure.py\n</code></pre></p> <p>The Linux/macOS bash command is the most concise single-line option. After running it, you'll have the complete directory structure with all empty files ready for implementation.</p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#implementation-steps_1","title":"Implementation Steps","text":""},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#1-environment-setup","title":"1. Environment Setup","text":"<p>requirements.txt: <pre><code>fastapi==0.104.1\nuvicorn==0.24.0\nrequests==2.31.0\nbeautifulsoup4==4.12.2\nselenium==4.15.2\npandas==2.1.3\nPillow==10.1.0\nfake-useragent==1.4.0\npython-dotenv==1.0.0\naiofiles==23.2.1\npytest==7.4.3\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#2-configuration","title":"2. Configuration","text":"<p>src/config/settings.py: <pre><code>import os\nfrom typing import List\n\nclass Settings:\n    # Scraping settings\n    REQUEST_DELAY = 2  # seconds between requests\n    MAX_RETRIES = 3\n    TIMEOUT = 30\n\n    # Data settings\n    CSV_FILE_PATH = \"data/products.csv\"\n    IMAGES_DIR = \"data/images\"\n\n    # API settings\n    API_HOST = \"0.0.0.0\"\n    API_PORT = 8000\n\n    # User agents rotation\n    USER_AGENTS = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n    ]\n\n    # Amazon categories to scrape\n    CATEGORIES = [\n        \"electronics\",\n        \"books\",\n        \"home-kitchen\",\n        \"clothing\"\n    ]\n\nsettings = Settings()\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#3-base-scraper","title":"3. Base Scraper","text":"<p>src/scrapers/base_scraper.py: <pre><code>import time\nimport random\nimport requests\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional\nfrom fake_useragent import UserAgent\nfrom src.utils.logger import get_logger\nfrom src.config.settings import settings\n\nclass BaseScraper(ABC):\n    def __init__(self):\n        self.session = requests.Session()\n        self.ua = UserAgent()\n        self.logger = get_logger(__name__)\n\n    def get_headers(self) -&gt; Dict[str, str]:\n        return {\n            'User-Agent': self.ua.random,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive',\n        }\n\n    def make_request(self, url: str, retries: int = 0) -&gt; Optional[requests.Response]:\n        if retries &gt;= settings.MAX_RETRIES:\n            self.logger.error(f\"Max retries exceeded for {url}\")\n            return None\n\n        try:\n            time.sleep(random.uniform(1, settings.REQUEST_DELAY))\n            response = self.session.get(\n                url, \n                headers=self.get_headers(),\n                timeout=settings.TIMEOUT\n            )\n            response.raise_for_status()\n            return response\n\n        except requests.RequestException as e:\n            self.logger.warning(f\"Request failed for {url}: {e}\")\n            return self.make_request(url, retries + 1)\n\n    @abstractmethod\n    def scrape_products(self, category: str) -&gt; List[Dict]:\n        pass\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#4-amazon-scraper","title":"4. Amazon Scraper","text":"<p>src/scrapers/amazon_scraper.py: <pre><code>import re\nfrom typing import Dict, List\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nfrom src.scrapers.base_scraper import BaseScraper\n\nclass AmazonScraper(BaseScraper):\n    def __init__(self):\n        super().__init__()\n        self.base_url = \"https://www.amazon.com\"\n\n    def scrape_products(self, category: str, pages: int = 3) -&gt; List[Dict]:\n        products = []\n\n        for page in range(1, pages + 1):\n            url = f\"{self.base_url}/s?k=best+sellers+{category}&amp;page={page}\"\n            response = self.make_request(url)\n\n            if not response:\n                continue\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n            product_containers = soup.find_all('div', {'data-component-type': 's-search-result'})\n\n            for container in product_containers:\n                product = self._extract_product_data(container)\n                if product:\n                    products.append(product)\n\n        return products\n\n    def _extract_product_data(self, container) -&gt; Dict:\n        try:\n            # Title\n            title_elem = container.find('h2', class_='a-size-mini')\n            title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n\n            # Price\n            price_elem = container.find('span', class_='a-price-whole')\n            price = price_elem.get_text(strip=True) if price_elem else \"N/A\"\n\n            # Rating\n            rating_elem = container.find('span', class_='a-icon-alt')\n            rating = rating_elem.get_text(strip=True) if rating_elem else \"N/A\"\n\n            # Image URL\n            img_elem = container.find('img', class_='s-image')\n            image_url = img_elem.get('src') if img_elem else None\n\n            # Product URL\n            link_elem = container.find('h2').find('a') if container.find('h2') else None\n            product_url = urljoin(self.base_url, link_elem.get('href')) if link_elem else None\n\n            # Generate affiliate link (placeholder)\n            affiliate_link = self._generate_affiliate_link(product_url) if product_url else None\n\n            return {\n                'title': title,\n                'price': price,\n                'rating': rating,\n                'image_url': image_url,\n                'product_url': product_url,\n                'affiliate_link': affiliate_link,\n                'category': category\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error extracting product data: {e}\")\n            return None\n\n    def _generate_affiliate_link(self, product_url: str) -&gt; str:\n        # Add your Amazon affiliate tag here\n        affiliate_tag = \"your-affiliate-tag\"\n        if '?' in product_url:\n            return f\"{product_url}&amp;tag={affiliate_tag}\"\n        else:\n            return f\"{product_url}?tag={affiliate_tag}\"\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#5-data-handlers","title":"5. Data Handlers","text":"<p>src/data_handlers/csv_handler.py: <pre><code>import pandas as pd\nimport os\nfrom typing import List, Dict\nfrom src.config.settings import settings\nfrom src.utils.logger import get_logger\n\nclass CSVHandler:\n    def __init__(self):\n        self.logger = get_logger(__name__)\n        self.csv_path = settings.CSV_FILE_PATH\n        self._ensure_directory_exists()\n\n    def _ensure_directory_exists(self):\n        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n\n    def save_products(self, products: List[Dict]) -&gt; bool:\n        try:\n            df = pd.DataFrame(products)\n\n            # Check if file exists to append or create new\n            if os.path.exists(self.csv_path):\n                existing_df = pd.read_csv(self.csv_path)\n                df = pd.concat([existing_df, df], ignore_index=True)\n                # Remove duplicates based on product_url\n                df = df.drop_duplicates(subset=['product_url'], keep='last')\n\n            df.to_csv(self.csv_path, index=False)\n            self.logger.info(f\"Saved {len(products)} products to {self.csv_path}\")\n            return True\n\n        except Exception as e:\n            self.logger.error(f\"Error saving products to CSV: {e}\")\n            return False\n\n    def load_products(self) -&gt; List[Dict]:\n        try:\n            if os.path.exists(self.csv_path):\n                df = pd.read_csv(self.csv_path)\n                return df.to_dict('records')\n            return []\n        except Exception as e:\n            self.logger.error(f\"Error loading products from CSV: {e}\")\n            return []\n</code></pre></p> <p>src/data_handlers/image_downloader.py: <pre><code>import os\nimport requests\nfrom urllib.parse import urlparse\nfrom typing import Optional\nfrom src.config.settings import settings\nfrom src.utils.logger import get_logger\n\nclass ImageDownloader:\n    def __init__(self):\n        self.logger = get_logger(__name__)\n        self.images_dir = settings.IMAGES_DIR\n        self._ensure_directory_exists()\n\n    def _ensure_directory_exists(self):\n        os.makedirs(self.images_dir, exist_ok=True)\n\n    def download_image(self, image_url: str, product_title: str) -&gt; Optional[str]:\n        try:\n            response = requests.get(image_url, timeout=30)\n            response.raise_for_status()\n\n            # Generate filename\n            parsed_url = urlparse(image_url)\n            extension = os.path.splitext(parsed_url.path)[1] or '.jpg'\n            filename = f\"{self._sanitize_filename(product_title)}{extension}\"\n            filepath = os.path.join(self.images_dir, filename)\n\n            # Save image\n            with open(filepath, 'wb') as f:\n                f.write(response.content)\n\n            self.logger.info(f\"Downloaded image: {filename}\")\n            return filepath\n\n        except Exception as e:\n            self.logger.error(f\"Error downloading image {image_url}: {e}\")\n            return None\n\n    def _sanitize_filename(self, filename: str) -&gt; str:\n        # Remove invalid characters for filename\n        invalid_chars = '&lt;&gt;:\"/\\\\|?*'\n        for char in invalid_chars:\n            filename = filename.replace(char, '_')\n        return filename[:50]  # Limit length\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#6-api-layer","title":"6. API Layer","text":"<p>src/api/app.py: <pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom src.api.routes import router\nfrom src.utils.logger import get_logger\n\napp = FastAPI(\n    title=\"Amazon Affiliate Scraper\",\n    description=\"Microservice for scraping Amazon products for affiliate marketing\",\n    version=\"1.0.0\"\n)\n\napp.include_router(router)\n\nlogger = get_logger(__name__)\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    logger.info(\"Amazon Affiliate Scraper API started\")\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    logger.info(\"Amazon Affiliate Scraper API stopped\")\n</code></pre></p> <p>src/api/routes.py: <pre><code>from fastapi import APIRouter, BackgroundTasks, HTTPException\nfrom typing import List, Dict\nfrom src.scrapers.amazon_scraper import AmazonScraper\nfrom src.data_handlers.csv_handler import CSVHandler\nfrom src.data_handlers.image_downloader import ImageDownloader\nfrom src.config.settings import settings\n\nrouter = APIRouter()\n\n@router.post(\"/scrape/{category}\")\nasync def scrape_category(category: str, background_tasks: BackgroundTasks):\n    if category not in settings.CATEGORIES:\n        raise HTTPException(status_code=400, detail=\"Invalid category\")\n\n    background_tasks.add_task(scrape_and_save, category)\n    return {\"message\": f\"Scraping started for category: {category}\"}\n\n@router.get(\"/products\")\nasync def get_products() -&gt; List[Dict]:\n    csv_handler = CSVHandler()\n    products = csv_handler.load_products()\n    return products\n\n@router.get(\"/products/{category}\")\nasync def get_products_by_category(category: str) -&gt; List[Dict]:\n    csv_handler = CSVHandler()\n    products = csv_handler.load_products()\n    filtered_products = [p for p in products if p.get('category') == category]\n    return filtered_products\n\nasync def scrape_and_save(category: str):\n    scraper = AmazonScraper()\n    csv_handler = CSVHandler()\n    image_downloader = ImageDownloader()\n\n    # Scrape products\n    products = scraper.scrape_products(category)\n\n    # Download images\n    for product in products:\n        if product.get('image_url'):\n            local_path = image_downloader.download_image(\n                product['image_url'], \n                product['title']\n            )\n            product['local_image_path'] = local_path\n\n    # Save to CSV\n    csv_handler.save_products(products)\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#7-utilities","title":"7. Utilities","text":"<p>src/utils/logger.py: <pre><code>import logging\nimport os\nfrom datetime import datetime\n\ndef get_logger(name: str) -&gt; logging.Logger:\n    logger = logging.getLogger(name)\n\n    if not logger.handlers:\n        logger.setLevel(logging.INFO)\n\n        # Create logs directory\n        os.makedirs('logs', exist_ok=True)\n\n        # File handler\n        file_handler = logging.FileHandler(\n            f'logs/scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n        )\n        file_handler.setLevel(logging.INFO)\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n\n        # Formatter\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        file_handler.setFormatter(formatter)\n        console_handler.setFormatter(formatter)\n\n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n\n    return logger\n</code></pre></p>"},{"location":"python/Microservice%20for%20Amazon%20affiliate%20marketing%20data%20scraping/#8-docker-setup","title":"8. Docker Setup","text":"<p>Dockerfile: <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"src.api.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre></p> <p>docker-compose.yml: ```yaml version: '3.8'</p> <p>services:   scraper-api:     build: .     ports:       - \"8000:8000\"     volumes:       - ./data:/app/data       - ./logs:/app/logs     environment</p>"},{"location":"python/Microservices%20-%20Project/","title":"Microservices   Project","text":""},{"location":"python/Microservices%20-%20Project/#1-project-structure-creation","title":"\ud83d\udcc1 1. Project Structure Creation","text":"<pre><code>mkdir -p scraper-microservices/{scraper-service/app/{routes,scrapers,models},shared,k8s/{scraper,kong,redis}}\ncd scraper-microservices\ntouch scraper-service/{Dockerfile,requirements.txt}\ntouch scraper-service/app/main.py\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#2-setup-python-scraper-microservice-fastapi","title":"\u2699\ufe0f 2. Setup Python Scraper Microservice (FastAPI)","text":"<p>scraper-service/requirements.txt</p> <pre><code>fastapi\nuvicorn\nrequests\nbeautifulsoup4\n</code></pre> <p>scraper-service/app/main.py</p> <pre><code>from fastapi import FastAPI\nfrom app.routes import scrape\n\napp = FastAPI()\napp.include_router(scrape.router)\n</code></pre> <p>scraper-service/app/routes/scrape.py</p> <pre><code>from fastapi import APIRouter, Query\nfrom app.scrapers import amazon, noon\n\nrouter = APIRouter()\n\n@router.get(\"/scrape\")\ndef scrape(site: str = Query(...), q: str = Query(...)):\n    if site == \"amazon\":\n        return amazon.scrape(q)\n    elif site == \"noon\":\n        return noon.scrape(q)\n    return {\"error\": \"unsupported site\"}\n</code></pre> <p>scraper-service/app/scrapers/amazon.py</p> <pre><code>def scrape(query):\n    return {\"site\": \"amazon\", \"query\": query}\n</code></pre> <p>scraper-service/app/scrapers/noon.py</p> <pre><code>def scrape(query):\n    return {\"site\": \"noon\", \"query\": query}\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#3-add-dockerfile","title":"\ud83d\udc33 3. Add Dockerfile","text":"<p>scraper-service/Dockerfile</p> <pre><code>FROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY ./app ./app\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#4-add-docker-compose-for-local-testing","title":"\ud83e\uddea 4. Add Docker Compose for Local Testing","text":"<p>docker-compose.yml</p> <pre><code>version: '3'\nservices:\n  scraper:\n    build: ./scraper-service\n    ports:\n      - \"8000:8000\"\n  redis:\n    image: redis:7\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#5-add-kubernetes-yamls","title":"\u2638\ufe0f 5. Add Kubernetes YAMLs","text":"<p>k8s/scraper/deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: scraper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: scraper\n  template:\n    metadata:\n      labels:\n        app: scraper\n    spec:\n      containers:\n        - name: scraper\n          image: ttl.sh/scraper:1h\n          ports:\n            - containerPort: 8000\n</code></pre> <p>k8s/scraper/service.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: scraper-service\nspec:\n  selector:\n    app: scraper\n  ports:\n    - port: 8000\n      targetPort: 8000\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#6-kong-gateway-setup-ingress","title":"\ud83c\udf10 6. Kong Gateway Setup (Ingress)","text":"<p>k8s/kong/scraper-ingress.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: scraper-ingress\n  annotations:\n    konghq.com/strip-path: \"true\"\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /scraper\n            pathType: Prefix\n            backend:\n              service:\n                name: scraper-service\n                port:\n                  number: 8000\n</code></pre>"},{"location":"python/Microservices%20-%20Project/#7-build-push-image-to-ttlsh","title":"\ud83d\udce6 7. Build &amp; Push Image to <code>ttl.sh</code>","text":"<pre><code>docker build -t ttl.sh/scraper:1h ./scraper-service\ndocker push ttl.sh/scraper:1h\n</code></pre> <p>Would you like the Helm install steps for Kong Gateway next?</p>"},{"location":"python/mkdocs%20to%20github%20pages/","title":"Mkdocs to github pages","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"python/mkdocs%20to%20github%20pages/#solution-initialize-a-git-repository","title":"Solution: Initialize a Git Repository","text":"<ol> <li>Check if you\u2019re in a Git repo: <pre><code>git status\n</code></pre></li> <li> <p>If you see <code>fatal: not a git repository...</code>, proceed to initialize Git.</p> </li> <li> <p>Initialize a new Git repository: <pre><code>git init\n</code></pre></p> </li> <li> <p>Connect to your GitHub repo (if not already linked): <pre><code>git remote add origin https://github.com/&lt;username&gt;/&lt;repo-name&gt;.git\n</code></pre></p> </li> <li> <p>Replace <code>&lt;username&gt;</code> and <code>&lt;repo-name&gt;</code> with your GitHub details.</p> </li> <li> <p>Stage and commit your MkDocs files: <pre><code>git add .\ngit commit -m \"Initial commit\"\n</code></pre></p> </li> <li> <p>Push to GitHub (if needed): <pre><code>git push -u origin main  # or `master` for older repos\n</code></pre></p> </li> <li> <p>Now try <code>mkdocs gh-deploy</code> again: <pre><code>mkdocs gh-deploy\n</code></pre></p> </li> <li>This should now work since Git is properly initialized.</li> </ol>"},{"location":"python/mkdocs%20to%20github%20pages/#alternative-fix-run-gh-deploy-from-the-correct-directory","title":"Alternative Fix: Run <code>gh-deploy</code> from the Correct Directory","text":"<ul> <li>Ensure you\u2019re running the command in the root of your MkDocs project (where <code>mkdocs.yml</code> is located).</li> <li>If you cloned a repo, <code>cd</code> into it first:   <pre><code>cd your-repo-name/\n</code></pre></li> </ul>"},{"location":"python/mkdocs%20to%20github%20pages/#still-getting-the-error","title":"Still Getting the Error?","text":"<ul> <li>If you\u2019re in a subdirectory (e.g., <code>docs/</code>), move back to the root:   <pre><code>cd ..\n</code></pre></li> <li>Verify the <code>.git</code> folder exists:   <pre><code>ls -la .git\n</code></pre></li> <li>If missing, reinitialize Git (<code>git init</code>).</li> </ul>"},{"location":"python/mkdocs%20to%20github%20pages/#final-check","title":"Final Check","text":"<p>After fixing, run: <pre><code>git add .\ngit commit -am \"new changes\"\npython .\\gen_index.py\nmkdocs gh-deploy --clean --remote-name origin\nmkdocs build\ngit status  # Should show tracked files\nmkdocs serve\nmkdocs gh-deploy  # Should now deploy to GitHub Pages\n\n\n\n(.venv) PS D:\\githubRepos\\homelab&gt; mkdocs gh-deploy\nINFO    -  Cleaning site directory\nINFO    -  Building documentation to directory: D:\\githubRepos\\homelab\\site\nINFO    -  Documentation built in 1.33 seconds\nINFO    -  Copying 'D:\\githubRepos\\homelab\\site' to 'gh-pages' branch and pushing to GitHub.\nEnumerating objects: 1, done.\nCounting objects: 100% (1/1), done.\nWriting objects: 100% (1/1), 216 bytes | 216.00 KiB/s, done.\nTotal 1 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nTo https://github.com/gowseshaik/homelab.git\n   5ba2b58..23f810a  gh-pages -&gt; gh-pages\nINFO    -  Your documentation should shortly be available at: https://gowseshaik.github.io/homelab/\n</code></pre></p> <p>wait for 5 mins, it will take some time to upload and generate the site </p> <p>Wala!, Your site should deploy successfully! \ud83d\ude80  </p>"},{"location":"python/python%20pyautogui/","title":"Python pyautogui","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p> <pre><code>You're hitting the **PEP 668** protection \u2014 Ubuntu is blocking system-wide `pip install` to avoid breaking Python.\n</code></pre> <pre><code>python3 -m venv ~/pyauto-env\nsource ~/pyauto-env/bin/activate\npip install pyautogui\n\npython3 -m venv pyauto-env\nsource ./pyauto-env/bin/activate\npip install pyautogui\n</code></pre> <pre><code>import pyautogui\nimport time\n\n# Give time to switch to desired app\ntime.sleep(3)\n\n# Move to specific screen location and click (adjust x, y)\npyautogui.moveTo(500, 400)\npyautogui.click()\n\n# Wait a bit\ntime.sleep(1)\n\n# Close window (Alt+F4 for Windows/Linux)\npyautogui.hotkey('alt', 'f4')\n\n\npython your_script.py\n</code></pre> <p>To run your PyAutoGUI script as a Windows background package, follow these steps:</p>"},{"location":"python/python%20pyautogui/#1-freeze-the-script-as-a-standalone-exe","title":"\u2705 1. Freeze the script as a standalone <code>.exe</code>","text":"<p>Install <code>pyinstaller</code>:</p> <pre><code>pip install pyinstaller\n</code></pre> <p>Create the <code>.exe</code>:</p> <pre><code>pyinstaller --noconsole --onefile demo.py\n</code></pre> <ul> <li> <p><code>--noconsole</code>: hides terminal window (runs silently in background)</p> </li> <li> <p><code>--onefile</code>: packs everything into a single <code>.exe</code></p> </li> <li> <p>Output will be in <code>dist/demo.exe</code></p> </li> </ul>"},{"location":"python/python%20pyautogui/#2-auto-run-in-background-optional","title":"\u2705 2. Auto-run in background (optional)","text":"<p>To run at startup silently:</p> <ul> <li> <p>Copy <code>demo.exe</code> to:</p> <pre><code>C:\\Users\\&lt;YourUsername&gt;\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\n</code></pre> </li> </ul>"},{"location":"python/python%20pyautogui/#3-run-minimized-or-background-safe-avoid-popups","title":"\u2705 3. Run minimized or background-safe (avoid popups)","text":"<p>Update your <code>demo.py</code> to avoid mouse jumps while idle:</p> <pre><code>import pyautogui\nimport time\n\npyautogui.FAILSAFE = False  # prevent crash on screen corner\ntime.sleep(2)  # time to move mouse away if needed\n\npyautogui.moveTo(500, 500)\npyautogui.click()\npyautogui.hotkey('alt', 'f4')\n</code></pre> <p>Let me know if you want to:</p> <ul> <li> <p>Add a tray icon to stop it</p> </li> <li> <p>Auto-close specific app by name</p> </li> <li> <p>Make a config file for coordinates</p> </li> </ul>"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/","title":"Schedule backups of NFS volumes using Velero","text":"<p>\u2705 Yes, you can back up Longhorn volumes and NFS volumes using Velero \u2014 but they follow different methods:</p>"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#1-backing-up-longhorn-volumes-native-csi-snapshot","title":"\u2705 1. Backing Up Longhorn Volumes (Native CSI Snapshot)","text":""},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#recommended-method-csi-snapshots-via-velero","title":"\ud83d\udd27 Recommended Method: CSI Snapshots via Velero","text":"Requirement Value Plugin <code>velero/velero-plugin-for-csi</code> VolumeSnapshotClass <code>longhorn</code> (with <code>driver.longhorn.io</code>) Restore Support \u2705 Full volume snapshot restore Speed \u26a1 Fast \u2014 uses native block-level snapshots \u2705 This is the best method for Longhorn PVs."},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#2-backing-up-nfs-volumes-restic-based-file-backup","title":"\u2705 2. Backing Up NFS Volumes (Restic-based File Backup)","text":""},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#method-file-level-backup-using-velero-restic","title":"\ud83d\udd27 Method: File-level Backup using Velero + Restic","text":"Requirement Value Plugin <code>velero/velero-plugin-for-restic</code> (enabled by default now) Annotate PVC pods Yes (optional) Restore Support \u2705 Works, but slower (file-level copy) Limitation \u274c No snapshotting, just file sync Speed \ud83d\udc22 Slower for large volumes <p>\u2705 Works well for NFS, since NFS is a file system and doesn't support CSI snapshots.</p>"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#summary-comparison","title":"\ud83e\udde0 Summary Comparison","text":"Feature Longhorn Volume NFS Volume Snapshot Type CSI VolumeSnapshot File-level (Restic) Fast Snapshot \u2705 \u274c Restore Accuracy \u2705 Block-perfect \u2705 File-level only Velero Plugin <code>velero-plugin-for-csi</code> <code>velero-plugin-for-restic</code> Performance \u26a1 Fast \ud83d\udc22 Slower (file copy)"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#how-to-enable-both-in-velero","title":"\u2705 How to Enable Both in Velero","text":""},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#install-velero-with-both-plugins","title":"Install Velero with both plugins:","text":"<pre><code>velero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.9.0,velero/velero-plugin-for-csi:v0.7.0 \\\n  --bucket velero \\\n  --secret-file ~/.velero/credentials-velero \\\n  --use-volume-snapshots=true \\\n  --use-restic \\\n  --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.velero.svc:9000 \\\n  --snapshot-location-config region=minio\n</code></pre>"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#annotate-pods-for-restic-backup-for-nfs","title":"Annotate Pods for Restic Backup (for NFS)","text":"<p><pre><code>kubectl annotate pod &lt;pod-name&gt; backup.velero.io/backup-volumes=&lt;volume-mount-name&gt;\n</code></pre> Example: <pre><code>kubectl annotate pod myapp-1234 backup.velero.io/backup-volumes=data\n</code></pre></p>"},{"location":"velero%20-%20for%20migration/Schedule%20backups%20of%20NFS%20volumes%20using%20Velero/#restore-flow","title":"\ud83d\udd01 Restore Flow","text":"Step Longhorn NFS <code>velero restore create</code> Uses CSI to create PVC from snapshot Recreates PVC and syncs files using Restic <p>Let me know if you want:</p> <ul> <li> <p>Working example backup/restore YAMLs for both</p> </li> <li> <p>MinIO setup locally to store Velero backups</p> </li> <li> <p>Longhorn <code>VolumeSnapshotClass</code> YAML for CSI setup</p> </li> </ul>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/","title":"Velero backup + restore setup with Longhorn","text":"<p>Here\u2019s a complete setup guide for Velero + Longhorn for CSI snapshot-based backup and restore \u2014 tested with K3s/K8s and fully automated.</p>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#high-level-plan","title":"\u2705 High-Level Plan","text":"Step Task 1\ufe0f\u20e3 Install Velero with CSI + backup plugins 2\ufe0f\u20e3 Set up <code>VolumeSnapshotClass</code> for Longhorn 3\ufe0f\u20e3 Create test app with PVC 4\ufe0f\u20e3 Backup the app using Velero 5\ufe0f\u20e3 Delete the app 6\ufe0f\u20e3 Restore and verify the data ## 1\ufe0f\u20e3 Install Velero with CSI Plugin + S3/MinIO Backup Support <p>Here we\u2019ll use MinIO for S3-compatible local backup.</p>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#a-deploy-minio-s3-in-k8s","title":"\ud83d\udce6 a. Deploy MinIO (S3) in K8s","text":"<pre><code>kubectl create namespace minio\n\nhelm repo add minio https://charts.min.io/\nhelm repo update\n\nhelm install minio minio/minio \\\n  --namespace minio \\\n  --set accessKey=minio \\\n  --set secretKey=minio123 \\\n  --set persistence.storageClass=longhorn \\\n  --set persistence.size=10Gi\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#b-create-velero-credentials-file","title":"\ud83d\udd11 b. Create Velero Credentials File","text":"<p>Create a file named: <code>minio-credentials</code></p> <pre><code>[default]\naws_access_key_id = minio\naws_secret_access_key = minio123\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#c-install-velero-cli-if-not-installed","title":"\ud83d\udd0c c. Install Velero CLI (if not installed)","text":"<pre><code>curl -LO https://github.com/vmware-tanzu/velero/releases/download/v1.13.0/velero-v1.13.0-linux-amd64.tar.gz\ntar -zxvf velero-v1.13.0-linux-amd64.tar.gz\nsudo mv velero-v1.13.0-linux-amd64/velero /usr/local/bin/\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#d-install-velero-in-cluster-with-csi-s3-plugins","title":"\ud83d\ude80 d. Install Velero in Cluster with CSI + S3 plugins","text":"<pre><code>velero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.9.0,velero/velero-plugin-for-csi:v0.7.0 \\\n  --bucket velero \\\n  --secret-file ./minio-credentials \\\n  --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.minio.svc:9000 \\\n  --use-volume-snapshots=true \\\n  --snapshot-location-config region=minio \\\n  --namespace velero\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#2-create-longhorn-volumesnapshotclass","title":"2\ufe0f\u20e3 Create Longhorn VolumeSnapshotClass","text":"<pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: longhorn\ndriver: driver.longhorn.io\ndeletionPolicy: Delete\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f volumesnapshotclass-longhorn.yaml\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#3-deploy-test-app-pvc-uses-longhorn","title":"3\ufe0f\u20e3 Deploy Test App + PVC (uses Longhorn)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: longhorn\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: test\n    image: busybox\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date) &gt;&gt; /data/out.txt; sleep 5; done\"]\n    volumeMounts:\n    - mountPath: /data\n      name: data-vol\n  volumes:\n  - name: data-vol\n    persistentVolumeClaim:\n      claimName: test-pvc\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#4-backup-with-velero-uses-csi-snapshot","title":"4\ufe0f\u20e3 Backup with Velero (uses CSI snapshot)","text":"<pre><code>velero backup create test-backup --include-namespaces default\n</code></pre> <p>Verify:</p> <pre><code>velero backup get\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#5-delete-the-app","title":"5\ufe0f\u20e3 Delete the App","text":"<pre><code>kubectl delete pod test-pod\nkubectl delete pvc test-pvc\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#6-restore-from-backup","title":"6\ufe0f\u20e3 Restore from Backup","text":"<pre><code>velero restore create --from-backup test-backup\n</code></pre> <p>Verify:</p> <pre><code>kubectl get pods\nkubectl get pvc\n</code></pre> <p>Check the <code>/data/out.txt</code> log in the pod:</p> <pre><code>kubectl exec -it test-pod -- tail /data/out.txt\n</code></pre> <p>You should see previously written data (restored from snapshot).</p>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20%2B%20restore%20setup%20with%20Longhorn/#success-you-now-have-full-csi-based-snapshot-backup-restore-with-longhorn-and-velero","title":"\u2705 Success: You now have Full CSI-based Snapshot Backup &amp; Restore with Longhorn and Velero.","text":"<p>Let me know if you want:</p> <ul> <li> <p>Scheduled recurring backups</p> </li> <li> <p>Pre/post hooks</p> </li> <li> <p>Restic + NFS backup combo</p> </li> <li> <p>Off-cluster DR (disaster recovery) setup across clusters</p> </li> </ul>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/","title":"Velero backup and restore command examples","text":"<p>\u2705 Perfect! Your MinIO is now correctly connected, and the <code>velero</code> bucket is created.</p>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#now-proceed-with-velero-installation","title":"\u2705 Now proceed with Velero installation:","text":"<pre><code>cat &lt;&lt;EOF &gt; credentials-velero\n[default]\naws_access_key_id = admin\naws_secret_access_key = admin123\nEOF\n\nvelero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.8.0 \\\n  --bucket velero \\\n  --secret-file ./credentials-velero \\\n  --backup-location-config region=minio,s3ForcePathStyle=true,s3Url=http://172.18.0.1:9000 \\\n  --use-volume-snapshots=false\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#after-installation","title":"\ud83d\udccc After installation","text":"<ul> <li>Check Velero pod: <pre><code>kubectl get pods -n velero\nkubectl logs deployment/velero -n velero\n</code></pre></li> </ul>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#1-manual-backup-of-all-namespaces","title":"\u2705 1. Manual Backup of All Namespaces","text":"<pre><code>velero backup create full-backup-$(date +%Y%m%d%H%M) --include-namespaces '*' --ttl 168h\n\ngouse@gouse:~/DevOps/multipass_scripts$ velero backup create full-backup-$(date +%Y%m%d%H%M) --include-namespaces '*' --ttl 168h\nBackup request \"full-backup-202507111054\" submitted successfully.\nRun `velero backup describe full-backup-202507111054` or `velero backup logs full-backup-202507111054` for more details.\n</code></pre> <ul> <li><code>--ttl 168h</code>: Backup expiry time (7 days)</li> </ul>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#2-backup-specific-namespace","title":"\u2705 2. Backup Specific Namespace","text":"<pre><code>velero backup create nginx-backup --include-namespaces nginx\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#3-backup-specific-resources-eg-deployments-only","title":"\u2705 3. Backup Specific Resources (e.g., deployments only)","text":"<pre><code>velero backup create only-deployments \\\n  --include-resources deployments \\\n  --include-namespaces default\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#4-list-all-backups","title":"\u2705 4. List All Backups","text":"<pre><code>velero backup get\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#5-describe-a-backup","title":"\u2705 5. Describe a Backup","text":"<pre><code>velero backup describe nginx-backup --details\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#6-restore-full-cluster","title":"\u2705 6. Restore Full Cluster","text":"<pre><code>velero restore create full-restore --from-backup full-backup-202507111000\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#7-restore-specific-namespace","title":"\u2705 7. Restore Specific Namespace","text":"<pre><code>velero restore create nginx-restore --from-backup nginx-backup \\\n  --namespace-mappings nginx:nginx-restore\n</code></pre>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#8-delete-backup","title":"\u2705 8. Delete Backup","text":"<pre><code>velero backup delete nginx-backup\n</code></pre> <p>\u2705 Yes \u2014 Velero stores all backup data in your MinIO bucket (<code>velero</code>) using the S3 API.</p>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#what-gets-stored-in-minio","title":"\ud83d\udce6 What gets stored in MinIO?","text":"<p>Each backup becomes a folder in the bucket:</p> <pre><code>velero/\n\u2514\u2500\u2500 backups/\n    \u2514\u2500\u2500 nginx-backup/\n        \u251c\u2500\u2500 backup.json\n        \u251c\u2500\u2500 pod-volumes/\n        \u2514\u2500\u2500 resources/\n</code></pre> <p>It includes:</p> <ul> <li> <p>Kubernetes resource definitions (YAML)</p> </li> <li> <p>Volume data (if PVCs + Restic/CSI used \u2014 you disabled this with <code>--use-volume-snapshots=false</code>)</p> </li> </ul>"},{"location":"velero%20-%20for%20migration/Velero%20backup%20and%20restore%20command%20examples/#you-can-verify-with","title":"\ud83d\udd0d You can verify with:","text":"<pre><code>mc ls local/velero/backups/\nmc tree local/velero/backups/nginx-backup/\n</code></pre> <p>Or browse <code>http://172.18.0.1:9001</code> (MinIO Console) and check the <code>velero/backups</code> folder.</p>"},{"location":"velero%20-%20for%20migration/Velero%20with%20kind/","title":"Velero with kind","text":"<p>Created: 2025-06-22 | Updated: 2025-07-06 | Author: Gouse Shaik</p>"},{"location":"velero%20-%20for%20migration/Velero%20with%20kind/#typical-setup-steps","title":"Typical setup steps:","text":"<ol> <li>Create kind cluster.</li> <li>Deploy MinIO in cluster (or use external S3).</li> <li>Install Velero with MinIO credentials.</li> <li>Create backups and restores.</li> </ol>"},{"location":"velero%20-%20for%20migration/Velero%20with%20kind/#key-points-for-velero-with-kind","title":"Key points for Velero with kind:","text":"<ul> <li> <p>Kind clusters use local Docker volumes, so by default Velero\u2019s object storage backups (like to S3) won\u2019t work without some setup.</p> </li> <li> <p>You need an object storage backend (e.g., MinIO, AWS S3, or compatible) reachable from the kind cluster for Velero to store backups.</p> </li> <li> <p>MinIO can be deployed inside the kind cluster as an S3-compatible object store for practicing Velero.</p> </li> <li> <p>Velero requires cluster-admin permissions in kind (which you can grant).</p> </li> <li> <p>Backup storage location and volume snapshot support need configuration.</p> </li> </ul> Storage Type Use case Supported by Velero? Longhorn Block storage (PV) No (not object store) MinIO Object storage (S3 API) Yes AWS S3 Object storage Yes Azure Blob Object storage Yes Google Cloud Storage Object storage Yes"}]}