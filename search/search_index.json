{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Knowledge Base","text":""},{"location":"#api-resources","title":"Api-resources","text":"<ul> <li>k8s-api-resources-details</li> </ul>"},{"location":"#crds","title":"Crds","text":"<ul> <li>Custom Resource Definition</li> <li>Deep Dive into CRDs</li> </ul>"},{"location":"#cert-manager","title":"Cert-manager","text":"<ul> <li>install-cert-manager</li> </ul>"},{"location":"#choosing-kubernetes-by-usecases","title":"Choosing kubernetes by usecases","text":"<ul> <li>Kubernetes cluster types on your usecases</li> </ul>"},{"location":"#clusteraccess","title":"Clusteraccess","text":"<ul> <li>Configuring Keycloak for Kubernetes authentication</li> <li>Create a kubeconfig file for the external users</li> </ul>"},{"location":"#configmaps","title":"Configmaps","text":"<ul> <li>ConfigMap is a Kubernetes object used to store non-confidential data</li> </ul>"},{"location":"#cronjob","title":"Cronjob","text":"<ul> <li>CronJobs in Kubernetes</li> <li>Real-World CronJob Example- Automated Database Backup n Cleanup</li> <li>k8s Jobs</li> </ul>"},{"location":"#daemonsets","title":"Daemonsets","text":"<ul> <li>CronJobs Vs DaemonSets</li> <li>DaemonSets in Kubernetes</li> </ul>"},{"location":"#dailytasks","title":"Dailytasks","text":"<ul> <li>pod exec and extract info from pods</li> </ul>"},{"location":"#deployments","title":"Deployments","text":"<ul> <li>Deployments Strategies</li> <li>Deployments in kubernetes</li> </ul>"},{"location":"#docker-podman-images","title":"Docker-podman-images","text":"<ul> <li>Enterprise-grade images as safer</li> <li>Tag images to push</li> <li>scp images in tar format to remote hosts - offline</li> </ul>"},{"location":"#helm","title":"Helm","text":"<ul> <li>1. Check Helm Release(deployed) Information</li> <li>Checking Traefik Helm Configuration</li> <li>Helm Commands - Cheatsheet</li> <li>Helm charts for deployments</li> </ul>"},{"location":"#ingress","title":"Ingress","text":"<ul> <li>1. Host based ingress routing</li> <li>2. Path based ingress routing</li> <li>3. subdomain based ingress routing</li> <li>Access Traefik Dashboard</li> <li>Check Traefik Configuration logs for config issues</li> <li>Diff between Host-path-subdomain based ingress routing</li> <li>How to confirm Ingress resources, routes, and active requests</li> <li>How to confirm the exact Ingress Controller used in your Kubernetes cluster</li> <li>Types of Ingress Controllers with their pros, cons, and pricing details</li> </ul>"},{"location":"#k3d","title":"K3d","text":"<ul> <li>Complete Traefik Setup Guide for k3d</li> <li>K3d cluster setup</li> <li>k3d cluster with traefik dashboard</li> <li>k3d with traefik dashboard setup</li> </ul>"},{"location":"#keycloak","title":"Keycloak","text":"<ul> <li>Efficient way to to Set Up Keycloak (with Helm)</li> </ul>"},{"location":"#keywords","title":"Keywords","text":"<ul> <li>Must know keywords</li> </ul>"},{"location":"#loadbalancers-types","title":"Loadbalancers types","text":"<ul> <li>Loadbalancer Types</li> <li>Reverse proxy means</li> </ul>"},{"location":"#longhorn","title":"Longhorn","text":"<ul> <li>Common Kubernetes storage types</li> <li>Hands-On - Deploying Longhorn on Kubernetes</li> <li>Longhorn Access Modes &amp; Volume Resizing Guide</li> <li>Longhorn in Kubernetes</li> </ul>"},{"location":"#namespace","title":"Namespace","text":"<ul> <li>Delete all resources from namespace</li> </ul>"},{"location":"#nexus","title":"Nexus","text":"<ul> <li>Create a Docker image registry in Nexus</li> </ul>"},{"location":"#opentofu","title":"Opentofu","text":"<ul> <li>About OpenTofu</li> </ul>"},{"location":"#pv-n-pvcs","title":"Pv n pvcs","text":"<ul> <li>PV's and PVC's in Kubernetes</li> </ul>"},{"location":"#pods","title":"Pods","text":"<ul> <li>Debug pod - create for test and delete on post test</li> <li>List all pods with their init containers and sidecar containers info</li> <li>Types of Containers in Kubernetes</li> </ul>"},{"location":"#secrets","title":"Secrets","text":"<ul> <li>Deep Dive into securing secrets in specific envs</li> <li>Secrets management for sensitive information</li> <li>Some real-world examples of secrets mismanagement</li> </ul>"},{"location":"#serviceaccounts","title":"Serviceaccounts","text":"<ul> <li>ServiceAccounts in Kubernetes</li> </ul>"},{"location":"#services","title":"Services","text":"<ul> <li>Headless Services in Kubernetes</li> <li>Types of Kubernetes Services</li> </ul>"},{"location":"#statefulsets","title":"Statefulsets","text":"<ul> <li>Updating a MongoDB StatefulSet in Kubernetes</li> <li>W3H-Understanding StatefulSets in Kubernetes</li> </ul>"},{"location":"#taint-tolerations","title":"Taint-tolerations","text":"<ul> <li>Understanding Tolerations in Kubernetes</li> <li>What is taint _</li> </ul>"},{"location":"#traefik","title":"Traefik","text":"<ul> <li>Securing Traefik Dashboard with Keycloak (OIDC or OAuth2)</li> <li>Traefik Dashboard only for Visibility not for Administration</li> </ul>"},{"location":"#ubuntu","title":"Ubuntu","text":"<ul> <li>list of good tools to extract rar part files</li> </ul>"},{"location":"#etcdctl","title":"Etcdctl","text":"<ul> <li>Install etcdctl tool with supported versions</li> <li>check etcd is running</li> <li>etcdctl commands for get, put, snapshot save and restore</li> </ul>"},{"location":"#fluentd","title":"Fluentd","text":"<ul> <li>Fluentd DaemonSet deployment YAML for Kubernetes</li> <li>Fluentd can run on standalone VMs</li> <li>Fluentd in Kubernetes</li> <li>Fluentd install and config steps for a Linux VM</li> </ul>"},{"location":"#mailhog","title":"Mailhog","text":"<ul> <li>Steps to deploy MailHog with Ingress</li> </ul>"},{"location":"#mkdocs","title":"Mkdocs","text":"<ul> <li>mkdocs to github projects</li> </ul>"},{"location":"#overrides","title":"Overrides","text":""},{"location":"#python","title":"Python","text":"<ul> <li>mkdocs to github pages</li> <li>python pyautogui</li> </ul>"},{"location":"API-Resources/k8s-api-resources-details/","title":"K8s api resources details","text":"<p>To discover Kubernetes API resources and their available fields/values for YAML files, you can use the following commands:</p>"},{"location":"API-Resources/k8s-api-resources-details/#1-list-all-api-resources","title":"1. List All API Resources","text":"<p><pre><code>kubectl api-resources\n</code></pre> Output Example: <pre><code>NAME          SHORTNAMES   APIVERSION     NAMESPACED   KIND\npods          po           v1             true         Pod\ndeployments   deploy       apps/v1        true         Deployment\nservices      svc          v1             true         Service\ningresses     ing          networking.k8s.io/v1   true  Ingress\n...\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#2-get-api-version-for-a-resource","title":"2. Get API Version for a Resource","text":"<p><pre><code>kubectl explain &lt;resource&gt; | head -n 2\n</code></pre> Example: <pre><code>kubectl explain deployment | head -n 2\n</code></pre> Output: <pre><code>KIND:     Deployment\nVERSION:  apps/v1\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#3-view-all-available-fields-in-a-resource","title":"3. View All Available Fields in a Resource","text":"<p><pre><code>kubectl explain &lt;resource&gt; --recursive\n</code></pre> Example (Deployment): <pre><code>kubectl explain deployment --recursive\n</code></pre> Output (Partial): <pre><code>KIND:     Deployment\nVERSION:  apps/v1\nDESCRIPTION:\n     Deployment enables declarative updates for Pods and ReplicaSets.\nFIELDS:\n   apiVersion   &lt;string&gt;\n   kind &lt;string&gt;\n   metadata     &lt;Object&gt;\n      annotations       &lt;map[string]string&gt;\n      creationTimestamp &lt;string&gt;\n      name      &lt;string&gt;\n      namespace &lt;string&gt;\n   spec &lt;Object&gt;\n      replicas  &lt;integer&gt;\n      selector  &lt;Object&gt;\n         matchLabels    &lt;map[string]string&gt;\n      template  &lt;Object&gt;\n         metadata       &lt;Object&gt;\n            labels      &lt;map[string]string&gt;\n         spec   &lt;Object&gt;\n            containers  &lt;[]Object&gt;\n               name     &lt;string&gt;\n               image    &lt;string&gt;\n               ports    &lt;[]Object&gt;\n                  containerPort    &lt;integer&gt;\n...\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#4-check-specific-field-details","title":"4. Check Specific Field Details","text":"<p><pre><code>kubectl explain &lt;resource&gt;.&lt;field&gt;\n</code></pre> Examples: <pre><code># Check Deployment's `spec.strategy`\nkubectl explain deployment.spec.strategy\n\n# Check Pod's `spec.containers`\nkubectl explain pod.spec.containers\n\n# Check Ingress `spec.rules`\nkubectl explain ingress.spec.rules\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#5-list-all-possible-values-for-enums-eg-restartpolicy","title":"5. List All Possible Values for Enums (e.g., <code>restartPolicy</code>)","text":"<p><pre><code>kubectl explain pod.spec.restartPolicy\n</code></pre> Output: <pre><code>KIND:     Pod\nVERSION:  v1\nFIELD:    restartPolicy &lt;string&gt;\nDESCRIPTION:\n     Restart policy for all containers within the pod.\n     Possible enum values:\n     - `\"Always\"`\n     - `\"OnFailure\"`\n     - `\"Never\"`\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#6-view-openapiswagger-docs-full-schema","title":"6. View OpenAPI/Swagger Docs (Full Schema)","text":"<p><pre><code>kubectl get --raw /openapi/v2 | jq . | less\n</code></pre> (Use <code>jq</code> for filtering, e.g., <code>kubectl get --raw /openapi/v2 | jq '.definitions[\"io.k8s.api.apps.v1.Deployment\"]'</code>)</p>"},{"location":"API-Resources/k8s-api-resources-details/#7-check-crd-custom-resource-schema","title":"7. Check CRD (Custom Resource) Schema","text":"<p><pre><code>kubectl get crd &lt;crd-name&gt; -o yaml\n</code></pre> Example: <pre><code>kubectl get crd traefikservices.traefik.io -o yaml\n</code></pre></p>"},{"location":"API-Resources/k8s-api-resources-details/#8-shortcut-for-common-resources","title":"8. Shortcut for Common Resources","text":"Resource Command to View Schema Pod <code>kubectl explain pod</code> Deployment <code>kubectl explain deployment</code> Service <code>kubectl explain service</code> Ingress <code>kubectl explain ingress</code> ConfigMap <code>kubectl explain configmap</code> Secret <code>kubectl explain secret</code>"},{"location":"API-Resources/k8s-api-resources-details/#summary-cheatsheet","title":"Summary Cheatsheet","text":"Command Purpose <code>kubectl api-resources</code> List all available resources <code>kubectl explain &lt;resource&gt;</code> Show YAML structure of a resource <code>kubectl explain &lt;resource&gt; --recursive</code> Show all nested fields <code>kubectl explain &lt;resource&gt;.&lt;field&gt;</code> Check a specific field <code>kubectl get --raw /openapi/v2</code> View full OpenAPI schema <p>These commands help you discover valid fields and values when writing Kubernetes YAML files. \ud83d\ude80</p>"},{"location":"CRDs/Custom%20Resource%20Definition/","title":"Custom Resource Definition","text":"<p>Certainly! Here's a W3H (What, Why, When, How) breakdown for Custom Resource Definitions (CRDs) in Kubernetes:</p>"},{"location":"CRDs/Custom%20Resource%20Definition/#1-what-are-crds","title":"1. What are CRDs?","text":"<p>Definition: CRDs (Custom Resource Definitions) extend the Kubernetes API to allow users to create and manage custom resources (besides built-in resources like Pods, Deployments, etc.).  </p> <p>Key Concepts: - Custom Resource (CR): An instance of a CRD (e.g., <code>MySQLDatabase</code>, <code>RedisCluster</code>). - CRD: The schema/definition that describes the custom resource (like a blueprint).  </p> <p>Example: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: mysqlsamples.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema: {...}\n  scope: Namespaced\n  names:\n    plural: mysqlsamples\n    singular: mysqlsample\n    kind: MySQLSample\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#2-why-use-crds","title":"2. Why Use CRDs?","text":"<p>Purpose/Benefits: - Extend Kubernetes: Add domain-specific resources (e.g., <code>PostgresDB</code>, <code>TensorFlowJob</code>). - Declarative APIs: Manage applications/resources using <code>kubectl</code> like native objects. - Automation: Integrate with operators (e.g., <code>etcd-operator</code>) for lifecycle management. - Reusability: Share CRDs across teams/organizations.  </p> <p>Use Cases: - Databases, ML workloads, CI/CD pipelines, etc.  </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#3-when-to-use-crds","title":"3. When to Use CRDs?","text":"<p>Scenarios: - When Kubernetes lacks a built-in resource for your use case. - When you need a declarative API for your application. - When paired with a Kubernetes Operator for complex logic (e.g., backup, scaling).  </p> <p>Alternatives: - Use ConfigMaps/Secrets for simple configurations. - Use Helm charts for packaging (but Helm doesn\u2019t provide API extensions).  </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#4-how-to-use-crds","title":"4. How to Use CRDs?","text":"<p>Steps to Create/Use a CRD: </p>"},{"location":"CRDs/Custom%20Resource%20Definition/#1-define-the-crd","title":"1. Define the CRD","text":"<p><pre><code># mysql-crd.yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: mysqlsamples.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                dbName:\n                  type: string\n                replicas:\n                  type: integer\n  scope: Namespaced\n  names:\n    plural: mysqlsamples\n    singular: mysqlsample\n    kind: MySQLSample\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-crd.yaml\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#2-create-a-custom-resource-cr","title":"2. Create a Custom Resource (CR)","text":"<p><pre><code># mysql-instance.yaml\napiVersion: example.com/v1\nkind: MySQLSample\nmetadata:\n  name: my-mysql\nspec:\n  dbName: \"mydb\"\n  replicas: 2\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-instance.yaml\n</code></pre></p>"},{"location":"CRDs/Custom%20Resource%20Definition/#3-verify","title":"3. Verify","text":"<pre><code>kubectl get crd                          # List CRDs\nkubectl get mysqlsamples                 # List custom resources\nkubectl describe mysqlsamples my-mysql   # Inspect a CR\n</code></pre>"},{"location":"CRDs/Custom%20Resource%20Definition/#4-optional-build-an-operator","title":"4. (Optional) Build an Operator","text":"<p>Use tools like: - Kubebuilder - Operator SDK to automate CRD management (e.g., reconcile loops).</p>"},{"location":"CRDs/Custom%20Resource%20Definition/#summary-w3h-table","title":"Summary (W3H Table)","text":"Aspect Description What Kubernetes API extension for custom resources. Why To add domain-specific resources declaratively. When When built-in resources are insufficient. How Define CRD \u2192 Create CR \u2192 (Optional) Use Operator."},{"location":"CRDs/Deep%20Dive%20into%20CRDs/","title":"Deep Dive into CRDs","text":"<p>Great! Let\u2019s dive deeper into Custom Resource Definitions (CRDs), covering advanced concepts, best practices, and real-world examples.  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#deep-dive-into-crds","title":"Deep Dive into CRDs","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#1-what-are-crds-extended","title":"1. What are CRDs? (Extended)","text":"<ul> <li>CRDs vs. Built-in Resources: </li> <li>Built-in resources (e.g., <code>Pods</code>, <code>Deployments</code>) are defined in the Kubernetes core code.  </li> <li> <p>CRDs let users define new resource types without modifying the Kubernetes source code.  </p> </li> <li> <p>CRD vs. Operator: </p> </li> <li>A CRD defines the schema (structure) of a custom resource.  </li> <li> <p>An Operator is a controller that manages CRs (e.g., handling creation, scaling, backup).  </p> </li> <li> <p>Example CRDs in the Wild: </p> </li> <li><code>CertManager</code> (<code>Certificate</code>, <code>Issuer</code>)  </li> <li><code>ArgoCD</code> (<code>Application</code>, <code>AppProject</code>)  </li> <li><code>Prometheus Operator</code> (<code>Prometheus</code>, <code>ServiceMonitor</code>)  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#2-why-use-crds-advanced-benefits","title":"2. Why Use CRDs? (Advanced Benefits)","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#a-declarative-apis","title":"a) Declarative APIs","text":"<ul> <li>Instead of imperative scripts (<code>kubectl run</code>, <code>helm install</code>), CRDs allow: <pre><code>apiVersion: databases.example.com/v1\nkind: PostgreSQL\nmetadata:\n  name: my-db\nspec:\n  replicas: 3\n  storage: 100Gi\n</code></pre>   Kubernetes automatically reconciles the desired state.  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#b-integration-with-kubernetes-tooling","title":"b) Integration with Kubernetes Tooling","text":"<ul> <li>CRs work with:  </li> <li><code>kubectl</code> (e.g., <code>kubectl get postgresql</code>)  </li> <li>RBAC (<code>Role</code>/<code>ClusterRole</code> for CR access)  </li> <li><code>kubectl explain postgresql.spec</code> (for self-documenting APIs)  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#c-operator-pattern","title":"c) Operator Pattern","text":"<ul> <li>CRDs + Controllers = Operators (e.g., <code>etcd-operator</code>, <code>redis-operator</code>).  </li> <li>Operators handle:  </li> <li>Provisioning  </li> <li>Scaling  </li> <li>Backups/restores  </li> <li>Upgrades  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#3-when-to-use-crds-decision-guide","title":"3. When to Use CRDs? (Decision Guide)","text":"Use Case CRD? Alternative Need a custom API object \u2705 Yes ConfigMap/Secret (if simple) Complex lifecycle logic \u2705 Yes Helm (if only templating) Reusable across clusters \u2705 Yes Ansible/scripts (imperative) Just configuration storage \u274c No ConfigMap <p>Example: - \u2705 Good for CRD: A <code>SparkJob</code> resource that needs autoscaling, monitoring. - \u274c Not needed: Storing a list of feature flags (use <code>ConfigMap</code>).  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#4-how-to-use-crds-advanced-topics","title":"4. How to Use CRDs? (Advanced Topics)","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#a-crd-schema-validation","title":"a) CRD Schema Validation","text":"<p>Define data types, required fields, and defaults in the CRD: <pre><code>schema:\n  openAPIV3Schema:\n    type: object\n    properties:\n      spec:\n        type: object\n        required: [\"dbName\"]  # Mandatory field\n        properties:\n          dbName:\n            type: string\n          replicas:\n            type: integer\n            default: 1        # Default value\n</code></pre></p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#b-versioning-conversion","title":"b) Versioning &amp; Conversion","text":"<ul> <li>Support multiple API versions (e.g., <code>v1alpha1</code>, <code>v1beta1</code>, <code>v1</code>).  </li> <li>Use conversion webhooks to migrate between versions.  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#c-finalizers-deletion-control","title":"c) Finalizers &amp; Deletion Control","text":"<p>Prevent accidental deletion: <pre><code>metadata:\n  finalizers:\n  - \"finalizer.database.example.com\"\n</code></pre> A controller must clear the finalizer before deletion.  </p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#d-subresources-statusscale","title":"d) Subresources (Status/Scale)","text":"<p>Enable <code>status</code> updates and horizontal scaling: <pre><code>subresources:\n  status: {}       # For .status field\n  scale:           # For `kubectl scale`\n    specReplicasPath: .spec.replicas\n    statusReplicasPath: .status.replicas\n</code></pre></p>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#e-admission-webhooks","title":"e) Admission Webhooks","text":"<ul> <li>MutatingWebhook: Modify CRs before creation (e.g., inject defaults).  </li> <li>ValidatingWebhook: Reject invalid CRs (e.g., invalid <code>replicas</code> value).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#5-crd-best-practices","title":"5. CRD Best Practices","text":"<ol> <li>Naming Conventions </li> <li>Use <code>&lt;plural&gt;.&lt;group&gt;</code> (e.g., <code>postgresqls.databases.example.com</code>).  </li> <li> <p>Follow DNS subdomain rules (lowercase, no underscores).  </p> </li> <li> <p>Schema Design </p> </li> <li>Use <code>required</code> fields for critical specs.  </li> <li> <p>Avoid frequent schema changes (break compatibility).  </p> </li> <li> <p>Operator Integration </p> </li> <li> <p>Use controller-runtime (Kubebuilder/Operator SDK) for reliable reconciliation.  </p> </li> <li> <p>RBAC </p> </li> <li>Restrict access to CRDs with <code>ClusterRole</code>: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: postgresql-admin\nrules:\n- apiGroups: [\"databases.example.com\"]\n  resources: [\"postgresqls\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n</code></pre></li> </ol>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#6-example-full-crd-operator-flow","title":"6. Example: Full CRD + Operator Flow","text":""},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-1-define-a-crd-cassandraclusteryaml","title":"Step 1: Define a CRD (<code>cassandracluster.yaml</code>)","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: cassandraclusters.db.example.com\nspec:\n  group: db.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema: { ... }\n  scope: Namespaced\n  names:\n    plural: cassandraclusters\n    singular: cassandracluster\n    kind: CassandraCluster\n</code></pre>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-2-create-a-cr-my-cassandrayaml","title":"Step 2: Create a CR (<code>my-cassandra.yaml</code>)","text":"<pre><code>apiVersion: db.example.com/v1\nkind: CassandraCluster\nmetadata:\n  name: my-cassandra\nspec:\n  nodes: 3\n  storage: 50Gi\n</code></pre>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#step-3-deploy-an-operator","title":"Step 3: Deploy an Operator","text":"<ul> <li>The operator watches <code>CassandraCluster</code> CRs and:  </li> <li>Creates <code>StatefulSet</code> + <code>Services</code>.  </li> <li>Handles scaling (<code>kubectl scale cassandracluster my-cassandra --replicas=5</code>).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#7-tools-for-working-with-crds","title":"7. Tools for Working with CRDs","text":"Tool Purpose Kubebuilder Scaffold CRDs + Operators (Go) Operator SDK Build Operators (Go/Ansible/Helm) kustomize Manage CRD YAMLs kubeval Validate CRD schemas"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#8-common-pitfalls","title":"8. Common Pitfalls","text":"<ul> <li>Schema Changes Breaking Clusters: Always support backward compatibility.  </li> <li>Orphaned CRs: Ensure your operator handles CRD deletion gracefully.  </li> <li>Performance Issues: Avoid CRDs with high churn (e.g., frequent updates).  </li> </ul>"},{"location":"CRDs/Deep%20Dive%20into%20CRDs/#summary","title":"Summary","text":"<ul> <li>CRDs = Custom API Objects in Kubernetes.  </li> <li>Operators = Controllers that manage CRs.  </li> <li>Use Cases: Databases, ML, CI/CD, etc.  </li> <li>Advanced Features: Webhooks, versioning, subresources.  </li> </ul> <p>Would you like a hands-on lab or a specific CRD example (e.g., <code>PostgresOperator</code>)? Let me know! \ud83d\ude80</p>"},{"location":"Cert-manager/install-cert-manager/","title":"Install cert manager","text":"<p>Here are the full steps to deploy MailHog with HTTPS (TLS) using Traefik + cert-manager on a <code>k3d</code> cluster:</p>"},{"location":"Cert-manager/install-cert-manager/#step-1-k3d-cluster-with-port-80-443","title":"\u2705 Step 1: <code>k3d</code> Cluster with Port 80 &amp; 443","text":"<pre><code># k3d-mailhog.yaml\napiVersion: k3d.io/v1alpha4\nkind: Simple\nname: mailhog-tls\nports:\n  - port: 80:80     # HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 443:443   # HTTPS\n    nodeFilters:\n      - loadbalancer\n</code></pre> <pre><code>k3d cluster create --config k3d-mailhog.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-2-install-cert-manager","title":"\u2705 Step 2: Install <code>cert-manager</code>","text":"<pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml\n</code></pre> <p>Wait until all pods in <code>cert-manager</code> namespace are ready.</p>"},{"location":"Cert-manager/install-cert-manager/#check-tls-certificate-secret-created","title":"\u2705 Check TLS Certificate Secret Created?","text":"<pre><code>kubectl get certificate\nkubectl describe certificate mailhog-cert\nkubectl get secret mailhog-tls\n\n\nIf all looks fine and still not working, paste the output of:\nkubectl get ingress\nkubectl describe ingress mailhog\nkubectl get certificate\nkubectl describe certificate mailhog-cert\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-3-install-clusterissuer-self-signed-for-dev","title":"\u2705 Step 3: Install <code>ClusterIssuer</code> (Self-Signed for Dev)","text":"<pre><code># selfsigned-clusterissuer.yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: selfsigned-issuer\nspec:\n  selfSigned: {}\n</code></pre> <pre><code>kubectl apply -f selfsigned-clusterissuer.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-4-create-tls-certificate-for-mailhoglocal","title":"\u2705 Step 4: Create TLS Certificate for <code>mailhog.local</code>","text":"<pre><code># mailhog-cert.yaml\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: mailhog-cert\nspec:\n  secretName: mailhog-tls\n  dnsNames:\n    - mailhog.local\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  commonName: mailhog.local\n</code></pre> <pre><code>kubectl apply -f mailhog-cert.yaml\n\ngouse@gouse:~/DevOps/k3d$ k apply -f mailhog-cert.yaml \nWarning: spec.privateKey.rotationPolicy: In cert-manager &gt;= v1.18.0, the default value changed from `Never` to `Always`.\ncertificate.cert-manager.io/mailhog-cert created\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-5-deploy-mailhog-service","title":"\u2705 Step 5: Deploy MailHog + Service","text":"<p>Same as before (<code>mailhog-deployment.yaml</code>):</p> <pre><code>kubectl apply -f mailhog-deployment.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-6-create-https-ingress-with-tls","title":"\u2705 Step 6: Create HTTPS Ingress with TLS","text":"<pre><code># mailhog-ingress-tls.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mailhog\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: websecure\nspec:\n  tls:\n  - hosts:\n    - mailhog.local\n    secretName: mailhog-tls\n  rules:\n  - host: mailhog.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: mailhog\n            port:\n              number: 8025\n</code></pre> <pre><code>kubectl apply -f mailhog-ingress-tls.yaml\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-7-add-to-etchosts","title":"\u2705 Step 7: Add to <code>/etc/hosts</code>","text":"<pre><code>127.0.0.1 mailhog.local\n</code></pre>"},{"location":"Cert-manager/install-cert-manager/#step-8-open-in-browser","title":"\u2705 Step 8: Open in Browser","text":"<pre><code>https://mailhog.local\n</code></pre> <p>You may need to accept the self-signed cert warning if using Chrome/Firefox.</p> <p>Let me know if you want the same using Let\u2019s Encrypt instead of self-signed.</p>"},{"location":"Choosing%20Kubernetes%20by%20Usecases/Kubernetes%20cluster%20types%20on%20your%20usecases/","title":"Kubernetes cluster types on your usecases","text":"<p>choose it wisely.</p> Category Tool/Platform Purpose Open Source Local Dev Clusters Minikube Single-node local testing \u2705 Yes Kind (Kubernetes in Docker) Runs Kubernetes in Docker containers \u2705 Yes K3d Lightweight K3s in Docker \u2705 Yes MicroK8s Lightweight snap-based K8s \u2705 Yes Lightweight Clusters K3s Lightweight Kubernetes for edge/IoT \u2705 Yes Managed Cloud Services GKE (Google Kubernetes Engine) Managed Kubernetes by Google Cloud \u274c No EKS (Elastic Kubernetes Service) AWS managed Kubernetes \u274c No AKS (Azure Kubernetes Service) Azure managed Kubernetes \u274c No Oracle OKE Oracle managed Kubernetes \u274c No IBM Cloud Kubernetes IBM managed Kubernetes \u274c No On-Prem &amp; Hybrid OpenShift (OKD = Open-source base) Enterprise-ready with CI/CD features \u2705 (OKD only) Rancher Cluster management and provisioning \u2705 Yes VMware Tanzu Enterprise Kubernetes with vSphere \u274c No Platform9 SaaS-managed Kubernetes anywhere \u274c No Installer Tools kubeadm Manual cluster setup \u2705 Yes Kubespray Ansible-based multi-node cluster setup \u2705 Yes Kops Production-ready cluster management \u2705 Yes Multi-Cluster Mgmt Anthos Hybrid/multi-cloud by Google \u274c No Fleet (Rancher) GitOps and cluster fleet management \u2705 Yes Specialized Use Amazon EKS Anywhere On-prem Kubernetes via AWS \u274c No Red Hat OpenShift Local (CRC) Local OpenShift cluster for dev/test \u274c No (OKD alt)"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/","title":"Configuring Keycloak for Kubernetes authentication","text":"<p>Configuring Keycloak for Kubernetes authentication allows centralized user management via OIDC (OpenID Connect). Below are the step-by-step instructions to set this up:</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-1-set-up-keycloak","title":"Step 1: Set Up Keycloak","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#11-install-configure-keycloak","title":"1.1 Install &amp; Configure Keycloak","text":"<ul> <li>Deploy Keycloak (standalone or in Kubernetes):   <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install keycloak bitnami/keycloak \\\n  --set auth.adminUser=admin \\\n  --set auth.adminPassword=admin \\\n  --set service.type=LoadBalancer\n</code></pre></li> <li>Access the Keycloak admin console (<code>http://&lt;keycloak-ip&gt;:8080/admin</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#12-create-a-new-realm","title":"1.2 Create a New Realm","text":"<ul> <li>Go to Admin Console \u2192 Add Realm (e.g., <code>kubernetes</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#13-create-a-client-for-kubernetes","title":"1.3 Create a Client for Kubernetes","text":"<ul> <li>Navigate to Clients \u2192 Create:</li> <li>Client ID: <code>kubernetes</code></li> <li>Client Protocol: <code>openid-connect</code></li> <li>Root URL: <code>https://&lt;k8s-api-server&gt;</code></li> <li>Under Settings:</li> <li>Access Type: <code>confidential</code></li> <li>Valid Redirect URIs: <code>*</code> (or restrict to allowed domains)</li> <li>Save.</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#14-create-users-groups","title":"1.4 Create Users &amp; Groups","text":"<ul> <li>Go to Users \u2192 Add User (e.g., <code>dev-user</code>).</li> <li>Set a password under Credentials.</li> <li>(Optional) Assign users to groups (e.g., <code>dev-team</code>, <code>admin-team</code>).</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#15-configure-mappers-for-grouprole-claims","title":"1.5 Configure Mappers (for Group/Role Claims)","text":"<ul> <li>Under Clients \u2192 <code>kubernetes</code> \u2192 Mappers \u2192 Create:</li> <li>Name: <code>groups</code></li> <li>Mapper Type: <code>Group Membership</code></li> <li>Token Claim Name: <code>groups</code></li> <li>Full group path: <code>OFF</code></li> <li>Add to ID token: <code>ON</code></li> <li>Add to access token: <code>ON</code></li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-2-configure-kubernetes-api-server-for-oidc","title":"Step 2: Configure Kubernetes API Server for OIDC","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#21-modify-api-server-flags","title":"2.1 Modify API Server Flags","text":"<p>Edit <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> (on the control plane) and add: <pre><code>spec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --oidc-issuer-url=https://&lt;keycloak-url&gt;/realms/kubernetes\n    - --oidc-client-id=kubernetes\n    - --oidc-username-claim=preferred_username\n    - --oidc-groups-claim=groups\n    - --oidc-ca-file=/etc/ssl/certs/keycloak-ca.crt  # If Keycloak uses HTTPS (recommended)\n</code></pre> - Restart the API server (if not auto-applied):   <pre><code>systemctl restart kubelet\n</code></pre></p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#22-optional-add-keycloak-ca-to-kubernetes-trust-store","title":"2.2 (Optional) Add Keycloak CA to Kubernetes Trust Store","text":"<p>If Keycloak uses HTTPS (self-signed cert): <pre><code>openssl s_client -connect &lt;keycloak-url&gt;:443 -showcerts &lt;/dev/null 2&gt;/dev/null | openssl x509 -outform PEM &gt; keycloak-ca.crt\nsudo cp keycloak-ca.crt /etc/ssl/certs/\nsudo update-ca-certificates\n</code></pre></p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-3-configure-kubectl-for-keycloak-auth","title":"Step 3: Configure kubectl for Keycloak Auth","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#31-install-kubelogin-oidc-helper","title":"3.1 Install <code>kubelogin</code> (OIDC Helper)","text":"<pre><code>wget https://github.com/int128/kubelogin/releases/download/v1.28.0/kubelogin_linux_amd64.zip\nunzip kubelogin_linux_amd64.zip\nsudo mv kubelogin /usr/local/bin/\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#32-create-a-kubeconfig-for-oidc","title":"3.2 Create a kubeconfig for OIDC","text":"<p><pre><code>kubectl config set-credentials dev-user \\\n  --auth-provider=oidc \\\n  --auth-provider-arg=idp-issuer-url=https://&lt;keycloak-url&gt;/realms/kubernetes \\\n  --auth-provider-arg=client-id=kubernetes \\\n  --auth-provider-arg=client-secret=&lt;keycloak-client-secret&gt; \\\n  --auth-provider-arg=refresh-token=&lt;optional&gt; \\\n  --auth-provider-arg=idp-certificate-authority=/etc/ssl/certs/keycloak-ca.crt\n</code></pre> - Get the <code>client-secret</code> from Keycloak (Clients \u2192 <code>kubernetes</code> \u2192 Credentials).</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#33-test-authentication","title":"3.3 Test Authentication","text":"<p><pre><code>kubectl get pods\n</code></pre> - A browser window will open for Keycloak login.</p>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-4-set-up-rbac-for-keycloak-users","title":"Step 4: Set Up RBAC for Keycloak Users","text":""},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#41-create-clusterrolebinding-example","title":"4.1 Create ClusterRoleBinding (Example)","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: keycloak-admin-binding\nsubjects:\n- kind: User\n  name: \"admin@example.com\"  # Must match OIDC username_claim\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#42-optional-map-groups-to-roles","title":"4.2 (Optional) Map Groups to Roles","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-team-binding\n  namespace: default\nsubjects:\n- kind: Group\n  name: \"dev-team\"  # From Keycloak groups\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#step-5-secure-keycloak-optional","title":"Step 5: Secure Keycloak (Optional)","text":"<ul> <li>Enable MFA in Keycloak.</li> <li>Set Token Lifetimes (e.g., 15-minute access tokens).</li> <li>Use Network Policies to restrict API server access.</li> </ul>"},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#troubleshooting","title":"Troubleshooting","text":"Issue Solution <code>OIDC: invalid bearer token</code> Check <code>--oidc-issuer-url</code> matches Keycloak realm. <code>x509: certificate signed by unknown authority</code> Add Keycloak CA to trust store. No groups in token Ensure <code>groups</code> mapper is configured in Keycloak."},{"location":"ClusterAccess/Configuring%20Keycloak%20for%20Kubernetes%20authentication/#final-notes","title":"Final Notes","text":"<p>\u2705 Keycloak + OIDC is ideal for enterprise Kubernetes access. \u2705 RBAC ensures least privilege. \u2705 Short-lived tokens improve security.  </p> <p>Would you like help automating this with Terraform or ArgoCD?</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/","title":"Create a kubeconfig file for the external users","text":"<p>Sharing access to a Kubernetes cluster with other users involves granting them the necessary permissions securely. Below are the steps to achieve this:</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#1-determine-the-authentication-method","title":"1. Determine the Authentication Method","text":"<p>Kubernetes supports several authentication methods: - kubeconfig File (Common for external users) - Service Accounts (For apps/automation) - Static Token File - OIDC Integration (For enterprise setups)</p> <p>For most cases, sharing a kubeconfig file (or generating one for the user) is the easiest approach.</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#2-createmodify-a-kubeconfig-file-for-the-user","title":"2. Create/Modify a kubeconfig File for the User","text":""},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#option-a-share-an-existing-admin-kubeconfig-not-recommended-for-security","title":"Option A: Share an Existing Admin kubeconfig (Not Recommended for Security)","text":"<ul> <li>Simply provide the <code>~/.kube/config</code> file (contains cluster CA, endpoint, and credentials).</li> <li>Risk: Grants full cluster access. Use only in trusted environments.</li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#option-b-generate-a-dedicated-kubeconfig-for-the-user-recommended","title":"Option B: Generate a Dedicated kubeconfig for the User (Recommended)","text":"<ol> <li>Create a new user certificate/key (if using TLS auth):    <pre><code>openssl genrsa -out username.key 2048\nopenssl req -new -key username.key -out username.csr -subj \"/CN=username/O=group\"  # O=group for RBAC\nopenssl x509 -req -in username.csr -CA /path/to/cluster-ca.crt -CAkey /path/to/cluster-ca.key -CAcreateserial -out username.crt -days 365\n</code></pre></li> <li>Create a kubeconfig for the user:    <pre><code>kubectl config set-credentials username \\\n  --client-certificate=username.crt \\\n  --client-key=username.key\nkubectl config set-context username-context \\\n  --cluster=&lt;cluster-name&gt; \\\n  --user=username\nkubectl config use-context username-context\n</code></pre></li> <li>Provide the generated config file to the user.</li> </ol>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#3-set-up-rbac-permissions-mandatory-for-least-privilege","title":"3. Set Up RBAC Permissions (Mandatory for Least Privilege)","text":"<ol> <li>Create a <code>Role</code> or <code>ClusterRole</code> (define permissions):    <pre><code># Example Role (namespace-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <pre><code># Example ClusterRole (cluster-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-viewer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre></li> <li>Bind the Role to the User:    <pre><code># RoleBinding (namespace-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: User\n  name: \"username\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code># ClusterRoleBinding (cluster-scoped)\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: view-nodes\nsubjects:\n- kind: User\n  name: \"username\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: node-viewer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></li> </ol>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#4-distribute-the-kubeconfig-securely","title":"4. Distribute the kubeconfig Securely","text":"<ul> <li>Send the kubeconfig file via encrypted channels (e.g., SSH, GPG, or a secure file-sharing tool).</li> <li>Instruct the user to place it at <code>~/.kube/config</code> or use it with:   <pre><code>kubectl --kubeconfig /path/to/user-config get pods\n</code></pre></li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#5-optional-restrict-cluster-access-further","title":"5. (Optional) Restrict Cluster Access Further","text":"<ul> <li>Network Policies: Limit access to the API server by IP.</li> <li>Short-Lived Certificates: Use tools like <code>cert-manager</code> to auto-expire credentials.</li> <li>Audit Logging: Monitor user activity with <code>--audit-policy-file</code>.</li> </ul>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#6-verify-access","title":"6. Verify Access","text":"<p>Ask the user to run: <pre><code>kubectl get pods\n</code></pre> If permissions are correct, they\u2019ll see only what they\u2019re allowed to.</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#alternative-temporary-access-with-kubectl-proxy","title":"Alternative: Temporary Access with <code>kubectl proxy</code>","text":"<p>If you don\u2019t want to share credentials: <pre><code>kubectl proxy --port=8080 &amp;\n</code></pre> Users can then access the cluster via <code>http://localhost:8080</code> on your machine (use SSH tunneling if remote).</p>"},{"location":"ClusterAccess/Create%20a%20kubeconfig%20file%20for%20the%20external%20users/#summary","title":"Summary","text":"<ol> <li>Choose an auth method (kubeconfig, tokens, OIDC).</li> <li>Generate credentials and a kubeconfig.</li> <li>Define RBAC roles/bindings.</li> <li>Securely share the kubeconfig.</li> <li>Restrict permissions to the least privilege.</li> </ol> <p>For production, consider OpenID Connect (OIDC) or SSO integration (e.g., Dex, Keycloak) for centralized user management.</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/","title":"ConfigMap is a Kubernetes object used to store non confidential data","text":""},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#what-is-a-configmap","title":"What is a ConfigMap?","text":"<p>A ConfigMap is a Kubernetes object used to store non-confidential data in key-value pairs. It allows you to decouple configuration artifacts from container images, making your applications more portable.</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#why-use-configmaps","title":"Why use ConfigMaps?","text":"<ul> <li>Separate configuration from application code</li> <li>Manage environment-specific configurations</li> <li>Update configurations without rebuilding images</li> <li>Share configurations between pods</li> <li>Maintain a single source of truth for configurations</li> </ul>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#when-to-use-configmaps","title":"When to use ConfigMaps?","text":"<ul> <li>Application configuration files</li> <li>Environment variables</li> <li>Command-line arguments</li> <li>Configuration data that doesn't contain sensitive information</li> </ul>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#how-to-use-configmaps","title":"How to use ConfigMaps?","text":""},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#creating-a-configmap","title":"Creating a ConfigMap","text":"<ol> <li> <p>Imperative command:    <pre><code>kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2\n</code></pre></p> </li> <li> <p>From a file:    <pre><code>kubectl create configmap my-config --from-file=path/to/config.file\n</code></pre></p> </li> <li> <p>YAML definition:    <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-config\ndata:\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n  ui.properties: |\n    color.good=purple\n    color.bad=yellow\n</code></pre></p> </li> </ol>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#using-configmaps-in-pods","title":"Using ConfigMaps in Pods","text":"<ol> <li> <p>As environment variables:    <pre><code>env:\n  - name: SPECIAL_LEVEL_KEY\n    valueFrom:\n      configMapKeyRef:\n        name: special-config\n        key: SPECIAL_LEVEL\n</code></pre></p> </li> <li> <p>As volume mounts:    <pre><code>volumes:\n  - name: config-volume\n    configMap:\n      name: game-config\ncontainers:\n  volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n</code></pre></p> </li> </ol>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#updating-configmaps","title":"Updating ConfigMaps","text":"<pre><code>kubectl edit configmap my-config\n# or\nkubectl create configmap my-config --from-literal=key=new-value --dry-run=client -o yaml | kubectl replace -f -\n</code></pre> <p>Note: Pods need to be restarted to pick up ConfigMap changes unless you're using a tool that watches for changes.</p> <p>Here are the commands for working with the ConfigMap manifests and examples provided earlier:</p>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#1-create-configmap-from-literal-values","title":"1. Create ConfigMap from literal values","text":"<pre><code>kubectl create configmap my-config \\\n  --from-literal=key1=value1 \\\n  --from-literal=key2=value2\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#2-create-configmap-from-a-file","title":"2. Create ConfigMap from a file","text":"<pre><code># Create from a single file\nkubectl create configmap game-config --from-file=game.properties\n\n# Create from multiple files\nkubectl create configmap game-config \\\n  --from-file=game.properties \\\n  --from-file=ui.properties\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#3-create-configmap-from-yaml-manifest","title":"3. Create ConfigMap from YAML manifest","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-config\ndata:\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n  ui.properties: |\n    color.good=purple\n    color.bad=yellow\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#4-create-pod-using-configmap-as-environment-variables","title":"4. Create Pod using ConfigMap as environment variables","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-env-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"env\"]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: SPECIAL_LEVEL\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#5-create-pod-using-configmap-as-volume","title":"5. Create Pod using ConfigMap as volume","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-volume-pod\nspec:\n  containers:\n    - name: test-container\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"ls /etc/config &amp;&amp; cat /etc/config/game.properties\"]\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: game-config\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#6-view-configmaps","title":"6. View ConfigMaps","text":"<pre><code># List all ConfigMaps\nkubectl get configmaps\n\n# View details of a specific ConfigMap\nkubectl get configmap my-config -o yaml\nkubectl describe configmap my-config\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#7-update-configmap","title":"7. Update ConfigMap","text":"<pre><code># Edit directly\nkubectl edit configmap my-config\n\n# Replace entirely (using new values)\nkubectl create configmap my-config \\\n  --from-literal=key1=new-value \\\n  --from-literal=key2=new-value2 \\\n  --dry-run=client -o yaml | kubectl replace -f -\n</code></pre>"},{"location":"ConfigMaps/ConfigMap%20is%20a%20Kubernetes%20object%20used%20to%20store%20non-confidential%20data/#8-delete-configmap","title":"8. Delete ConfigMap","text":"<pre><code>kubectl delete configmap my-config\n</code></pre> <p>These commands cover all the scenarios mentioned in your original W3H explanation for ConfigMaps. You can run them in sequence to see how ConfigMaps work in Kubernetes.</p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/","title":"CronJobs in Kubernetes: What, Why, When, and How","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#1-what-is-a-cronjob","title":"1. What is a CronJob?","text":"<p>A CronJob is a Kubernetes resource that runs scheduled tasks (like <code>cron</code> in Linux) at specified intervals. - It creates one-time pods that execute and terminate after completion. - Works similarly to <code>kubectl run --schedule</code> but with better management.</p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#2-why-use-cronjobs","title":"2. Why Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#key-benefits","title":"Key Benefits","text":"<p>\u2705 Automated Scheduling \u2013 Run tasks at fixed times without manual intervention. \u2705 Kubernetes-Native \u2013 Managed by K8s (retries, logging, scaling). \u2705 Idempotent Operations \u2013 Good for cleanup, backups, reports. \u2705 Failure Recovery \u2013 Can retry failed jobs automatically.  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#use-cases","title":"Use Cases","text":"<ul> <li>Database backups (e.g., daily MySQL dump)</li> <li>Log rotation (compress/delete old logs hourly)</li> <li>Batch processing (nightly report generation)</li> <li>API sync jobs (fetch external data every 5 minutes)</li> <li>Cleanup tasks (delete temporary files weekly)</li> </ul>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#3-when-to-use-cronjobs","title":"3. When to Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#best-for","title":"Best For:","text":"<p>\ud83d\udd52 Recurring tasks (e.g., every hour/day/week) \u26a1 Short-lived jobs (not long-running services) \ud83d\udd27 Maintenance operations (cleanup, backups)  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#avoid-when","title":"Avoid When:","text":"<p>\u274c You need long-running processes (use <code>Deployments</code> instead). \u274c You need per-node execution (use <code>DaemonSet</code> instead). \u274c You need real-time processing (use <code>Kafka</code>/<code>RabbitMQ</code>).  </p>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#4-how-to-use-cronjobs","title":"4. How to Use CronJobs?","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-backup\nspec:\n  schedule: \"0 2 * * *\"  # Runs at 2 AM daily (cron syntax)\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:latest\n            command: [\"/bin/sh\", \"-c\", \"pg_dump -U user dbname &gt; /backup/db.sql\"]\n            volumeMounts:\n            - name: backup-volume\n              mountPath: /backup\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-volume\n            hostPath:\n              path: /mnt/backups\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#key-fields","title":"Key Fields","text":"Field Purpose <code>schedule</code> Cron syntax (e.g., <code>\"*/5 * * * *\"</code> = every 5 mins) <code>concurrencyPolicy</code> <code>Allow</code> (default), <code>Forbid</code> (skip if previous job runs), <code>Replace</code> (kill old job) <code>startingDeadlineSeconds</code> Max time to start a missed job (e.g., <code>300</code> = 5 mins) <code>successfulJobsHistoryLimit</code> How many completed jobs to keep (default <code>3</code>) <code>failedJobsHistoryLimit</code> How many failed jobs to keep (default <code>1</code>)"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#5-common-cronjob-patterns","title":"5. Common CronJob Patterns","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#1-running-a-script-inside-a-pod","title":"1. Running a Script Inside a Pod","text":"<pre><code>command: [\"/bin/sh\", \"-c\", \"echo 'Hello at $(date)' &gt;&gt; /logs/log.txt\"]\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#2-using-configmapssecrets","title":"2. Using ConfigMaps/Secrets","text":"<pre><code>envFrom:\n- configMapRef:\n    name: backup-config\n- secretRef:\n    name: db-credentials\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#3-handling-retries","title":"3. Handling Retries","text":"<pre><code>backoffLimit: 3  # Retry 3 times before marking as failed\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#4-preventing-overlapping-runs","title":"4. Preventing Overlapping Runs","text":"<pre><code>concurrencyPolicy: Forbid  # Skip new job if old one is running\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#6-monitoring-debugging","title":"6. Monitoring &amp; Debugging","text":""},{"location":"CronJob/CronJobs%20in%20Kubernetes/#check-cronjobs","title":"Check CronJobs","text":"<pre><code>kubectl get cronjobs\nkubectl describe cronjob/daily-backup\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#view-job-logs","title":"View Job Logs","text":"<pre><code>kubectl logs -l job-name=daily-backup-123456\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#manual-trigger-for-testing","title":"Manual Trigger (For Testing)","text":"<pre><code>kubectl create job --from=cronjob/daily-backup manual-run\n</code></pre>"},{"location":"CronJob/CronJobs%20in%20Kubernetes/#7-alternatives-to-cronjobs","title":"7. Alternatives to CronJobs","text":"Tool When to Use K8s CronJob Best for simple, K8s-native scheduling. Argo Workflows For complex DAG-based jobs. Airflow Advanced scheduling with dependencies. External Cron (e.g., <code>systemd</code>) If you prefer host-level scheduling."},{"location":"CronJob/CronJobs%20in%20Kubernetes/#8-summary","title":"8. Summary","text":"<ul> <li>What? Kubernetes-native scheduled jobs.  </li> <li>Why? Automate recurring tasks (backups, cleanup, sync).  </li> <li>When? For periodic, short-lived jobs (not long-running services).  </li> <li>How? Define <code>schedule</code>, <code>jobTemplate</code>, and optional policies.  </li> </ul>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/","title":"Real-World CronJob Example: Automated Database Backup &amp; Cleanup","text":"<p>Here's a complete, production-ready example of a Kubernetes CronJob that: 1. Backs up a PostgreSQL database daily at 2 AM 2. Compresses the backup 3. Uploads it to AWS S3 4. Cleans up old backups (older than 7 days)</p>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#1-the-cronjob-definition","title":"1. The CronJob Definition","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: postgres-backup\n  namespace: production\nspec:\n  schedule: \"0 2 * * *\"  # Runs at 2 AM UTC daily\n  concurrencyPolicy: Forbid  # Prevents overlapping jobs\n  successfulJobsHistoryLimit: 3  # Keeps logs of last 3 successes\n  failedJobsHistoryLimit: 5      # Keeps logs of last 5 failures\n  jobTemplate:\n    spec:\n      backoffLimit: 2  # Retries twice on failure\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:14.5\n            env:\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-credentials\n                  key: username\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-credentials\n                  key: password\n            - name: S3_BUCKET\n              value: \"my-db-backups\"\n            - name: AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: access_key\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: aws-credentials\n                  key: secret_key\n            command: \n              - /bin/sh\n              - -c\n              - |\n                # Create timestamped backup filename\n                BACKUP_FILE=\"/backups/db-$(date +%Y-%m-%d-%H%M%S).sql\"\n\n                # Dump database\n                pg_dump -U $POSTGRES_USER -h postgres-service.production.svc.cluster.local -d myapp &gt; $BACKUP_FILE\n\n                # Compress backup\n                gzip $BACKUP_FILE\n\n                # Upload to S3\n                aws s3 cp ${BACKUP_FILE}.gz s3://${S3_BUCKET}/\n\n                # Cleanup local files\n                rm ${BACKUP_FILE}.gz\n\n                # Delete backups older than 7 days from S3\n                aws s3 ls s3://${S3_BUCKET}/ | \\\n                awk '{print $4}' | \\\n                while read -r file; do\n                  file_date=$(echo $file | cut -d'-' -f2-4)\n                  if [[ $(date -d \"$file_date\" +%s) -lt $(date -d \"7 days ago\" +%s) ]]; then\n                    aws s3 rm s3://${S3_BUCKET}/$file\n                  fi\n                done\n            volumeMounts:\n            - name: backup-storage\n              mountPath: /backups\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-storage\n            emptyDir: {}\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#2-required-supporting-resources","title":"2. Required Supporting Resources","text":""},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#postgresql-credentials-secret","title":"PostgreSQL Credentials Secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-credentials\n  namespace: production\ntype: Opaque\ndata:\n  username: base64_encoded_username\n  password: base64_encoded_password\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#aws-s3-credentials-secret","title":"AWS S3 Credentials Secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-credentials\n  namespace: production\ntype: Opaque\ndata:\n  access_key: base64_encoded_access_key\n  secret_key: base64_encoded_secret_key\n</code></pre>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#3-key-features-explained","title":"3. Key Features Explained","text":"<ol> <li>Security:</li> <li>Uses Kubernetes Secrets for sensitive data</li> <li> <p>Never stores credentials in the CronJob definition</p> </li> <li> <p>Reliability:</p> </li> <li>Prevents concurrent runs with <code>concurrencyPolicy: Forbid</code></li> <li> <p>Retries failed jobs twice (<code>backoffLimit: 2</code>)</p> </li> <li> <p>Storage Management:</p> </li> <li>Uses <code>emptyDir</code> for temporary storage during backup</li> <li> <p>Automatically cleans up old backups (&gt;7 days) from S3</p> </li> <li> <p>Monitoring:</p> </li> <li>Keeps history of 3 successful and 5 failed runs</li> <li>Easy to check logs with <code>kubectl logs &lt;pod-name&gt;</code></li> </ol>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#4-how-to-deploy-test","title":"4. How to Deploy &amp; Test","text":"<ol> <li> <p>Apply the configuration:    <pre><code>kubectl apply -f postgres-backup-cronjob.yaml\nkubectl apply -f postgres-secret.yaml\nkubectl apply -f aws-secret.yaml\n</code></pre></p> </li> <li> <p>Verify the CronJob:    <pre><code>kubectl get cronjobs -n production\nkubectl describe cronjob/postgres-backup -n production\n</code></pre></p> </li> <li> <p>Manually trigger for testing:    <pre><code>kubectl create job --from=cronjob/postgres-backup manual-test -n production\n</code></pre></p> </li> <li> <p>Check logs:    <pre><code># Get the pod name\nkubectl get pods -n production | grep manual-test\n\n# View logs\nkubectl logs &lt;pod-name&gt; -n production\n</code></pre></p> </li> </ol>"},{"location":"CronJob/Real-World%20CronJob%20Example-%20Automated%20Database%20Backup%20n%20Cleanup/#5-advanced-enhancements","title":"5. Advanced Enhancements","text":"<p>For production environments, consider adding:</p> <ol> <li> <p>Notifications on failure (via Slack/Email):    <pre><code># Add to the container's command:\n- curl -X POST -H 'Content-type: application/json' \\\n  --data '{\"text\":\"Database backup failed!\"}' \\\n  $SLACK_WEBHOOK_URL || true\n</code></pre></p> </li> <li> <p>Prometheus monitoring:    <pre><code>annotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n</code></pre></p> </li> <li> <p>Backup verification:    <pre><code># Add after upload to S3:\naws s3 ls s3://${S3_BUCKET}/$(basename ${BACKUP_FILE}.gz) || \\\n(echo \"Backup verification failed!\" &amp;&amp; exit 1)\n</code></pre></p> </li> </ol> <p>This example provides a complete, production-grade solution for database backups with proper security, reliability, and maintenance features. Would you like any modifications for your specific use case?</p>"},{"location":"CronJob/k8s%20Jobs/","title":"k8s Jobs","text":""},{"location":"CronJob/k8s%20Jobs/#1-what-are-kubernetes-jobs","title":"1. WHAT are Kubernetes Jobs?","text":"<p>Definition: A Kubernetes <code>Job</code> creates one or more Pods to run a short-lived, batch-style task (unlike Deployments, which run continuously).  </p>"},{"location":"CronJob/k8s%20Jobs/#types-of-jobs","title":"Types of Jobs:","text":"Type Description Example Use Case One-shot Job Runs a Pod until completion (exit code 0). Database migration, data export. Parallel Job Runs multiple Pods in parallel (e.g., <code>completions: 5</code>). Batch image processing. CronJob Scheduled Jobs (e.g., daily backups). Log cleanup, report generation."},{"location":"CronJob/k8s%20Jobs/#2-why-use-kubernetes-jobs","title":"2. WHY Use Kubernetes Jobs?","text":""},{"location":"CronJob/k8s%20Jobs/#benefits","title":"Benefits:","text":"<p>\u2705 Automation: Replace manual/cron scripts with Kubernetes-managed tasks. \u2705 Resilience: Auto-retries failed Pods (configurable via <code>backoffLimit</code>). \u2705 Resource Efficiency: Runs on existing K8s clusters (no need for separate VMs).  </p>"},{"location":"CronJob/k8s%20Jobs/#real-world-examples","title":"Real-World Examples:","text":"<ul> <li>Netflix: Uses Jobs for encoding video files in parallel.  </li> <li>Airbnb: Runs nightly data warehouse ETL Jobs.  </li> <li>AI/ML: Training models with distributed Jobs (e.g., Kubeflow).  </li> </ul>"},{"location":"CronJob/k8s%20Jobs/#3-who-manages-kubernetes-jobs","title":"3. WHO Manages Kubernetes Jobs?","text":"Role Responsibilities DevOps Engineers Deploy/CronJob setup, error monitoring. Data Engineers Batch processing (Spark, PyTorch Jobs). SREs Ensure Jobs don\u2019t overload the cluster."},{"location":"CronJob/k8s%20Jobs/#4-how-to-run-jobs-effectively","title":"4. HOW to Run Jobs Effectively?","text":""},{"location":"CronJob/k8s%20Jobs/#a-basic-job-example","title":"A. Basic Job Example","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrator\n        image: postgres:14\n        command: [\"psql\", \"-f\", \"/scripts/migrate.sql\"]\n      restartPolicy: Never  # Or \"OnFailure\"\n  backoffLimit: 3          # Retry failed Pods up to 3 times\n</code></pre>"},{"location":"CronJob/k8s%20Jobs/#b-best-practices","title":"B. Best Practices","text":"<ol> <li>Set Resource Limits: Prevent Jobs from hogging cluster CPU/memory. <pre><code>resources:\n  limits:\n    cpu: \"1\"\n    memory: \"512Mi\"\n</code></pre></li> <li>TTL Controller: Auto-delete finished Jobs to save resources. <pre><code>ttlSecondsAfterFinished: 3600  # Delete Job after 1 hour\n</code></pre></li> <li>Use Active Deadlines: Force-fail long-running Jobs. <pre><code>activeDeadlineSeconds: 300     # Timeout after 5 minutes\n</code></pre></li> </ol>"},{"location":"CronJob/k8s%20Jobs/#c-advanced-patterns","title":"C. Advanced Patterns","text":"<ul> <li>DAG Workflows: Use Argo Workflows or Tekton for multi-step Jobs.  </li> <li>GPU Jobs: Schedule Jobs on GPU nodes (e.g., <code>nvidia.com/gpu: 1</code>).  </li> <li>CI/CD Integration: Run integration tests as Jobs in pipelines.  </li> </ul>"},{"location":"CronJob/k8s%20Jobs/#tools-for-managing-jobs","title":"\ud83d\udee0\ufe0f Tools for Managing Jobs:","text":"Tool Purpose Kubectl Basic Job management (<code>kubectl create job</code>). Argo Workflows Orchestrate complex Job DAGs. Volcano Batch scheduling for high-performance Jobs."},{"location":"CronJob/k8s%20Jobs/#common-pitfalls-fixes","title":"\u26a0\ufe0f Common Pitfalls &amp; Fixes","text":"Issue Solution Zombie Jobs (leftover Pods) Use <code>ttlSecondsAfterFinished</code>. Hung Jobs Set <code>activeDeadlineSeconds</code>. Resource Starvation Add PodAntiAffinity/ResourceQuotas."},{"location":"CronJob/k8s%20Jobs/#checklist-for-production-jobs","title":"\ud83d\ude80 Checklist for Production Jobs","text":"<ol> <li>Logging: Ensure Jobs stream logs to centralized tools (e.g., ELK, Datadog).  </li> <li>Monitoring: Alert on Job failures (Prometheus + Grafana).  </li> <li>Security: Run Jobs with least-privilege ServiceAccounts.  </li> <li>Cleanup: Automate Job deletion with TTL.  </li> </ol>"},{"location":"CronJob/k8s%20Jobs/#real-world-example-cronjob-for-daily-reports","title":"Real-World Example: CronJob for Daily Reports","text":"<pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: sales-report\nspec:\n  schedule: \"0 2 * * *\"  # 2 AM daily\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: reporter\n            image: python:3.9\n            command: [\"python\", \"/scripts/generate_report.py\"]\n          restartPolicy: OnFailure\n</code></pre> <p>**Need a deep dive on a specific Job use case (e.g., ML training, CI/CD integration)? **</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/","title":"CronJobs Vs DaemonSets","text":"<p>No, CronJobs behave differently from DaemonSets in Kubernetes. Here\u2019s the key difference and how they interact with nodes:</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#1-cronjobs-vs-daemonsets-key-differences","title":"1. CronJobs vs. DaemonSets: Key Differences","text":"Feature CronJob DaemonSet Purpose Runs scheduled tasks (like <code>cron</code> in Linux). Ensures a pod runs on all (or some) nodes. Node Coverage Runs pods on any eligible node (based on scheduling rules). Runs one pod per node (automatically covers all nodes). Use Case Periodic jobs (backups, reports, cleanup). Cluster-wide services (logging, monitoring). Persistence Pods are short-lived (die after completion). Pods stay running (persistent daemons). Example <code>0 * * * *</code> (runs every hour). <code>fluentd</code> (runs on every node forever)."},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#2-will-a-cronjob-run-on-all-nodes","title":"2. Will a CronJob Run on All Nodes?","text":"<p>No, by default, a CronJob does not run on all nodes. Instead: - It schedules pods like a regular Deployment, subject to:   - Node Selectors (<code>nodeSelector</code>)   - Taints/Tolerations (if nodes are tainted)   - Affinity/Anti-Affinity rules</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#example-scenario","title":"Example Scenario","text":"<p>If you create a CronJob: <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hourly-cleanup\nspec:\n  schedule: \"0 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleaner\n            image: busybox\n            command: [\"/bin/sh\", \"-c\", \"echo Cleaning...\"]\n          restartPolicy: OnFailure\n</code></pre> - This will run one pod per scheduled execution (on any available node). - It does not guarantee coverage on all nodes.</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#3-how-to-make-a-cronjob-run-on-all-nodes","title":"3. How to Make a CronJob Run on All Nodes?","text":"<p>If you really need a CronJob to run on every node (like a DaemonSet), you have two options:</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#option-1-use-a-daemonset-with-a-cron-like-container","title":"Option 1: Use a DaemonSet with a Cron-like Container","text":"<p>Instead of a CronJob, use a DaemonSet with a container that runs periodically (e.g., using <code>sleep</code> + <code>while</code> loop).</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-cleaner\nspec:\n  selector:\n    matchLabels:\n      name: node-cleaner\n  template:\n    metadata:\n      labels:\n        name: node-cleaner\n    spec:\n      containers:\n      - name: cleaner\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - \"while true; do echo Cleaning...; sleep 3600; done\" # Runs hourly\n</code></pre>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#option-2-use-a-cronjob-affinity-to-force-multi-node-execution","title":"Option 2: Use a CronJob + Affinity to Force Multi-Node Execution","text":"<p>If you must use a CronJob, you can: 1. Run multiple Jobs in parallel (one per node). 2. Use <code>podAntiAffinity</code> to prevent multiple pods on the same node.</p> <p><pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: per-node-cleanup\nspec:\n  schedule: \"0 * * * *\"\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      parallelism: 10 # Adjust based on node count\n      template:\n        spec:\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: job-name\n                    operator: In\n                    values:\n                    - per-node-cleanup\n                topologyKey: kubernetes.io/hostname\n          containers:\n          - name: cleaner\n            image: busybox\n            command: [\"/bin/sh\", \"-c\", \"echo Cleaning...\"]\n          restartPolicy: OnFailure\n</code></pre> (This is complex and not recommended\u2014DaemonSet is better for per-node tasks.)</p>"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#4-which-should-you-use","title":"4. Which Should You Use?","text":"Use Case Best Choice Run a task periodically on all nodes DaemonSet (with an internal loop) Run a task periodically on any node CronJob Run a persistent service on all nodes DaemonSet"},{"location":"DaemonSets/CronJobs%20Vs%20DaemonSets/#5-summary","title":"5. Summary","text":"<ul> <li>CronJobs are for scheduled tasks, not node coverage.</li> <li>DaemonSets ensure per-node execution (better for node-level tasks).</li> <li>If you need both scheduling and per-node execution, use a DaemonSet with a looping container.</li> </ul> <p>Would you like a real-world example (e.g., log rotation on all nodes)? \ud83d\udee0\ufe0f</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/","title":"DaemonSets in Kubernetes","text":"<p>DaemonSets in Kubernetes:</p> Aspect Details What A DaemonSet ensures that a pod runs on every node (or specific nodes) in a Kubernetes cluster. Why To run background or node-specific services, like log collectors, monitoring agents, or storage plugins, on each node. When Use when a service must be on all nodes, such as for metrics collection (Prometheus node exporter), log aggregation (Fluentd), or network daemons (CNI). How Create a DaemonSet YAML with kind: <code>DaemonSet</code>, target <code>nodeSelector</code> (if needed), and deploy it using <code>kubectl apply -f</code>. <p>Let me know if you want the YAML example. Here's a real-time example of a DaemonSet that deploys Fluentd to collect logs from all nodes and push to a log aggregator:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd:v1.16-1\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n</code></pre> <p>This deploys Fluentd as a pod on every node, even masters (due to toleration), and mounts the host logs for collection.</p> <p>Let me know if you want more such DaemonSet use cases (e.g., node-exporter, antivirus, custom script runner, etc.).</p> <p>No, DaemonSets and Taints/Tolerations are related concepts in Kubernetes but serve different purposes. Let me clarify the difference:</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#1-daemonsets-vs-taintstolerations","title":"1. DaemonSets vs. Taints/Tolerations","text":"Feature DaemonSet Taints &amp; Tolerations Purpose Ensures a pod runs on all (or some) nodes in a cluster. Prevents pods from running on certain nodes unless they explicitly tolerate the taint. Use Case Deploying cluster-wide services (logging, monitoring, networking). Reserving nodes for specific workloads (e.g., GPU nodes for ML workloads). How It Works Automatically schedules pods on new nodes. Nodes repel pods unless the pod has a matching toleration. Example <code>fluentd</code> (logging agent) running on every node. A node with <code>NoSchedule</code> taint only allows pods that tolerate it."},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#2-how-they-work-together","title":"2. How They Work Together","text":"<p>DaemonSets often use tolerations to ensure they can run on tainted nodes (e.g., master nodes).</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#example-daemonset-with-tolerations","title":"Example: DaemonSet with Tolerations","text":"<p><pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  template:\n    spec:\n      tolerations:\n      - key: \"node-role.kubernetes.io/master\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n      containers:\n      - name: my-container\n        image: nginx\n</code></pre> - This DaemonSet will run on all nodes, including those with the <code>NoSchedule</code> taint (like master nodes).</p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#3-when-to-use-each","title":"3. When to Use Each?","text":""},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#use-a-daemonset-when","title":"Use a DaemonSet When:","text":"<p>\u2705 You need a pod (e.g., logging agent, monitoring tool) on every node. \u2705 You want Kubernetes to automatically deploy pods when new nodes join.  </p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#use-taintstolerations-when","title":"Use Taints/Tolerations When:","text":"<p>\u2705 You want to restrict certain nodes to only run specific pods (e.g., GPU nodes for AI workloads). \u2705 You need to protect critical nodes (e.g., master nodes) from running arbitrary workloads.  </p>"},{"location":"DaemonSets/DaemonSets%20in%20Kubernetes/#4-summary","title":"4. Summary","text":"<ul> <li>DaemonSet = \"Run this pod on all (or selected) nodes.\"  </li> <li>Taints/Tolerations = \"Only run pods on this node if they explicitly accept the taint.\"  </li> </ul> <p>They work together\u2014DaemonSets often include tolerations to bypass taints and ensure they run everywhere.  </p>"},{"location":"DailyTasks/pod%20exec%20and%20extract%20info%20from%20pods/","title":"Pod exec and extract info from pods","text":"<pre><code># For Shell\noc exec -it myingress-68dc769f7f-8cbpp -- sh\n\n# to run some command and sub commands\noc exec -it ibmmq-68dc769f7f-8cbpp -c qmgr -- cat /etc/mqm/qm.ini\noc exec -it ibmmq-68dc769f7f-8cbpp -- runmqsc &lt; mq.mqsc.rollback\noc exec -it myingress-68dc769f7f-8cbpp -- ls /path/to/dir\n\n# for multiple paths\noc exec -it myingress-68dc769f7f-8cbpp -- ls -ltr /home/user/somepath/onepath /home/user/somepath/twopath\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/","title":"Deployments Strategies","text":"<p>Here's a table summarizing Deployment Strategies with the What, Why, When, and How (W3H) breakdown:  </p> Strategy What? Why? When? How? Rolling Update (Default) Gradually replaces old Pods with new ones in phases. Ensures zero downtime during updates. When high availability is critical (e.g., production environments). Controlled via <code>maxUnavailable</code> (how many Pods can be down) and <code>maxSurge</code> (how many extra Pods can be created). Recreate Terminates all old Pods before creating new ones. Ensures only one version runs at a time. When the app cannot handle multiple versions running simultaneously (e.g., database schema changes). Set <code>strategy.type: Recreate</code> in the Deployment spec. Blue-Green Deploys a new version alongside the old one, then switches traffic at once. Minimizes risk and allows instant rollback. When testing new versions before full release (e.g., canary testing). Requires two identical environments (Deployments + Services). Switch traffic via Service selector update. Canary Slowly rolls out a new version to a subset of users. Reduces risk by testing in production with real users. When gradual validation is needed (e.g., A/B testing). Use labels and selectors to route a portion of traffic to the new version. A/B Testing Routes traffic based on user attributes (headers, cookies). Tests different features with real users. When comparing multiple versions for performance or UX. Requires Service Mesh (Istio, Linkerd) or Ingress controllers (Nginx, Traefik). Type Description Manual Done by developer/admin, not automated Automated (CI/CD) Triggered by code pushes using Jenkins, GitHub Actions, etc. Blue-Green Two environments \u2013 switch traffic between them to avoid downtime Canary Deploy to small % of users first, then gradually roll out Rolling Update instances gradually, one at a time Recreate Stop old version completely, then deploy new one A/B Testing Serve different app versions to test user response"},{"location":"Deployments/Deployments%20Strategies/#example-commands-for-strategies","title":"Example Commands for Strategies","text":""},{"location":"Deployments/Deployments%20Strategies/#rolling-update-default","title":"Rolling Update (Default)","text":"<pre><code>spec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1  # Only 1 Pod down at a time\n      maxSurge: 1        # Only 1 extra Pod created\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#recreate-strategy","title":"Recreate Strategy","text":"<pre><code>spec:\n  strategy:\n    type: Recreate\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#blue-green-deployment-manual-switch","title":"Blue-Green Deployment (Manual Switch)","text":"<pre><code># Deploy v1 (blue)\nkubectl apply -f blue-deployment.yaml\n\n# Deploy v2 (green)\nkubectl apply -f green-deployment.yaml\n\n# Switch traffic from blue to green\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n</code></pre>"},{"location":"Deployments/Deployments%20Strategies/#canary-deployment-partial-traffic","title":"Canary Deployment (Partial Traffic)","text":"<pre><code># Scale canary to 10% of traffic\nkubectl scale deployment myapp-canary --replicas=2  # If main has 18 replicas (10%)\n</code></pre> <p>This table helps choose the right strategy based on uptime requirements, risk tolerance, and testing needs. \ud83d\ude80</p>"},{"location":"Deployments/Deployments%20in%20kubernetes/","title":"Deployments in Kubernetes: A Comprehensive Guide","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#what-are-deployments","title":"What are Deployments?","text":"<p>Deployments are Kubernetes objects that provide declarative updates for Pods and ReplicaSets. They allow you to describe an application's life cycle, including which images to use, the number of pods, and how to update them.</p>"},{"location":"Deployments/Deployments%20in%20kubernetes/#why-use-deployments","title":"Why Use Deployments?","text":"<ol> <li>Rolling updates: Update applications with zero downtime</li> <li>Rollback capability: Revert to previous versions if something goes wrong</li> <li>Scaling: Easily scale your application up or down</li> <li>Self-healing: Automatically replaces failed pods</li> <li>Version tracking: Maintain deployment history</li> </ol>"},{"location":"Deployments/Deployments%20in%20kubernetes/#when-to-use-deployments","title":"When to Use Deployments?","text":"<ul> <li>When you need to run multiple replicas of your application</li> <li>When you need rolling updates and rollbacks</li> <li>For stateless applications (for stateful apps, consider StatefulSets)</li> <li>When you need declarative management of pods</li> </ul>"},{"location":"Deployments/Deployments%20in%20kubernetes/#how-to-use-deployments","title":"How to Use Deployments","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#basic-deployment-yaml","title":"Basic Deployment YAML","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#types-of-deployment-strategies","title":"Types of Deployment Strategies","text":"<ol> <li>Rolling Update (default)</li> <li>Gradually replaces old pods with new ones</li> <li>No downtime during update</li> <li> <p>Controlled by <code>maxUnavailable</code> and <code>maxSurge</code> parameters</p> </li> <li> <p>Recreate</p> </li> <li>Kills all old pods before creating new ones</li> <li>Results in downtime</li> <li>Useful when your application can't run multiple versions simultaneously</li> </ol>"},{"location":"Deployments/Deployments%20in%20kubernetes/#deployment-commands-cheatsheet","title":"Deployment Commands Cheatsheet","text":""},{"location":"Deployments/Deployments%20in%20kubernetes/#basic-commands","title":"Basic Commands","text":"<pre><code># Create a deployment\nkubectl create deployment nginx --image=nginx\n\n# Get deployments\nkubectl get deployments\n\n# Describe a deployment\nkubectl describe deployment &lt;deployment-name&gt;\n\n# Delete a deployment\nkubectl delete deployment &lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#scaling","title":"Scaling","text":"<pre><code># Scale a deployment\nkubectl scale deployment &lt;deployment-name&gt; --replicas=5\n\n# Auto-scale a deployment\nkubectl autoscale deployment &lt;deployment-name&gt; --min=2 --max=5 --cpu-percent=80\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#updates-and-rollbacks","title":"Updates and Rollbacks","text":"<pre><code># Update deployment image\nkubectl set image deployment/&lt;deployment-name&gt; nginx=nginx:1.16.1\n\n# Check rollout status\nkubectl rollout status deployment/&lt;deployment-name&gt;\n\n# Pause a rollout\nkubectl rollout pause deployment/&lt;deployment-name&gt;\n\n# Resume a rollout\nkubectl rollout resume deployment/&lt;deployment-name&gt;\n\n# Rollback to previous version\nkubectl rollout undo deployment/&lt;deployment-name&gt;\n\n# Rollback to specific revision\nkubectl rollout undo deployment/&lt;deployment-name&gt; --to-revision=2\n\n# View rollout history\nkubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#advanced-operations","title":"Advanced Operations","text":"<pre><code># Dry-run to test deployment changes\nkubectl apply -f deployment.yaml --dry-run=client\n\n# View deployment as YAML\nkubectl get deployment &lt;deployment-name&gt; -o yaml\n\n# View deployment as JSON\nkubectl get deployment &lt;deployment-name&gt; -o json\n\n# Edit deployment\nkubectl edit deployment &lt;deployment-name&gt;\n</code></pre>"},{"location":"Deployments/Deployments%20in%20kubernetes/#troubleshooting","title":"Troubleshooting","text":"<pre><code># View deployment events\nkubectl describe deployment &lt;deployment-name&gt; | grep -i events -A10\n\n# View associated pods\nkubectl get pods -l app=&lt;label-selector&gt;\n\n# View pod logs\nkubectl logs &lt;pod-name&gt;\n</code></pre> <p>Remember that deployments work hand-in-hand with other Kubernetes objects like Services to expose your application to the network.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/","title":"Enterprise grade images as safer","text":"<p>Yes, it's normal for UBI (Universal Base Image) images to increase a bit in size when adding packages like <code>unzip</code>.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#details","title":"\ud83d\udccc Details:","text":"<ul> <li>Base UBI Image: <code>213 MB</code></li> <li>After installing <code>unzip</code>: <code>228 MB</code> \u279d \u2705 only +15 MB</li> <li>That\u2019s a reasonable and expected increase.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#why-not-aggressively-reduce-size-here","title":"\ud83e\udde0 Why not aggressively reduce size here:","text":"<ul> <li>UBI is meant for Red Hat compatibility and enterprise-grade support.</li> <li>You keep <code>dnf</code> and required system libs which add size.</li> <li>Stripping too much (like removing <code>dnf</code>, locale files, etc.) may break system tooling or future updates.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#best-practices-already-covered-in-your-dockerfile","title":"\u2705 Best practices (already covered in your Dockerfile):","text":"<ul> <li><code>dnf clean all</code> \u2705</li> <li><code>rm -rf /var/cache/dnf</code> \u2705 You already did the right optimizations.</li> </ul>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#if-you-want-minimal-base-image-with-just-unzip","title":"\u2705 If you want minimal base image with just unzip:","text":"<p>You can consider using <code>alpine</code>: <pre><code>FROM alpine:3.20\nRUN apk add --no-cache unzip\nWORKDIR /app\nCMD [\"sh\"]\n</code></pre></p> <p>Size: ~6 MB But not compatible with RHEL environments or UBI policies.</p>"},{"location":"Docker-Podman-images/Enterprise-grade%20images%20as%20safer/#summary","title":"\ud83d\udd0d Summary","text":"Option Base Size Final Size Notes <code>ubi8/ubi</code> ~213 MB ~228 MB \u2705 Safe, supported, expected size <code>alpine</code> ~5 MB ~6\u20137 MB \u26a0\ufe0f Smaller but not RHEL-compatible So for enterprise or UBI environments, your image is fine. No need to reduce further."},{"location":"Docker-Podman-images/Tag%20images%20to%20push/","title":"Tag images to push","text":"<pre><code>podman tag localhost/ubi-unzip:latest 100.210.10.6:5007/cron-image/busybox:latest\n</code></pre>"},{"location":"Docker-Podman-images/scp%20images%20in%20tar%20format%20to%20remote%20hosts%20-%20offline/","title":"Scp images in tar format to remote hosts   offline","text":"<pre><code>podman save -o ubi-unzip.tar localhost/ubi-unzip:latest\n\nupload to remote host, offline On Remote Host:\npodman load -i ubi-unzip.tar\n</code></pre>"},{"location":"Helm/1.%20Check%20Helm%20Release%28deployed%29%20Information/","title":"1. Check Helm Release(deployed) Information","text":"<pre><code># List all Helm releases to find Traefik\nhelm list -A\n\n# Get detailed information about Traefik release\nhelm status traefik -n traefik-system  # Replace with actual namespace\n\n# Get the values used for Traefik installation\nhelm get values traefik -n traefik-system\n\n# Get all values (including defaults)\nhelm get values traefik -n traefik-system --all\n</code></pre>"},{"location":"Helm/Checking%20Traefik%20Helm%20Configuration/","title":"Checking Traefik Helm Configuration","text":"<pre><code>#!/bin/bash\n\necho \"=== Checking Traefik Helm Configuration ===\"\n\n# 1. Find Traefik release\necho \"1. Finding Traefik Helm releases...\"\nhelm list -A | grep traefik\n\n# 2. Get Traefik values (replace namespace as needed)\necho -e \"\\n2. Getting Traefik Helm values...\"\nTRAEFIK_NAMESPACE=$(kubectl get pods -A | grep traefik | head -1 | awk '{print $1}')\nTRAEFIK_RELEASE=$(helm list -A | grep traefik | head -1 | awk '{print $1}')\n\nif [ ! -z \"$TRAEFIK_RELEASE\" ] &amp;&amp; [ ! -z \"$TRAEFIK_NAMESPACE\" ]; then\n    echo \"Found Traefik release: $TRAEFIK_RELEASE in namespace: $TRAEFIK_NAMESPACE\"\n\n    echo -e \"\\n=== Current Traefik Values ===\"\n    helm get values $TRAEFIK_RELEASE -n $TRAEFIK_NAMESPACE\n\n    echo -e \"\\n=== All Traefik Values (including defaults) ===\"\n    helm get values $TRAEFIK_RELEASE -n $TRAEFIK_NAMESPACE --all | grep -A 20 -B 5 \"ports:\\|entryPoints:\\|tls:\"\nelse\n    echo \"Traefik release not found via Helm\"\nfi\n\n# 3. Check Traefik service ports\necho -e \"\\n3. Checking Traefik service configuration...\"\nkubectl get service -A | grep traefik\nkubectl get service traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -A 10 -B 5 \"ports:\"\n\n# 4. Check for HTTPS/TLS configuration\necho -e \"\\n4. Checking for HTTPS/TLS configuration...\"\nkubectl get service traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -E \"443|websecure|tls\"\n\n# 5. Check Traefik deployment args\necho -e \"\\n5. Checking Traefik deployment arguments...\"\nkubectl get deployment traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -A 20 \"args:\"\n\n# 6. Check if Traefik dashboard is enabled\necho -e \"\\n6. Checking Traefik dashboard configuration...\"\nkubectl get deployment traefik -n $TRAEFIK_NAMESPACE -o yaml | grep -E \"dashboard|api\"\n\necho -e \"\\n=== Traefik Configuration Check Complete ===\"\n</code></pre>"},{"location":"Helm/Helm%20Commands%20-%20Cheatsheet/","title":"Helm Commands   Cheatsheet","text":"Task Command Create chart <code>helm create &lt;name&gt;</code> Install chart <code>helm install &lt;release&gt; &lt;chart-path&gt;</code> Upgrade chart <code>helm upgrade &lt;release&gt; &lt;chart-path&gt;</code> Uninstall chart <code>helm uninstall &lt;release&gt;</code> Show values <code>helm show values &lt;chart&gt;</code> Validate chart <code>helm lint &lt;chart-path&gt;</code> Dry-run install <code>helm install --dry-run --debug ...</code>"},{"location":"Helm/Helm%20charts%20for%20deployments/","title":"Helm charts for deployments","text":""},{"location":"Helm/Helm%20charts%20for%20deployments/#what-is-a-helm-chart","title":"\ud83d\udd27 What is a Helm Chart?","text":"<p>A Helm chart is a packaged Kubernetes application\u2014like a Docker image but for k8s deployments.</p>"},{"location":"Helm/Helm%20charts%20for%20deployments/#helm-chart-structure","title":"\ud83d\udce6 Helm Chart Structure","text":"<pre><code>mychart/\n\u251c\u2500\u2500 Chart.yaml          # Chart metadata (name, version, etc.)\n\u251c\u2500\u2500 values.yaml         # Default config values\n\u251c\u2500\u2500 templates/          # Kubernetes YAML templates\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2514\u2500\u2500 _helpers.tpl    # Template helpers (optional)\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#sample-chart-for-nginx-deployment","title":"\ud83e\uddea Sample Chart for Nginx Deployment","text":""},{"location":"Helm/Helm%20charts%20for%20deployments/#1-create-chart","title":"1. Create chart:","text":"<pre><code>helm create nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#2-edit-valuesyaml","title":"2. Edit <code>values.yaml</code>:","text":"<pre><code>replicaCount: 2\nimage:\n  repository: nginx\n  tag: latest\nservice:\n  type: ClusterIP\n  port: 80\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#3-deploy","title":"3. Deploy:","text":"<pre><code>helm install my-nginx ./nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#4-upgrade","title":"4. Upgrade:","text":"<pre><code>helm upgrade my-nginx ./nginx-app\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#5-uninstall","title":"5. Uninstall:","text":"<pre><code>helm uninstall my-nginx\n</code></pre>"},{"location":"Helm/Helm%20charts%20for%20deployments/#commands-cheat-sheet","title":"\ud83e\uddf0 Commands Cheat Sheet","text":"Task Command Create chart <code>helm create &lt;name&gt;</code> Install chart <code>helm install &lt;release&gt; &lt;chart-path&gt;</code> Upgrade chart <code>helm upgrade &lt;release&gt; &lt;chart-path&gt;</code> Uninstall chart <code>helm uninstall &lt;release&gt;</code> Show values <code>helm show values &lt;chart&gt;</code> Validate chart <code>helm lint &lt;chart-path&gt;</code> Dry-run install <code>helm install --dry-run --debug ...</code>"},{"location":"Ingress/1.%20Host%20based%20ingress%20routing/","title":"1. Host based ingress routing","text":"<p>create below manifest files - todo-app namespace - todo-green deployment with replicas, image, container port 80     - todo-green service with ClusterIP &amp; 80 port - todo-blue deployment     - todo-blue service with ClusterIP &amp; 80 port - ingress with traefik class <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo\n  namespace: todo-app\n  labels:\n    app: todo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todo\n  template:\n    metadata:\n      labels:\n        app: todo\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo\n  namespace: todo-app\nspec:\n  selector:\n    app: todo\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-ingress\n  namespace: todo-app\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todo.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo\n                port:\n                  number: 80\n</code></pre></p> <p>Add this to your <code>/etc/hosts</code>:</p> <pre><code>sudo echo \"127.0.0.1 todo.localhost\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 todo.localhost\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre> <p>Access app via:</p> <pre><code>http://todo.localhost:30080\n</code></pre> <p>(adjust port based on your k3d loadbalancer port mapping).</p>"},{"location":"Ingress/2.%20Path%20based%20ingress%20routing/","title":"2. Path based ingress routing","text":"<p>create below manifest files - todo-app namespace - todo-green deployment with replicas, image, container port 80      (image : gowseshaik/todo-app-green:1.0)     - todo-green service with ClusterIP &amp; 80 port - todo-blue deployment    (image: gowseshaik/todo-app-blue:1.0)     - todo-blue service with ClusterIP &amp; 80 port - ingress with traefik class <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-green\n  template:\n    metadata:\n      labels:\n        app: todo-green\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-blue\n  template:\n    metadata:\n      labels:\n        app: todo-blue\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-green\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-blue\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-ingress\n  namespace: todo-app\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todoapp.localhost.com\n      http:\n        paths:\n          - path: /green\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n          - path: /blue\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-blue\n                port:\n                  number: 80\n</code></pre></p> <p>Add to <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 todoapp.localhost.com\nsudo echo \"127.0.0.1 todoapp.localhost.com\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 todoapp.localhost.com\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre> <p>Access:</p> <pre><code>http://todoapp.localhost.com:30080/green\nhttp://todoapp.localhost.com:30080/blue\n</code></pre>"},{"location":"Ingress/3.%20subdomain%20based%20ingress%20routing/","title":"3. subdomain based ingress routing","text":"<p>Here's the corrected manifest file using subdomain routing, which will completely avoid the CSS/JS loading issues:Key changes with subdomain routing:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: todo-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-green\n  template:\n    metadata:\n      labels:\n        app: todo-green\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-green:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: todo-blue\n  template:\n    metadata:\n      labels:\n        app: todo-blue\n    spec:\n      containers:\n        - name: todo\n          image: gowseshaik/todo-app-blue:1.0\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-green\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-green\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-blue\n  namespace: todo-app\nspec:\n  selector:\n    app: todo-blue\n  ports:\n    - port: 80\n      targetPort: 80\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-green-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: green.todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-blue-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: blue.todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-blue\n                port:\n                  number: 80\n---\n# Optional: Main domain ingress that can redirect to one of the versions\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: todo-main-ingress\n  namespace: todo-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: todo-app-redirect-to-green@kubernetescrd\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: todoapp.localhost.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: todo-green\n                port:\n                  number: 80\n---\n# Optional: Redirect middleware for main domain\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: redirect-to-green\n  namespace: todo-app\nspec:\n  redirectRegex:\n    regex: \"^https?://todoapp.localhost.com/(.*)\"\n    replacement: \"http://green.todoapp.localhost.com/${1}\"\n    permanent: false\n</code></pre> <ol> <li>Green app: Accessible at <code>green.todoapp.localhost.com</code></li> <li>Blue app: Accessible at <code>blue.todoapp.localhost.com</code></li> <li>No path prefixes: Each app serves from the root path <code>/</code>, so CSS/JS files load normally</li> <li>Optional main domain: <code>todoapp.localhost.com</code> can redirect to one version or serve a landing page</li> </ol> <p>DNS Configuration Required:</p> <p>You'll need to configure DNS or your hosts file to point the subdomains to your Traefik ingress:</p> <p>For local development (add to <code>/etc/hosts</code> on Linux/Mac or <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> on Windows):</p> <pre><code>127.0.0.1 green.todoapp.localhost.com\n127.0.0.1 blue.todoapp.localhost.com\n127.0.0.1 todoapp.localhost.com\n</code></pre> <p>For production, configure your DNS provider to create A records:</p> <ul> <li><code>green.todoapp.yourdomain.com</code> \u2192 Your load balancer IP</li> <li><code>blue.todoapp.yourdomain.com</code> \u2192 Your load balancer IP</li> </ul> <p>Testing:</p> <pre><code># Test green version\ncurl http://green.todoapp.localhost.com\n\n# Test blue version  \ncurl http://blue.todoapp.localhost.com\n\n# Test main domain (should redirect to green)\ncurl http://todoapp.localhost.com\n</code></pre> <p>Advantages of subdomain routing:</p> <ul> <li>\u2705 CSS/JS files load correctly (no path issues)</li> <li>\u2705 Cleaner URLs</li> <li>\u2705 Better for SEO if needed</li> <li>\u2705 Easier to manage cookies and local storage per version</li> <li>\u2705 No middleware complexity needed</li> </ul> <p>This approach completely eliminates the static asset loading issues you were experiencing with path-based routing.</p>"},{"location":"Ingress/Access%20Traefik%20Dashboard/","title":"Access Traefik Dashboard","text":"<pre><code># Port forward to access Traefik dashboard\nkubectl port-forward -n traefik-system service/traefik 9000:9000\n\n# Then open browser to: http://localhost:9000/dashboard/\n\n$ kubectl port-forward -n kube-system service/traefik 9000:9000\nerror: Service traefik does not have a service port 9000\n</code></pre>"},{"location":"Ingress/Check%20Traefik%20Configuration%20logs%20for%20config%20issues/","title":"Check Traefik Configuration logs for config issues","text":"<pre><code># Check Traefik deployment\nkubectl get deployment traefik -n traefik-system -o yaml\n\n# Check Traefik service (look for ports configuration)\nkubectl get service traefik -n traefik-system -o yaml\n\n# Check Traefik ConfigMap (if using static configuration)\nkubectl get configmap -n traefik-system\nkubectl describe configmap traefik-config -n traefik-system  # if exists\n\n# Check Traefik pods\nkubectl get pods -n traefik-system\nkubectl describe pod traefik-xxx-xxx -n traefik-system\n</code></pre>"},{"location":"Ingress/Check%20Traefik%20Configuration%20logs%20for%20config%20issues/#check-traefik-logs-for-configuration-issues","title":"Check Traefik Logs for Configuration Issues","text":"<pre><code># Check Traefik logs\nkubectl logs -n traefik-system deployment/traefik\n\n# Follow logs in real-time\nkubectl logs -n traefik-system deployment/traefik -f\n\n# Check for specific configuration errors\nkubectl logs -n traefik-system deployment/traefik | grep -i \"error\\|warn\\|config\"\n</code></pre>"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/","title":"Routing Methods - Real-Time Issues Comparison","text":""},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#main-issues-summary-table","title":"Main Issues Summary Table","text":"Issue Category Path-Based Routing Host-Based Routing Subdomain Routing Static Assets (CSS/JS) \ud83d\udd34 MAJOR ISSUE - Relative paths break, requires app modification \ud83d\udfe2 NO ISSUES - Works perfectly \ud83d\udfe2 NO ISSUES - Works perfectly DNS Management \ud83d\udfe2 SIMPLE - Single domain \ud83d\udd34 COMPLEX - Multiple domains to manage \ud83d\udfe1 MODERATE - Wildcard DNS setup SSL Certificates \ud83d\udfe2 SIMPLE - Single certificate \ud83d\udd34 COMPLEX - Multiple certificates \ud83d\udfe1 MODERATE - Wildcard certificate Application Changes \ud83d\udd34 REQUIRED - Apps must handle base paths \ud83d\udfe2 NONE - Apps work as-is \ud83d\udfe2 NONE - Apps work as-is Cost \ud83d\udfe2 LOW - Single load balancer \ud83d\udd34 HIGH - Multiple hostnames = higher costs \ud83d\udfe1 MODERATE - Single wildcard cert Cookie/Session Management \ud83d\udd34 CONFLICTS - Shared domain causes issues \ud83d\udfe2 ISOLATED - Separate cookie domains \ud83d\udfe1 CONFIGURABLE - Can share or isolate WebSocket Support \ud83d\udd34 PROBLEMATIC - Many libraries don't handle paths \ud83d\udfe2 PERFECT - Full support \ud83d\udfe2 PERFECT - Full support CORS Issues \ud83d\udfe2 NONE - Same origin \ud83d\udfe1 SOME - Cross-origin requests \ud83d\udfe1 SOME - Cross-subdomain requests SEO Impact \ud83d\udfe1 MODERATE - Paths affect ranking \ud83d\udd34 SEPARATE - Different domains compete \ud83d\udfe1 MODERATE - Subdomains share authority Debugging Complexity \ud83d\udd34 HIGH - Path conflicts hard to trace \ud83d\udfe2 LOW - Clear separation \ud83d\udfe2 LOW - Clear separation Legacy App Compatibility \ud83d\udd34 POOR - Often requires code changes \ud83d\udfe2 EXCELLENT - Works with any app \ud83d\udfe2 EXCELLENT - Works with any app"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#specific-real-time-production-issues","title":"Specific Real-Time Production Issues","text":"Problem Type Path-Based Host-Based Subdomain Broken Images/CSS \u274c Very Common (60-80% of apps) \u2705 Never happens \u2705 Never happens Authentication Conflicts \u274c Apps share cookies, login conflicts \u2705 Isolated auth per domain \u26a0\ufe0f Configurable sharing Load Balancer Costs \u2705 $50-100/month \u274c $50-100/month PER hostname \u26a0\ufe0f $50-100/month total Certificate Renewal \u2705 Single cert to manage \u274c Multiple certs, complex automation \u26a0\ufe0f Wildcard cert management DNS Propagation Delays \u2705 No delays for new services \u274c 24-48hr delays for new hostnames \u26a0\ufe0f Instant for new subdomains Mobile App Integration \u26a0\ufe0f Apps must handle base paths \u2705 Standard URL handling \u26a0\ufe0f Some apps struggle with subdomains API Conflicts \u274c <code>/api/users</code> conflicts between apps \u2705 Separate namespaces \u2705 Separate namespaces Caching Issues \u274c CDN caching becomes complex \u2705 Simple per-domain caching \u2705 Simple per-subdomain caching"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#implementation-effort-required","title":"Implementation Effort Required","text":"Task Path-Based Host-Based Subdomain Initial Setup \ud83d\udfe1 Medium (2-4 hours) \ud83d\udd34 High (1-2 days) \ud83d\udfe2 Easy (30 minutes) App Modifications \ud83d\udd34 High (varies by app) \ud83d\udfe2 None \ud83d\udfe2 None DevOps Automation \ud83d\udfe2 Simple \ud83d\udd34 Complex scripts needed \ud83d\udfe1 Moderate automation Monitoring Setup \ud83d\udd34 Complex (path-based metrics) \ud83d\udfe2 Standard per-host metrics \ud83d\udfe2 Standard per-subdomain metrics Troubleshooting \ud83d\udd34 High skill required \ud83d\udfe2 Standard techniques \ud83d\udfe2 Standard techniques"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#failure-impact-severity","title":"Failure Impact Severity","text":"Failure Type Path-Based Host-Based Subdomain CSS/JS Loading Fails \ud83d\udd34 CRITICAL - App completely broken \ud83d\udfe2 Never happens \ud83d\udfe2 Never happens Certificate Expires \ud83d\udfe1 All apps affected \ud83d\udd34 CRITICAL - Specific app down \ud83d\udfe1 All subdomains affected DNS Issues \ud83d\udfe1 All apps affected \ud83d\udd34 CRITICAL - App completely unreachable \ud83d\udfe1 Specific subdomain affected Load Balancer Problems \ud83d\udfe1 All apps affected \ud83d\udfe1 Specific app affected \ud83d\udfe1 All subdomains affected Session/Auth Issues \ud83d\udd34 CRITICAL - Cross-app login conflicts \ud83d\udfe2 Isolated impact \ud83d\udfe1 Configurable impact"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#real-production-examples","title":"Real Production Examples","text":"Company/Use Case Path-Based Host-Based Subdomain Netflix \u274c Not used \u274c Not used \u2705 api.netflix.com, assets.netflix.com GitHub \u2705 github.com/user/repo \u274c Not primary \u2705 api.github.com, gist.github.com Google \u274c Not used \u2705 gmail.com, drive.google.com \u2705 mail.google.com, docs.google.com Slack \u274c Not used \u274c Not used \u2705 api.slack.com, app.slack.com AWS Console \u2705 console.aws.amazon.com/s3 \u274c Not used \u2705 s3.amazonaws.com, ec2.amazonaws.com"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#recommendation-matrix","title":"Recommendation Matrix","text":"Your Situation Best Choice Why Small startup, budget-conscious \ud83d\udfe1 Path-Based Lowest cost, but prepare for CSS/JS issues Medium company, 5-20 services \u2705 Subdomain Best balance of simplicity and functionality Large enterprise, separate teams \u2705 Host-Based Maximum isolation, teams can manage independently Legacy applications \u274c Never Path-Based Use Subdomain or Host-Based SPA/React/Vue apps \ud83d\udfe1 Path-Based OK Modern frameworks handle base paths better Blue-Green deployments \u2705 Subdomain Easy switching between versions Microservices architecture \u2705 Subdomain Clean service separation"},{"location":"Ingress/Diff%20between%20Host-path-subdomain%20based%20ingress%20routing/#bottom-line-for-your-todo-app","title":"Bottom Line for Your Todo App","text":"Aspect Recommendation Current Issue CSS/JS not loading due to relative paths in path-based routing Quick Fix Switch to subdomain routing: <code>green.todoapp.localhost.com</code> Long-term Best Subdomain routing - most production-ready approach Avoid Continuing with path-based without fixing Nginx configuration The table clearly shows that your current CSS/JS loading issue is the #1 problem with path-based routing. For your specific todo app scenario: <p>\ud83c\udfaf Immediate Recommendation: Switch to subdomain routing (<code>green.todoapp.localhost.com</code>, <code>blue.todoapp.localhost.com</code>) - it eliminates your static asset problems completely and is the most production-ready approach for your use case.</p> <p>\ud83d\udd27 Alternative: If you must stick with path-based routing, use the Nginx configuration I provided earlier to properly handle the <code>/green</code> and <code>/blue</code> paths.</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/","title":"How to confirm Ingress resources, routes, and active requests","text":"<p>To confirm Ingress resources, routes, and active requests, follow these steps:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#1-list-all-ingress-resources","title":"\u2705 1. List All Ingress Resources","text":"<pre><code>kubectl get ingress -A\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#2-inspect-ingress-rules-and-routes","title":"\u2705 2. Inspect Ingress Rules and Routes","text":"<pre><code>kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Look for:</p> <ul> <li> <p><code>Host</code></p> </li> <li> <p><code>Path</code></p> </li> <li> <p><code>Service Name</code></p> </li> <li> <p><code>IngressClassName</code></p> </li> </ul>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#3-check-ingress-controller-logs-for-requests","title":"\u2705 3. Check Ingress Controller Logs for Requests","text":"<p>For NGINX:</p> <pre><code>kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>For Traefik:</p> <pre><code>kubectl logs -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre> <p>For Istio:</p> <pre><code>kubectl logs -n istio-system -l app=istio-ingressgateway\n</code></pre> <p>Perfect \u2014 if Traefik is already installed by default with K3d, here's the minimal setup to route traffic using ClusterIP services like ALB:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#use-default-k3d-traefik-with-ingress-alb-style","title":"\u2705 Use Default K3d Traefik with Ingress (ALB Style)","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-1-exposing-traefik-ports-in-k3d-cluster-config-yaml-file","title":"**Step 1: exposing Traefik ports in k3d cluster config yaml file","text":"<p>You're exposing Traefik port 80 to host port <code>30080</code> like this: <pre><code>- port: 30080:80\n  nodeFilters:\n    - loadbalancer\n</code></pre></p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-2-deploy-sample-app","title":"Step 2: Deploy Sample App","text":"<pre><code>kubectl create namespace demo\n\nkubectl create deployment whoami --image=traefik/whoami -n demo\nkubectl expose deployment whoami --port=80 --name=whoami -n demo\n</code></pre> <p>\u2705 This app uses a <code>ClusterIP</code> service (default)</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-3-create-ingress-resource","title":"Step 3: Create Ingress Resource","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#whoami-ingressyaml","title":"whoami-ingress.yaml","text":"<pre><code># whoami-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: whoami-ingress\n  namespace: demo\nspec:\n  rules:\n    - host: whoami.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: whoami\n                port:\n                  number: 80\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f whoami-ingress.yaml\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#gousegousedevopsk3d-k-get-ingress-a-namespace-name-class-hosts-address-ports-age-demo-whoami-ingress-traefik-whoamilocalhost-172180217218031721804-80-8m13s","title":"<pre><code>gouse@gouse:~/DevOps/k3d$ k get ingress -A\nNAMESPACE   NAME             CLASS     HOSTS              ADDRESS                            PORTS   AGE\ndemo        whoami-ingress   traefik   whoami.localhost   172.18.0.2,172.18.0.3,172.18.0.4   80      8m13s\n</code></pre>","text":""},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-3-add-host-mapping","title":"Step 3: Add Host Mapping","text":"<pre><code>sudo echo \"127.0.0.1 whoami.localhost\" &gt;&gt; /etc/hosts # Localhost for homeLab\n\nsudo echo \"172.18.0.2 whoami.localhost\" &gt;&gt; /etc/hosts # LoadBalancer IP for Production grade setup and configurations\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#step-4-access-in-browser","title":"Step 4: Access in Browser","text":"<p>Open: http://whoami.localhost</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#optional-if-traefik-doesnt-route","title":"\u26a0\ufe0f Optional: If Traefik Doesn't Route","text":"<p>Check if the default Traefik has IngressClass enabled:</p> <pre><code>kubectl get ingressclass\n</code></pre> <p>You should see something like:</p> <pre><code>NAME      CONTROLLER             PARAMETERS   AGE\ntraefik   traefik.io/ingress-controller   ...   ...\n</code></pre> <p>If not, you can explicitly set it in your Ingress:</p> <pre><code>spec:\n  ingressClassName: traefik\n</code></pre> <p>You're now running a ClusterIP-only app with Ingress routing through default Traefik, exactly like an AWS ALB setup. Let me know if you want TLS added.</p> <p>Here's a Traefik Dashboard Ingress manifest for your K3d setup using existing port <code>30900:9000</code> (from your YAML) and ClusterIP service:</p>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#traefik-dashboardyaml","title":"\u2705 <code>traefik-dashboard.yaml</code>","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: traefik-dashboard\n  namespace: kube-system\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: traefik.localhost\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: traefik\n                port:\n                  number: 9000\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#etchosts-entry","title":"\u2705 <code>/etc/hosts</code> Entry","text":"<pre><code>127.0.0.1 traefik.localhost\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20Ingress%20resources%2C%20routes%2C%20and%20active%20requests/#access-dashboard","title":"\u2705 Access Dashboard","text":"<pre><code>http://traefik.localhost:30900\n</code></pre> <p>OR (if you prefer Ingress via port 80 \u2192 30080):</p> <pre><code>http://traefik.localhost:30080\n</code></pre> <p>In that case, change <code>port: 9000</code> to <code>port: 80</code> in the Ingress and make sure Traefik listens on web entrypoint for both.</p> <p>Let me know which way you prefer:</p> <ul> <li> <p>Direct: <code>30900</code></p> </li> <li> <p>Through Ingress: <code>30080</code>     I\u2019ll adjust the manifest accordingly.</p> </li> </ul>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/","title":"How to confirm the exact Ingress Controller used in your Kubernetes cluster","text":"<p>To confirm the exact Ingress Controller used in your Kubernetes cluster, follow these steps:</p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#1-check-ingressclass-resource","title":"\u2705 1. Check IngressClass Resource","text":"<pre><code>kubectl get ingressclass\n</code></pre> <p>Output example:</p> <pre><code>NAME             CONTROLLER                   ...\nnginx            k8s.io/ingress-nginx         ...\ntraefik          traefik.io/ingress-controller ...\n</code></pre>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#2-describe-the-active-ingressclass","title":"\u2705 2. Describe the Active IngressClass","text":"<pre><code>kubectl describe ingressclass &lt;name&gt;\n</code></pre> <p>Look for the <code>controller:</code> field. It tells you which controller manages it.</p>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#3-verify-the-ingress-controller-deployment","title":"\u2705 3. Verify the Ingress Controller Deployment","text":"<p>Use the controller name from above to filter pods:</p> <pre><code>kubectl get pods --all-namespaces | grep ingress\n</code></pre> <p>Or more specifically:</p> <pre><code>kubectl get deployments --all-namespaces | grep ingress\n</code></pre> <p>Look for deployments like:</p> <ul> <li> <p><code>ingress-nginx-controller</code></p> </li> <li> <p><code>traefik</code></p> </li> <li> <p><code>istio-ingressgateway</code></p> </li> </ul>"},{"location":"Ingress/How%20to%20confirm%20the%20exact%20Ingress%20Controller%20used%20in%20your%20Kubernetes%20cluster/#4-confirm-by-ingress-resource","title":"\u2705 4. Confirm by Ingress Resource","text":"<pre><code>kubectl get ingress -A\n</code></pre> <p>Then:</p> <pre><code>kubectl describe ingress &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Check if it refers to a specific <code>ingressClassName</code>.</p> <p>Let me know if you want a script to automate this.</p>"},{"location":"Ingress/Types%20of%20Ingress%20Controllers%20with%20their%20pros%2C%20cons%2C%20and%20pricing%20details/","title":"Types of Ingress Controllers with their pros, cons, and pricing details","text":"<p>Here's a table listing popular types of Ingress Controllers with their pros, cons, and pricing details:</p> Ingress Controller Pros Cons Pricing NGINX Ingress Widely used, well-documented, flexible config, supports custom rules Limited advanced routing, lower performance at scale Free (Open Source), NGINX Plus is commercial (~$2,500/year/node) HAProxy Ingress High performance, low latency, advanced routing Smaller community, fewer features than NGINX Free (Open Source), Enterprise support available (custom pricing) Traefik Auto-discovery, native K8s, easy dashboard Learning curve, fewer advanced features vs. NGINX Free (Open Source), Traefik Enterprise (starts ~$2,000/year) Istio Ingress Gateway Deep integration with service mesh, security features (mTLS, etc.) Heavy, complex to configure and manage Free (Open Source) Kong Ingress Controller API Gateway features, plugins, scalability Paid features gated in Enterprise version OSS free, Kong Enterprise (starts ~$2,500+/year) AWS ALB Ingress Managed by AWS, auto-scalable, native integration AWS only, slower config propagation Pay-per-use (based on ALB usage - ~$0.0225/hr + LCU costs) GKE Ingress (Google) Managed, autoscaling, native GCP integration GCP only, limited customization Free tier + Pay-per-use (based on Load Balancer usage) Azure Application Gateway Ingress Native Azure integration, WAF support Azure only, limited custom routing Pay-per-use (~$0.008/hr + data processing fees) Ambassador Edge Stack API Gateway features, easy developer interface Full features need paid version OSS free, Enterprise (starts ~$3,000+/year) Contour Built on Envoy, fast updates, CRD support Smaller community, fewer features than Istio or NGINX Free (Open Source) <p>Let me know if you want a filtered list for cloud-native, performance-focused, or self-hosted only.</p>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/","title":"Complete Traefik Setup Guide for k3d","text":"<p>Great! I'm glad it's working now. Let me provide you with a comprehensive, clean setup guide for Traefik with dashboard on k3d:This guide provides a complete, production-ready setup for Traefik on k3d. The key points to remember:</p> <ol> <li>Disable default k3s Traefik to install your own version with full control</li> <li>Use NodePort service type with specific port mappings matching your k3d config</li> <li>Enable dashboard explicitly with <code>--api.dashboard=true</code> and <code>--api.insecure=true</code> flags</li> <li>Access patterns:<ul> <li>Direct dashboard: <code>http://localhost:30900/dashboard/</code></li> <li>Through ingress: <code>http://traefik.localhost:30080/</code></li> </ul> </li> <li>Always include the trailing slash when accessing <code>/dashboard/</code></li> </ol> <p>The setup is now modular and easy to extend with additional services, middleware, and security features. You can use this as a template for all your k3d clusters with Traefik.</p>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#complete-traefik-setup-guide-for-k3d","title":"Complete Traefik Setup Guide for k3d","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-1-create-k3d-cluster-configuration","title":"Step 1: Create k3d Cluster Configuration","text":"<p>Create <code>dev-cluster.yaml</code>:</p> <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:9000     # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\n  # Add your other application ports here\noptions:\n  k3s:\n    extraArgs:\n      - arg: --disable=traefik\n        nodeFilters:\n          - server:*\n  kubeconfig:\n    updateDefaultKubeconfig: true\n    switchCurrentContext: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-2-create-the-cluster","title":"Step 2: Create the Cluster","text":"<pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-3-install-traefik-with-helm","title":"Step 3: Install Traefik with Helm","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#31-add-helm-repository","title":"3.1 Add Helm Repository","text":"<pre><code>helm repo add traefik https://traefik.github.io/charts\nhelm repo update\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#32-create-traefik-namespace","title":"3.2 Create Traefik Namespace","text":"<pre><code>kubectl create namespace traefik-system\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#33-create-traefik-values-file","title":"3.3 Create Traefik Values File","text":"<p>Create <code>traefik-values.yaml</code>:</p> <pre><code># Traefik Helm Chart Values\nimage:\n  pullPolicy: IfNotPresent\n\ndeployment:\n  enabled: true\n  replicas: 1\n  annotations: {}\n  podAnnotations: {}\n  additionalContainers: []\n  initContainers: []\n\n# Configure ports\nports:\n  web:\n    port: 8000\n    expose:\n      default: true\n    exposedPort: 80\n    nodePort: 30080\n    protocol: TCP\n  websecure:\n    port: 8443\n    expose:\n      default: true\n    exposedPort: 443\n    nodePort: 30443\n    protocol: TCP\n  traefik:\n    port: 9000\n    expose:\n      default: true\n    exposedPort: 9000\n    nodePort: 30900\n    protocol: TCP\n\n# Service configuration\nservice:\n  enabled: true\n  type: NodePort\n  annotations: {}\n  labels: {}\n  spec: {}\n  loadBalancerSourceRanges: []\n  externalIPs: []\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    annotations: {}\n    labels: {}\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true\n\n# Configure providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n    allowExternalNameServices: true\n    allowEmptyServices: true\n  kubernetesIngress:\n    enabled: true\n    allowExternalNameServices: true\n    allowEmptyServices: true\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--global.checknewversion=false\"\n  - \"--global.sendanonymoususage=false\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--api.dashboard=true\"\n  - \"--api.insecure=true\"\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n  - \"--entrypoints.web.http.redirections.entrypoint.to=websecure\"\n  - \"--entrypoints.web.http.redirections.entrypoint.scheme=https\"\n\n# Enable persistent storage for acme certificates\npersistence:\n  enabled: false\n  name: data\n  accessMode: ReadWriteOnce\n  size: 128Mi\n  storageClass: \"\"\n  path: /data\n  annotations: {}\n\n# Security configurations\npodSecurityContext:\n  fsGroup: 65532\n\nsecurityContext:\n  capabilities:\n    drop: [ALL]\n  readOnlyRootFilesystem: true\n  runAsGroup: 65532\n  runAsNonRoot: true\n  runAsUser: 65532\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#34-install-traefik","title":"3.4 Install Traefik","text":"<pre><code>helm install traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values.yaml \\\n  --wait\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-4-configure-dashboard-access","title":"Step 4: Configure Dashboard Access","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#41-create-ingressroute-for-dashboard","title":"4.1 Create IngressRoute for Dashboard","text":"<p>Create <code>traefik-dashboard-ingress.yaml</code>:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik-system\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n      kind: Rule\n      services:\n        - name: api@internal\n          kind: TraefikService\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f traefik-dashboard-ingress.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#42-optional-secure-dashboard-with-basic-auth","title":"4.2 (Optional) Secure Dashboard with Basic Auth","text":"<p>Generate password:</p> <pre><code># Generate password (replace 'admin' and 'your-password')\nhtpasswd -nb admin your-password | base64\n</code></pre> <p>Create <code>traefik-dashboard-auth.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\ndata:\n  users: |\n    &lt;base64-encoded-htpasswd-output&gt;\n---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard-secure\n  namespace: traefik-system\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n      kind: Rule\n      middlewares:\n        - name: traefik-dashboard-auth\n      services:\n        - name: api@internal\n          kind: TraefikService\n</code></pre> <p>Apply authentication:</p> <pre><code>kubectl apply -f traefik-dashboard-auth.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-5-configure-dns-optional","title":"Step 5: Configure DNS (Optional)","text":"<p>Add to <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 traefik.localhost\n127.0.0.1 app.localhost\n# Add other domains as needed\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-6-deploy-sample-application","title":"Step 6: Deploy Sample Application","text":"<p>Create <code>whoami-app.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  selector:\n    app: whoami\n  ports:\n    - port: 80\n      targetPort: 80\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`whoami.localhost`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre> <p>Deploy the application:</p> <pre><code>kubectl apply -f whoami-app.yaml\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-7-access-methods","title":"Step 7: Access Methods","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#dashboard-access","title":"Dashboard Access:","text":"<ul> <li>Direct Port: http://localhost:30900/dashboard/</li> <li>Via Ingress: http://traefik.localhost:30080/dashboard/</li> </ul>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#application-access","title":"Application Access:","text":"<ul> <li>Via Ingress: http://whoami.localhost:30080/</li> </ul>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-8-useful-commands","title":"Step 8: Useful Commands","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#check-traefik-status","title":"Check Traefik Status:","text":"<pre><code># Check pods\nkubectl get pods -n traefik-system\n\n# Check services\nkubectl get svc -n traefik-system\n\n# Check logs\nkubectl logs -n traefik-system -l app.kubernetes.io/name=traefik -f\n\n# Check IngressRoutes\nkubectl get ingressroute -A\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#debug-commands","title":"Debug Commands:","text":"<pre><code># Check if dashboard is accessible\ncurl http://localhost:30900/api/overview\n\n# Test ingress routing\ncurl -H \"Host: whoami.localhost\" http://localhost:30080/\n\n# Port forward for debugging\nkubectl port-forward -n traefik-system deployment/traefik 9000:9000\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#step-9-common-middleware-examples","title":"Step 9: Common Middleware Examples","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#rate-limiting","title":"Rate Limiting:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: rate-limit\nspec:\n  rateLimit:\n    average: 100\n    burst: 50\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#headers","title":"Headers:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: security-headers\nspec:\n  headers:\n    frameDeny: true\n    sslRedirect: true\n    browserXssFilter: true\n    contentTypeNosniff: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#redirect","title":"Redirect:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: redirect-https\nspec:\n  redirectScheme:\n    scheme: https\n    permanent: true\n</code></pre>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#troubleshooting","title":"Troubleshooting","text":""},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#dashboard-not-accessible","title":"Dashboard Not Accessible:","text":"<ol> <li> <p>Check if the service has port 9000 exposed:</p> <pre><code>kubectl get svc traefik -n traefik-system -o yaml | grep -A 5 \"port: 9000\"\n</code></pre> </li> <li> <p>Verify dashboard is enabled:</p> <pre><code>kubectl get deployment traefik -n traefik-system -o yaml | grep -E \"(dashboard|insecure)\"\n</code></pre> </li> <li> <p>Check endpoints:</p> <pre><code>kubectl get endpoints -n traefik-system\n</code></pre> </li> </ol>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#ingress-not-working","title":"Ingress Not Working:","text":"<ol> <li> <p>Check IngressRoute:</p> <pre><code>kubectl describe ingressroute -n &lt;namespace&gt; &lt;name&gt;\n</code></pre> </li> <li> <p>Check Traefik logs for errors:</p> <pre><code>kubectl logs -n traefik-system -l app.kubernetes.io/name=traefik --tail=100\n</code></pre> </li> </ol>"},{"location":"K3d/Complete%20Traefik%20Setup%20Guide%20for%20k3d/#best-practices","title":"Best Practices","text":"<ol> <li>Always use HTTPS in production - Configure proper TLS certificates</li> <li>Secure the dashboard - Use authentication middleware</li> <li>Use namespaces - Organize your IngressRoutes by namespace</li> <li>Monitor logs - Set up proper logging and monitoring</li> <li>Resource limits - Set appropriate resource requests and limits</li> <li>Regular updates - Keep Traefik updated for security patches</li> </ol> <p>This setup provides a solid foundation for using Traefik in your k3d development environment!</p>"},{"location":"K3d/K3d%20cluster%20setup/","title":"K3d cluster setup","text":"<p><code>k3d</code> also supports cluster creation using a YAML config file, similar to `kind.</p>"},{"location":"K3d/K3d%20cluster%20setup/#1-sample-k3d-cluster-config-yaml-k3d-configyaml","title":"\u2705 1. Sample k3d Cluster Config YAML (<code>k3d-config.yaml</code>)","text":"<pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:3000     # Grafana (optional)\n    nodeFilters:\n      - loadbalancer\n</code></pre>"},{"location":"K3d/K3d%20cluster%20setup/#2-create-cluster-using-config","title":"\u2705 2. Create Cluster Using Config","text":"<pre><code>k3d cluster create --config k3d-config.yaml\n</code></pre> <p>This will expose your services directly on the host ports without needing port-forward. You can add or change ports anytime by editing the config.</p> <p>Let me know if you want an example with volumes or registries too.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/","title":"K3d cluster with traefik dashboard","text":"<p>To enable and expose the Traefik dashboard in <code>k3d</code> at cluster creation, you need to do two things in the config file:</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#1-enable-traefik-dashboard-via-k3s-arg","title":"\u2705 1. Enable Traefik Dashboard via k3s Arg","text":"<p>Add this under the <code>k3s</code> section:</p> <pre><code>k3s:\n  extraArgs:\n    - arg: \"--traefik.dashboard=true\"\n      nodeFilters:\n        - server:*\n</code></pre>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#2-expose-traefik-dashboard-port-default-is-9000","title":"\u2705 2. Expose Traefik Dashboard Port (default is 9000)","text":"<p>Add this under the <code>ports:</code> section:</p> <pre><code>  - port: 30900:9000      # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\n</code></pre>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#final-config-k3d-configyaml","title":"\u2705 \u2705 Final Config: <code>k3d-config.yaml</code>","text":"<pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:3000     # Grafana (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:9000     # Traefik Dashboard\n    nodeFilters:\n      - loadbalancer\nk3s:                      # not working for k3s version, so you can remove it.\n  extraArgs:\n    - arg: \"--traefik.dashboard=true\"\n      nodeFilters:\n        - server:*\n</code></pre> <p>but it getting below error for k3s  <pre><code> k3d cluster create --config dev-cluster.yaml\nFATA[0000] Schema Validation failed for config file dev-cluster.yaml: - (root): Additional property k3s is not allowed\n</code></pre></p> <p>You're seeing the error because <code>k3d v5+</code> no longer supports the <code>k3s:</code> block in the config file. That field is not valid in the latest schema (v1alpha5).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#solution-use-k3s-arg-from-cli-instead","title":"\u2705 Solution: Use <code>--k3s-arg</code> from CLI instead","text":"<p>Yes, exactly.</p> <p>Remove the entire <code>k3s:</code> section from your config file.</p> <p>Use only the cluster definition and port mappings in the YAML, then pass k3s arguments like <code>--traefik.dashboard=true</code> via the CLI when creating the cluster.</p> <p>Since the config file doesn't support <code>k3s:</code> anymore, you can enable the Traefik dashboard like this:</p> <pre><code>k3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre> <p>This passes the arg <code>--traefik.dashboard=true</code> to the first server node.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#optional-alias-for-simplicity","title":"\u2705 Optional: Alias for Simplicity","text":"<p>If you want to reuse this, put it in a shell script:</p> <pre><code>#!/bin/bash\nk3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:*\"\n</code></pre> <p>Let me know if you want a workaround to fully control the k3s startup args from config (e.g., custom entrypoint container).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#why-cant-we-use-these-values-in-config-file","title":"why can't we use these values in config file ?","text":"<p>Great question \u2014 here\u2019s a straight answer:</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#why-you-cant-use-k3s-in-k3d-config-anymore","title":"\ud83d\udd34 Why you can't use <code>k3s:</code> in <code>k3d</code> config anymore","text":"<ul> <li> <p>In <code>k3d v5+</code>, the config file follows a strict JSON schema (<code>v1alpha5</code>) that does not include a <code>k3s:</code> field.</p> </li> <li> <p>The field was removed to simplify the config format and separate cluster structure (YAML) from runtime behavior (CLI flags).</p> </li> <li> <p>So, <code>k3s:</code> and <code>extraArgs</code> were deprecated and replaced by <code>--k3s-arg</code> flags in the CLI only.</p> </li> </ul>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#current-rule","title":"\u2705 Current Rule:","text":"<ul> <li> <p>Cluster topology (nodes, ports, volumes, registries) \u2192 \u2705 defined in YAML.</p> </li> <li> <p>Runtime args (like <code>--traefik.*</code>, <code>--disable-*</code>, <code>--tls-san</code>, etc.) \u2192 \u274c not in YAML \u2192 \u2705 only via CLI <code>--k3s-arg</code>.</p> </li> </ul>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#legacy-compatibility","title":"\ud83d\udd04 Legacy Compatibility?","text":"<p>No official backward support. Even if you add <code>k3s:</code> in the YAML, <code>k3d</code> will fail schema validation.</p> <p>Let me know if you want to build a wrapper that merges YAML + CLI for easier reuse.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#access-traefik-dashboard","title":"\ud83c\udf10 Access Traefik Dashboard:","text":"<p>After cluster creation:</p> <pre><code>http://localhost:30900/dashboard/\n</code></pre> <p>Let me know if you want to secure it or route it via Ingress host like <code>traefik.localhost</code>.</p> <p>If your cluster creation hangs on starting the server, here are the most common and effective fixes:</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#1-check-for-docker-conflicts","title":"\u2705 1. Check for Docker Conflicts","text":"<p>Run this to see if containers are stuck or conflicting:</p> <pre><code>docker ps -a | grep k3d\n</code></pre> <p>If any containers are in a <code>Created</code> or <code>Exited</code> state, clean them:</p> <pre><code>k3d cluster delete dev-cluster\ndocker system prune -f\n</code></pre> <p>Then try again.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#2-make-sure-ports-arent-already-in-use","title":"\u2705 2. Make Sure Ports Aren\u2019t Already in Use","text":"<p>You\u2019re mapping ports like <code>31446</code>, <code>31447</code>, etc. Run:</p> <pre><code>lsof -i -P -n | grep LISTEN\n</code></pre> <p>Look for conflicting ports. If a port is in use, either:</p> <ul> <li> <p>Stop the app using it, or</p> </li> <li> <p>Change the port in <code>dev-cluster.yaml</code></p> </li> </ul>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#3-disable-traefik-temporarily","title":"\u2705 3. Disable Traefik Temporarily","text":"<p>Test if the arg is causing the issue by removing it:</p> <pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre> <p>If that works, try:</p> <pre><code>k3d cluster create --config dev-cluster.yaml --k3s-arg \"--disable=traefik@server:0\"\n</code></pre> <p>Then install your own ingress later (e.g., Traefik/NGINX manually).</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#4-increase-docker-memory","title":"\u2705 4. Increase Docker Memory","text":"<p>If your Docker is low on memory (common with 2+ agents), bump it up in Docker Desktop:</p> <p>Settings \u2192 Resources \u2192 Memory \u2192 Increase to at least 4\u20136 GB</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#5-check-logs","title":"\u2705 5. Check Logs","text":"<p>If it still hangs, grab logs from the server node:</p> <pre><code>docker logs k3d-dev-cluster-server-0\n</code></pre> <p>Paste the output here if you'd like me to help debug further.</p> <p>Thanks \u2014 this confirms it clearly:</p> <p>Port 9000 is mapped, but nothing is listening on it inside the cluster. \u2192 That means the Traefik dashboard is not enabled in your k3s setup.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#fix-recreate-cluster-with-dashboard-enabled","title":"\u2705 Fix: Recreate Cluster with Dashboard Enabled","text":"<p>You need to add this flag during cluster creation:</p> <pre><code>--k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre> <p>So update your <code>k3d</code> create command like:</p> <pre><code>k3d cluster delete dev-cluster\n\nk3d cluster create --config dev-cluster.yaml \\\n  --k3s-arg \"--traefik.dashboard=true@server:0\"\n</code></pre> <p>Make sure this is already in your <code>dev-cluster.yaml</code>:</p> <pre><code>- port: 30900:9000\n  nodeFilters:\n    - loadbalancer\n</code></pre>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#optional-also-expose-dashboard-via-ingress-cleaner","title":"\ud83d\udd0d Optional: Also expose dashboard via ingress (cleaner)","text":"<p>You can create an ingress like this (after cluster up):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: traefik-dashboard\n  namespace: kube-system\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: kube-system-auth@kubernetescrd\nspec:\n  rules:\n    - host: traefik.localhost\n      http:\n        paths:\n          - path: /dashboard/\n            pathType: Prefix\n            backend:\n              service:\n                name: traefik\n                port:\n                  number: 9000\n</code></pre> <p>Then access:</p> <pre><code>http://traefik.localhost/dashboard/\n</code></pre> <p>(using a <code>hosts</code> file entry if needed)</p> <p>Let me know once you recreate the cluster with the dashboard flag \u2014 you should be able to access:</p> <pre><code>http://localhost:30900/dashboard/\n</code></pre> <p>To access <code>http://traefik.localhost/dashboard/</code> from your browser, you need to map the custom domain <code>traefik.localhost</code> to your local machine.</p>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#steps-to-add-entry-in-etchosts","title":"\u2705 Steps to Add Entry in <code>/etc/hosts</code>","text":"<ol> <li> <p>Open the <code>/etc/hosts</code> file with root privileges:</p> <pre><code>sudo nano /etc/hosts\n</code></pre> </li> <li> <p>Add this line at the end of the file:</p> <pre><code>127.0.0.1 traefik.localhost\n</code></pre> </li> <li> <p>Save and exit:</p> <ul> <li> <p>Press <code>Ctrl + O</code> to save</p> </li> <li> <p>Press <code>Enter</code> to confirm</p> </li> <li> <p>Press <code>Ctrl + X</code> to exit</p> </li> </ul> </li> </ol>"},{"location":"K3d/k3d%20cluster%20with%20traefik%20dashboard/#after-that","title":"\ud83d\udccc After That","text":"<p>Make sure:</p> <ul> <li> <p>You\u2019ve created an Ingress with <code>host: traefik.localhost</code></p> </li> <li> <p>Traefik is listening on port 80 (mapped via <code>30080:80@loadbalancer</code>)</p> </li> </ul> <p>Then access:</p> <pre><code>http://traefik.localhost:30080/dashboard/\n</code></pre> <p>(or just <code>http://traefik.localhost/dashboard/</code> if port 80 is directly forwarded)</p> <p>Let me know if you want the ingress YAML again.</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/","title":"K3d with traefik dashboard setup","text":"<p>Your config file looks good, but I notice a few things to optimize. Here's an improved version with some corrections and the steps to deploy Traefik with dashboard access:</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#1-updated-k3d-cluster-config-2-create-the-cluster","title":"1. Updated k3d cluster config## 2. Create the cluster","text":"<p>create dev-cluster confi file to setup a cluster by disabling traefik  <pre><code>apiVersion: k3d.io/v1alpha5\nkind: Simple\nmetadata:\n  name: dev-cluster\nservers: 1\nagents: 2\nports:\n  - port: 30907:30907    # Cyclops / Minio\n    nodeFilters:\n      - loadbalancer\n  - port: 31446:31446    # Jenkins\n    nodeFilters:\n      - loadbalancer\n  - port: 31447:31447    # ArgoCD\n    nodeFilters:\n      - loadbalancer\n  - port: 31448:31448    # Gitea\n    nodeFilters:\n      - loadbalancer\n  - port: 30080:80       # Ingress HTTP\n    nodeFilters:\n      - loadbalancer\n  - port: 30443:443      # Ingress HTTPS\n    nodeFilters:\n      - loadbalancer\n  - port: 31000:31000    # Longhorn UI (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30090:30090    # Prometheus (optional)\n    nodeFilters:\n      - loadbalancer\n  - port: 30099:30099    # Grafana (optional) - Fixed port mapping\n    nodeFilters:\n      - loadbalancer\n  - port: 30900:30900    # Traefik Dashboard - Fixed port mapping\n    nodeFilters:\n      - loadbalancer\noptions:\n  k3s:\n    extraArgs:\n      - arg: --disable=traefik\n        nodeFilters:\n          - server:*\n  kubeconfig:\n    updateDefaultKubeconfig: true\n    switchCurrentContext: true\n</code></pre> create a cluster with this config file <pre><code>k3d cluster create --config dev-cluster.yaml\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#3-install-traefik-using-helm","title":"3. Install Traefik using Helm","text":"<p>Since we're disabling the default k3s Traefik, we'll install it manually with custom configuration:</p> <p>create helm traefik values: <pre><code># Traefik Helm Chart Values\nimage:\n  tag: \"3.0\"\n\ndeployment:\n  replicas: 1\n\nservice:\n  type: NodePort\n  # Map standard ports\n  ports:\n    web:\n      port: 80\n      nodePort: 30080\n    websecure:\n      port: 443\n      nodePort: 30443\n    traefik:\n      port: 9000\n      expose: true\n      nodePort: 30900\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: Host(`traefik.localhost`) &amp;&amp; (PathPrefix(`/dashboard`) || PathPrefix(`/api`))\n    entryPoints: [\"traefik\"]\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true  # For development only\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Enable providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n  kubernetesIngress:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--api.insecure=true\"\n  - \"--api.dashboard=true\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n  - \"--metrics.prometheus=true\"\n  - \"--metrics.prometheus.entryPoint=metrics\"\n\n# Prometheus metrics\nmetrics:\n  prometheus:\n    entryPoint: metrics\n\n# Enable persistent storage for certificates (optional)\npersistence:\n  enabled: false\n</code></pre></p> <pre><code># Add Traefik Helm repository\nhelm repo add traefik https://traefik.github.io/charts\nhelm repo update\n\n# Create namespace\nkubectl create namespace traefik-system\n\n# Create Traefik values file\n## 4. Install Traefik\n\nhelm install traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values.yaml \\\n  --wait\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#5-verify-traefik-installation","title":"5. Verify Traefik installation","text":"<pre><code># Check pods\nkubectl get pods -n traefik-system\n\n# Check services\nkubectl get svc -n traefik-system\n\n# Check if dashboard is accessible\ncurl http://localhost:30900/dashboard/\n# checked not accessible, getting below error\ngouse@gouse:~$ curl http://localhost:30900/dashboard/\ncurl: (52) Empty reply from server\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#6-create-an-ingressroute-for-easier-dashboard-access-optionalapply-the-ingressroute","title":"6. Create an IngressRoute for easier dashboard access (optional)Apply the IngressRoute:","text":"<p>create traefik dashboard ingress route traefik-dashboard-ingress.yaml <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik-system\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.dev.local`)\n      kind: Rule\n      services:\n        - name: api@internal\n          kind: TraefikService\n---\n# Optional: Middleware for basic auth\napiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\ntype: kubernetes.io/basic-auth\nstringData:\n  username: admin\n  password: changeme\n---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik-system\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n</code></pre></p> <pre><code>kubectl apply -f traefik-dashboard-ingress.yaml\n\ngouse@gouse:~/DevOps/k3d$ kubectl apply -f traefik-dashboard-ingress.yaml\nWarning: resource ingressroutes/traefik-dashboard is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ningressroute.traefik.io/traefik-dashboard configured\nsecret/traefik-dashboard-auth created\nmiddleware.traefik.io/traefik-dashboard-auth created\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#7-access-the-traefik-dashboard","title":"7. Access the Traefik Dashboard","text":"<p>You can now access the Traefik dashboard in multiple ways:</p> <ol> <li>Direct NodePort access: http://localhost:30900/dashboard/ <pre><code>in browser, it tried but getting as below\n\n# The connection was reset\nThe connection to the server was reset while the page was loading.\n</code></pre></li> <li>Via Ingress (if you added the host to /etc/hosts): http://traefik.dev.local</li> </ol>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#8-deploy-a-sample-application-to-test-traefikdeploy-the-sample-app","title":"8. Deploy a sample application to test TraefikDeploy the sample app:","text":"<p>create a sample app: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  selector:\n    app: whoami\n  ports:\n    - port: 80\n      targetPort: 80\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami\n  namespace: default\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`whoami.dev.local`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre></p> <pre><code>kubectl apply -f sample-app.yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#important-notes","title":"Important Notes:","text":"<ol> <li> <p>Port mappings: I fixed the port mappings for Grafana and Traefik dashboard. They should map to the same ports (30099:30099 and 30900:30900).</p> </li> <li> <p>Traefik installation: Since k3s comes with Traefik by default, I added the <code>--disable=traefik</code> flag to install our own version with custom configuration.</p> </li> <li> <p>Dashboard access: The dashboard will be available at http://localhost:30900/dashboard/ (note the trailing slash).</p> </li> <li> <p>Security: For production, always secure the dashboard with authentication and use HTTPS.</p> </li> <li> <p>Host entries: Add these to your <code>/etc/hosts</code> file for easier access:</p> <pre><code>127.0.0.1 traefik.dev.local\n127.0.0.1 whoami.dev.local\n</code></pre> </li> </ol> <p>This setup gives you full control over Traefik configuration and makes it easy to add more services with their respective ingress routes.</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#post-above-setup-my-observations-on-not-working","title":"post above setup, my observations on not working","text":"<pre><code>in my k3d conifg file i routed with below details for traefik - port: 30900:30900 # Traefik Dashboard - Fixed port mapping nodeFilters: - loadbalancer\n\nbut created service with different port, i think this is the issue which not able to access the curl [http://localhost:30900/dashboard/](http://localhost:30900/dashboard/) curl: (52) Empty reply from server\n\n$ kubectl get svc -n traefik-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik NodePort 10.43.48.148 &lt;none&gt; 80:32560/TCP,443:31788/TCP 22m\n\nand also i added values in /etc/hosts 127.0.0.1 traefik.dev.local 127.0.0.1 whoami.dev.local\n\nand i think it should be loadbalancer instead of nodeport for traefik-system svc what could be the issue, that i have not able to access\n</code></pre> <p>You're absolutely right! The issue is that the Traefik service isn't exposing port 9000 (the dashboard port) and the NodePort mappings don't match your k3d configuration. Let's fix this:</p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#1-first-lets-check-the-current-traefik-service-configuration","title":"1. First, let's check the current Traefik service configuration:","text":"<pre><code>kubectl get svc traefik -n traefik-system -o yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#2-update-the-traefik-values-file-to-properly-expose-the-dashboard-port-3-upgrade-traefik-with-the-corrected-configuration","title":"2. Update the Traefik values file to properly expose the dashboard port:## 3. Upgrade Traefik with the corrected configuration:","text":"<p>update traefik-values-fixed.yaml <pre><code># Traefik Helm Chart Values - Fixed Schema\nimage:\n  tag: \"3.0\"\n\ndeployment:\n  replicas: 1\n\nservice:\n  type: NodePort\n  spec: {}\n\nports:\n  web:\n    port: 8000\n    expose:\n      default: true\n    exposedPort: 80\n    nodePort: 30080\n    protocol: TCP\n  websecure:\n    port: 8443\n    expose:\n      default: true\n    exposedPort: 443\n    nodePort: 30443\n    protocol: TCP\n  traefik:\n    port: 9000\n    expose:\n      default: true\n    exposedPort: 9000\n    nodePort: 30900\n    protocol: TCP\n\n# Enable dashboard\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: PathPrefix(`/dashboard`) || PathPrefix(`/api`)\n    entryPoints: [\"traefik\"]\n\n# API and Dashboard settings\napi:\n  dashboard: true\n  debug: true\n  insecure: true  # For development only\n\n# Logs\nlogs:\n  general:\n    level: INFO\n  access:\n    enabled: true\n\n# Enable providers\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n  kubernetesIngress:\n    enabled: true\n\n# Global arguments\nglobalArguments:\n  - \"--api.insecure=true\"\n  - \"--api.dashboard=true\"\n\n# Additional arguments\nadditionalArguments:\n  - \"--log.level=INFO\"\n  - \"--accesslog=true\"\n\n# Enable persistent storage for certificates (optional)\npersistence:\n  enabled: false\n</code></pre></p> <pre><code>helm upgrade traefik traefik/traefik \\\n  --namespace traefik-system \\\n  --values traefik-values-fixed.yaml \\\n  --wait\n</code></pre> <pre><code>$ helm upgrade traefik traefik/traefik   --namespace traefik-system   --values traefik-values-fixed.yaml   --wait\nRelease \"traefik\" has been upgraded. Happy Helming!\nNAME: traefik\nLAST DEPLOYED: Sat Jun  7 11:15:27 2025\nNAMESPACE: traefik-system\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\ntraefik with docker.io/traefik:3.0 has been deployed successfully on traefik-system namespace !\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#4-if-the-above-doesnt-work-k3d-sometimes-has-issues-with-loadbalancer-type-lets-patch-the-service-directlyapply-the-patch","title":"4. If the above doesn't work (k3d sometimes has issues with LoadBalancer type), let's patch the service directly:Apply the patch:","text":"<pre><code>kubectl apply -f traefik-service-patch.yaml\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#5-alternative-create-a-separate-service-for-the-dashboardbash","title":"5. Alternative: Create a separate service for the dashboard:```bash","text":"<p>kubectl apply -f traefik-dashboard-service.yaml</p> <pre><code>## 6. Verify the service has the correct ports:\n\n```bash\n# Check services\nkubectl get svc -n traefik-system\n\n# You should see something like:\n# NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                     AGE\n# traefik             NodePort   10.43.48.148    &lt;none&gt;        80:30080/TCP,443:30443/TCP,9000:30900/TCP  22m\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#7-check-if-traefik-pod-is-exposing-port-9000","title":"7. Check if Traefik pod is exposing port 9000:","text":"<pre><code># Get pod name\nkubectl get pods -n traefik-system\n\n# Check if port 9000 is exposed\nkubectl describe pod -n traefik-system &lt;traefik-pod-name&gt; | grep -A 5 \"Ports:\"\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#8-test-access-again","title":"8. Test access again:","text":"<pre><code># Direct dashboard access\ncurl http://localhost:30900/dashboard/\n\n# Or check the API endpoint\ncurl http://localhost:30900/api/rawdata\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#9-if-still-having-issues-check-traefik-logs","title":"9. If still having issues, check Traefik logs:","text":"<pre><code>kubectl logs -n traefik-system -l app.kubernetes.io/name=traefik -f\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#quick-debug-commands","title":"Quick Debug Commands:","text":"<pre><code># Check if the port is actually listening\nkubectl exec -n traefik-system deployment/traefik -- netstat -tlnp | grep 9000\n\n# Port forward directly to test\nkubectl port-forward -n traefik-system deployment/traefik 9000:9000\n# Then try: curl http://localhost:9000/dashboard/\n</code></pre> <p>The main issue was that the Traefik service wasn't configured to expose port 9000 with the correct NodePort (30900) that matches your k3d configuration. The above solutions should fix this issue.</p> <p>Still not working: <pre><code>https://traefik.dev.local:30900/dashboard/` also getting 404 error\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#check-traefik-deployment-arguments","title":"Check Traefik deployment arguments:","text":"<pre><code>kubectl get deployment traefik -n traefik-system -o yaml | grep -A 30 \"args:\"\n</code></pre>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#if-dashboard-is-not-enabled-lets-force-enable-it","title":"If dashboard is not enabled, let's force enable it:","text":"<p>create a script and check <pre><code>#!/bin/bash\n\n# Patch Traefik deployment to ensure dashboard is enabled\nkubectl patch deployment traefik -n traefik-system --type='json' -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.dashboard=true\"\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.insecure=true\"\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/args/-\",\n    \"value\": \"--api.debug=true\"\n  }\n]'\n\n# Wait for rollout\nkubectl rollout status deployment/traefik -n traefik-system\n\n# Check the service has the traefik port\nkubectl get svc traefik -n traefik-system -o jsonpath='{.spec.ports[?(@.name==\"traefik\")].nodePort}'\n</code></pre></p>"},{"location":"K3d/k3d%20with%20traefik%20dashboard%20setup/#summary","title":"Summary:","text":"<p>DO NOT USE: <code>https://traefik.dev.local:30900/dashboard/</code></p> <p>USE ONE OF THESE:</p> <ul> <li><code>http://localhost:30900/dashboard/</code> (direct dashboard access)</li> <li><code>http://traefik.dev.local:30080/</code> (through ingress HTTP)</li> <li><code>https://traefik.dev.local:30443/</code> (through ingress HTTPS)</li> </ul> <p>The key is understanding that:</p> <ul> <li>Port 30900 = Direct dashboard port (use with localhost)</li> <li>Port 30080/30443 = Ingress ports (use with domain names)</li> </ul>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/","title":"Efficient way to to Set Up Keycloak (with Helm)","text":"<p>Here's the most efficient way to set up Keycloak on a Kubernetes/OpenShift cluster using the official Helm chart.</p>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/#steps-to-set-up-keycloak-with-helm","title":"\u2705 Steps to Set Up Keycloak (with Helm)","text":"<ol> <li> <p>Add Bitnami repo</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> </li> <li> <p>Create namespace</p> <pre><code>kubectl create namespace keycloak\n</code></pre> </li> <li> <p>Install Keycloak Yes \u2014 since you're using k3d with the default Traefik ingress controller, you should expose Keycloak via an Ingress.</p> </li> </ol>"},{"location":"Keycloak/Efficient%20way%20to%20to%20Set%20Up%20Keycloak%20%28with%20Helm%29/#steps-to-create-ingress-for-keycloak-on-k3d-with-traefik","title":"\u2705 Steps to Create Ingress for Keycloak on k3d with Traefik","text":"<ol> <li>Install Keycloak with hostname  ```bash     helm install keycloak bitnami/keycloak -n keycloak \\       --set auth.adminUser=admin \\       --set auth.adminPassword=adminpassword \\       --set service.type=ClusterIP \\       --set ingress.enabled=true \\       --set ingress.hostname=keycloak.local \\       --set ingress.annotations.\"traefik.ingress.kubernetes.io/router.entrypoints\"=web</li> </ol> <p>helm install keycloak bitnami/keycloak -n keycloak \\       --set auth.adminUser=admin \\       --set auth.adminPassword=admin123 \\       --set service.type=ClusterIP \\       --set ingress.enabled=true \\       --set ingress.hostname=keycloak.local \\       --set ingress.annotations.\"traefik.ingress.kubernetes.io/router.entrypoints\"=web <pre><code>```bash\ngouse@gouse:~/DevOps/k3d$ helm install keycloak bitnami/keycloak -n keycloak \\\n      --set auth.adminUser=admin \\\n      --set auth.adminPassword=admin123 \\\n      --set service.type=ClusterIP \\\n      --set ingress.enabled=true \\\n      --set ingress.hostname=keycloak.local \\\n      --set ingress.annotations.\"traefik\\.ingress\\.kubernetes\\.io/router\\.entrypoints\"=web\nNAME: keycloak\nLAST DEPLOYED: Mon Jun  9 18:31:36 2025\nNAMESPACE: keycloak\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: keycloak\nCHART VERSION: 24.7.3\nAPP VERSION: 26.2.5\n\nDid you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.\n\n** Please be patient while the chart is being deployed **\n\nKeycloak can be accessed through the following DNS name from within your cluster:\n\n    keycloak.keycloak.svc.cluster.local (port 80)\n\nTo access Keycloak from outside the cluster execute the following commands:\n\n1. Get the Keycloak URL and associate its hostname to your cluster external IP:\n\n   export CLUSTER_IP=$(minikube ip) # On Minikube. Use: `kubectl cluster-info` on others K8s clusters\n   echo \"Keycloak URL: http://keycloak.local/\"\n   echo \"$CLUSTER_IP  keycloak.local\" | sudo tee -a /etc/hosts\n\n2. Access Keycloak using the obtained URL.\n3. Access the Administration Console using the following credentials:\n\n  echo Username: admin\n  echo Password: $(kubectl get secret --namespace keycloak keycloak -o jsonpath=\"{.data.admin-password}\" | base64 -d)\n\nWARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n  - resources\n+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n</code></pre></p> <ol> <li> <p>Add host entry (on your machine)</p> <pre><code>echo \"127.0.0.1 keycloak.local\" | sudo tee -a /etc/hosts\nget the ip from ingress deployed for keycloak\necho \"172.18.0.2 keycloak.local\" | sudo tee -a /etc/hosts\n</code></pre> </li> <li> <p>Check ingress is created</p> <pre><code>kubectl get ingress -n keycloak\n</code></pre> </li> <li> <p>Access Keycloak</p> <ul> <li>Open browser: http://keycloak.local</li> </ul> </li> </ol> <p>Let me know if you need HTTPS (TLS) setup with Traefik too.</p>"},{"location":"Keywords/Must%20know%20keywords/","title":"Must know keywords","text":"<p>==Ephemeral== means temporary \u2014 it exists only while the pod is running and gets deleted when the pod is deleted or restarted.</p> Term Definition Ephemeral Temporary \u2014 data or resource that disappears after session/pod is gone Immutable Cannot be changed after creation (e.g., Docker images, versioned infra) Idempotent Can be repeated with same result, no side effects (used in scripts, APIs) Stateless Doesn\u2019t store data between requests (all info comes from request itself) Stateful Stores data or state across sessions (e.g., databases, login sessions) Orchestration Automated management of multiple containers/apps (e.g., Kubernetes) Provisioning Setting up infrastructure automatically (servers, storage, etc.) Taint Marking a node to repel certain pods (Kubernetes term) Toleration Allowing a pod to be scheduled on a tainted node Throttling Limiting usage of CPU/memory or API rate Drain Gracefully removing a node (evicts all pods) in Kubernetes Scaling Adding/removing resources (pods, instances) based on load Bootstrap Initial setup process (e.g., first-time server config or cluster init) Latency Time delay between request and response Throughput Amount of data processed in a given time Saturation A resource being fully utilized (CPU, disk, etc.) Cold Start First-time boot/load of an app/container; slower due to init time Warm Start Restart where some resources/state are already cached A/B Testing Testing two versions of an app to compare performance Canary Deploy Releasing new version to a small group before full rollout Image Read-only template used to create containers (like a snapshot of an app) Container Running instance of an image Registry Storage/repository for container images (Docker Hub, Quay, GitHub Container Registry) Dockerfile Text file with instructions to build a container image Layer Each step in a Dockerfile creates a filesystem layer Volume Persistent storage that can be attached to containers Bind Mount Maps a directory from host into a container (live sync) Overlay Network Docker\u2019s network allowing containers to communicate across hosts Namespace Linux kernel feature isolating container processes (PID, network, mount) Cgroup Linux kernel feature limiting resource usage (CPU, memory) for containers Entrypoint Command that runs when container starts CMD Default command/arguments for container; overridden by runtime command Daemon Background service managing containers (Docker daemon or Podman service) Pod Group of one or more containers sharing the same network/volume (Podman/K8s) Image Tag Version label of an image (e.g., <code>nginx:1.21</code>) Docker Compose Tool to define and run multi-container apps with YAML config Container Runtime Software that runs containers (Docker, containerd, Podman, CRI-O) Build Context Files and directories sent to daemon to build the image Entrypoint vs CMD Entrypoint is fixed command, CMD is default args, CMD can override Entrypoint Container Orchestration Managing multiple containers (e.g., Kubernetes, Docker Swarm) Sidecar Container Helper container running alongside main container (logging, proxy) Rootless Mode Running containers without root privileges (safer) Image Pull Downloading an image from a registry Image Push Uploading a built image to a registry Container Snapshot Saving the current state of a container as an image Dangling Images Unused images without tags, can be cleaned to free space Garbage Collection Removing unused containers, images, volumes"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/","title":"Loadbalancer Types","text":"<p>In Kubernetes, there are three main types of load balancers commonly used:</p> Type Description Internal LoadBalancer Used for internal services, only accessible within the cluster or VPC. External LoadBalancer Exposes services to the internet using a cloud provider\u2019s external LB. Ingress Controller Acts as a reverse proxy and manages HTTP/S traffic with routing rules. LB Type Layer Performance Use Case ELB L4+L7 Moderate Legacy/simple apps NLB L4 High Real-time, fast apps like APIs ALB L7 HTTP-aware Use with Ingress, not <code>Service</code> Additionally, you might use: Type Description NodePort Exposes service on each node\u2019s IP at a static port (not a full LB, but basic). MetalLB A load balancer implementation for bare metal clusters."},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#when-and-how-to-use-internal-vs-external-loadbalancers","title":"When and How to Use Internal vs External LoadBalancers","text":"Type When to Use How to Use Internal LoadBalancer For intra-cluster or private communication between services (e.g., backend APIs, internal DBs) Use annotation to make it internal:<code>service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"</code> External LoadBalancer When exposing a service to the public internet (e.g., front-end web app, public API) Just define the service type as <code>LoadBalancer</code>, Kubernetes/cloud will handle provisioning."},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#example-yamls","title":"Example YAMLs:","text":"<p>Internal LoadBalancer (AWS/Azure/GCP-specific annotations): <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre></p> <p>External LoadBalancer: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre></p> <p>You're almost spot on \u2014 let me break it down clearly:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#correct-understanding","title":"\u2705 Correct Understanding:","text":"<ul> <li> <p>LoadBalancers work with Services</p> </li> <li> <p>Services route traffic to Pods</p> </li> <li> <p>Pods are created by Deployments (or StatefulSets, etc.)</p> </li> <li> <p>So, indirectly, LoadBalancers affect traffic to Deployments via Services.</p> </li> </ul>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#but-not-directly-tied-to-deployment-strategies","title":"\u274c But NOT directly tied to Deployment Strategies","text":"<ul> <li> <p>Deployment strategies like <code>RollingUpdate</code> or <code>Recreate</code> are just about how pods are updated</p> </li> <li> <p>LoadBalancers don\u2019t care how or when the pods are created/updated \u2014 they just route to available, healthy pods via the Service.</p> </li> </ul>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#flow-summary","title":"Flow Summary:","text":"<p><code>LoadBalancer \u2192 Service \u2192 Pods \u2190 Deployment (with strategy)</code></p> <ul> <li> <p>LoadBalancer: handles external/internal traffic</p> </li> <li> <p>Service: handles routing logic</p> </li> <li> <p>Deployment: manages pod lifecycle</p> </li> <li> <p>Strategy: manages how pods get updated, not how traffic is routed</p> </li> </ul> <p>So yes, they all work together, but the LoadBalancer doesn\u2019t depend on deployment strategy.</p> <p>In your k3d cluster using Traefik, you're using an Ingress Controller, which is a type of Layer 7 (HTTP) Load Balancer.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#details","title":"Details:","text":"Component Role Traefik Acts as an Ingress Controller (application-level load balancer) Ingress Defines routing rules (host/path based) to backend services Service Maps traffic from Traefik to Pods Deployment Manages Pods running your apps"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#so-the-type-of-load-balancer-youre-using-is","title":"So, the type of Load Balancer you're using is:","text":"<p>\u2705 Ingress-based Load Balancer (via Traefik)</p> <p>It\u2019s not a Service of type <code>LoadBalancer</code> (which works on cloud providers), but rather an internal reverse proxy that routes external traffic based on Ingress rules.</p> <p>Here's a full list of commonly used annotations for different types of LoadBalancers across major cloud providers and Traefik:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#1-internal-loadbalancer-private","title":"\ud83d\udd39 1. Internal LoadBalancer (Private)","text":"Cloud Annotation AWS <code>service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"</code> Azure <code>service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"</code> GCP <code>cloud.google.com/load-balancer-type: \"Internal\"</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#2-external-loadbalancer-public","title":"\ud83d\udd39 2. External LoadBalancer (Public)","text":"<p>(These are usually default; no annotation needed, but here\u2019s how to customize)</p> Cloud Annotation Example AWS <code>service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"</code> (or \"elb\") Azure <code>service.beta.kubernetes.io/azure-load-balancer-internal: \"false\"</code> GCP <code>cloud.google.com/load-balancer-type: \"External\"</code> (default)"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#3-ingress-based-loadbalancer-eg-traefik","title":"\ud83d\udd39 3. Ingress-based LoadBalancer (e.g., Traefik)","text":"Use Case Annotation Traefik IngressClass <code>kubernetes.io/ingress.class: \"traefik\"</code> TLS redirection <code>traefik.ingress.kubernetes.io/redirect-entry-point: https</code> Rewrite Path <code>traefik.ingress.kubernetes.io/rewrite-target: /</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#4-metallb-for-bare-metal-local","title":"\ud83d\udd39 4. MetalLB (for bare metal / local)","text":"Use Case Annotation Assign static IP <code>metallb.universe.tf/address-pool: \"default\"</code> <p>Great \u2014 here are ready-to-use YAML examples with annotations for each type of LoadBalancer based on your setup:</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#1-aws","title":"\ud83d\udd38 1. AWS","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#external-loadbalancer-public","title":"\u2705 External LoadBalancer (Public)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-public-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer-private","title":"\u2705 Internal LoadBalancer (Private)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#azure","title":"\ud83d\udd38 Azure","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer","title":"\u2705 Internal LoadBalancer","text":"<pre><code>metadata:\n  name: my-internal-azure-svc\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#gcp","title":"\ud83d\udd38 GCP","text":""},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#internal-loadbalancer_1","title":"\u2705 Internal LoadBalancer","text":"<pre><code>metadata:\n  name: my-internal-gcp-svc\n  annotations:\n    cloud.google.com/load-balancer-type: \"Internal\"\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#metallb-localbare-metal","title":"\ud83d\udd38 MetalLB (Local/Bare Metal)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-metallb-service\n  annotations:\n    metallb.universe.tf/address-pool: \"default\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#traefik-ingress-for-your-k3d-setup","title":"\ud83d\udd38 Traefik Ingress (for your k3d setup)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-traefik-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"traefik\"\n    traefik.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: myapp.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-service\n                port:\n                  number: 80\n</code></pre>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#what-is-nlb-in-aws-annotation","title":"\ud83d\udd39 What is <code>nlb</code> in AWS Annotation?","text":"<p><code>nlb</code> stands for Network Load Balancer in AWS. It\u2019s a high-performance, Layer 4 (TCP/UDP) load balancer that handles millions of requests per second with low latency.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#aws-annotation-meaning","title":"\ud83d\udd39 AWS Annotation Meaning:","text":"<pre><code>service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n</code></pre> <p>This tells Kubernetes to create an AWS Network Load Balancer (NLB) instead of the default Classic Load Balancer (ELB).</p> LB Type Layer Performance Use Case ELB L4+L7 Moderate Legacy/simple apps NLB L4 High Real-time, fast apps like APIs ALB L7 HTTP-aware Use with Ingress, not <code>Service</code>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#are-these-services-predefined","title":"\ud83d\udd39 Are These Services Predefined?","text":"<p>No \u2014 these are not predefined. When you create a <code>Service</code> of type <code>LoadBalancer</code>, Kubernetes asks the cloud provider (via the Cloud Controller Manager) to provision a real load balancer resource dynamically.</p>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#creation-process-simplified","title":"\ud83d\udd39 Creation Process (Simplified):","text":"<ol> <li> <p>You define a <code>Service</code> with <code>type: LoadBalancer</code></p> </li> <li> <p>Kubernetes talks to the cloud provider API</p> </li> <li> <p>The cloud creates:</p> <ul> <li> <p>A load balancer (ELB, NLB, etc.)</p> </li> <li> <p>Configures listeners and target groups</p> </li> <li> <p>Maps it to the appropriate nodes/pods</p> </li> </ul> </li> <li> <p>The LB's public or private IP is reflected in the service status (<code>EXTERNAL-IP</code>)</p> </li> </ol>"},{"location":"Loadbalancers%20Types/Loadbalancer%20Types/#so-you-control","title":"\ud83d\udd39 So, You Control:","text":"<ul> <li> <p>Type (<code>LoadBalancer</code>, <code>ClusterIP</code>, <code>NodePort</code>)</p> </li> <li> <p>Provider behavior via annotations</p> </li> <li> <p>Target pods via selectors</p> </li> <li> <p>Protocols via ports</p> </li> </ul> <p>Let me know if you want a visual flow or deeper dive into a specific cloud (e.g., NLB with TLS, ALB via Ingress, etc.).</p>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/","title":"Reverse proxy means","text":"<p>A reverse proxy is a server that sits in front of one or more backend servers and forwards client requests to those servers. It hides the backend servers from clients and can provide:</p> <ul> <li>Load balancing</li> <li>SSL termination</li> <li>Caching</li> <li>Compression</li> <li>Security filtering</li> </ul>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/#simple-flow","title":"Simple flow:","text":"<pre><code>Client \u2192 Reverse Proxy \u2192 Backend Server\n</code></pre>"},{"location":"Loadbalancers%20Types/Reverse%20proxy%20means/#example","title":"Example:","text":"<p>When you access a website:</p> <pre><code>User \u2192 Nginx (Reverse Proxy) \u2192 Web App Server\n</code></pre> <p>The client doesn\u2019t see or talk directly to the backend server\u2014only to the reverse proxy.</p>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/","title":"Common Kubernetes storage types","text":"<ul> <li>Storage providers</li> <li>Container Storage Interface (CSI)</li> <li>The Kubernetes persistent volume subsystem</li> <li>Dynamic provisioning with Storage Classes </li> </ul>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#kubernetes-storage-types-comparison","title":"\ud83d\udce6 Kubernetes Storage Types Comparison","text":"Type Access Mode Description Example Use Case Example emptyDir Pod-level only Ephemeral storage shared by containers in the same pod. Caching/temp files during pod lifetime <code>emptyDir: {}</code> in pod spec hostPath Node-level only Mounts a file/dir from host node into the pod. Accessing logs or shared folders on node <code>hostPath: /data/logs</code> NFS ReadWriteMany (RWX) Network File System \u2014 shared, file-based storage. Shared configs, data between multiple pods <code>nfs: { server: 10.0.0.1, path: \"/data\" }</code> Longhorn RWO, ROX, RWX Distributed block storage with replication, snapshot, backup. Stateful apps needing HA volumes <code>storageClassName: longhorn</code> in PVC CSI (Cloud) Varies Cloud-native block/file storage via Container Storage Interface. Cloud apps needing persistent volumes <code>storageClassName: gp2</code> (AWS), <code>premium</code> (Azure) Local PV Node-local Directly uses disks on the Kubernetes node. Performance-heavy workloads <code>local: { path: /mnt/disks/ssd1 }</code> GlusterFS RWX Distributed, scalable file storage system. Shared volumes across pods/nodes <code>glusterfs: { endpoints: ..., path: /data }</code> Ceph RBD/FS RWO, RWX Ceph RBD (block), CephFS (file) storage. Scalable &amp; reliable for prod workloads. DBs or shared file workloads <code>rbd: { monitors: [...], pool: ..., image: ... }</code> iSCSI RWO Raw block storage via iSCSI protocol. Legacy storage integration <code>iscsi: { targetPortal: ..., iqn: ..., lun: 0 }</code> Here\u2019s a clear comparison of Block, Network (File/Shared), Local, and CephFS storage types: Storage Type Description Use Case Access Type Examples Block Storage Raw storage volumes presented as disks to OS or containers. Databases, VMs, apps needing fast, low-latency storage ReadWriteOnce (usually) Longhorn, AWS EBS, iSCSI Network File Storage (NFS, GlusterFS) Shared storage accessed over network as file system. Shared configs, logs, or data across pods ReadWriteMany NFS, GlusterFS, CephFS Local Storage Storage physically attached to node (host). Fast access, node-specific data ReadWriteOnce hostPath, local PV CephFS Distributed POSIX-compliant file system on Ceph cluster. Shared storage with high availability ReadWriteMany CephFS ### Key Differences: <ul> <li> <p>Block Storage acts like a virtual disk, fast and low-level, but typically mounted by one pod at a time.</p> </li> <li> <p>Network File Storage allows multiple pods to read/write simultaneously over the network.</p> </li> <li> <p>Local Storage is node-specific, fast but tied to that node only.</p> </li> <li> <p>CephFS combines distributed file system features with scalability and high availability.</p> </li> </ul>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#pvc-pv-manifest-examples","title":"PVC + PV manifest examples","text":""},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#1-emptydir-ephemeral","title":"\ud83e\uddea 1. <code>emptyDir</code> (Ephemeral)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: emptydir-example\nspec:\n  containers:\n    - name: app\n      image: nginx\n      volumeMounts:\n        - name: cache\n          mountPath: /cache\n  volumes:\n    - name: cache\n      emptyDir: {}\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#2-hostpath","title":"\ud83c\udfe0 2. <code>hostPath</code>","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-example\nspec:\n  containers:\n    - name: app\n      image: nginx\n      volumeMounts:\n        - name: host-vol\n          mountPath: /data\n  volumes:\n    - name: host-vol\n      hostPath:\n        path: /mnt/data\n        type: Directory\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#3-nfs","title":"\ud83c\udf10 3. <code>NFS</code>","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: 10.0.0.100\n    path: /exports/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#4-longhorn","title":"\ud83d\udc02 4. <code>Longhorn</code>","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: longhorn-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 2Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#5-cloud-csi-aws-ebs-example","title":"\u2601\ufe0f 5. Cloud CSI (AWS EBS example)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: aws-ebs-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: gp2\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#6-local-pv","title":"\ud83d\udccd 6. Local PV","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: local-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  local:\n    path: /mnt/disks/ssd1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: kubernetes.io/hostname\n              operator: In\n              values:\n                - node-name\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: local-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#7-ceph-rbd-block-storage","title":"\ud83e\uddf1 7. Ceph RBD (Block Storage)","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ceph-secret\ntype: kubernetes.io/rbd\ndata:\n  key: &lt;base64-encoded-ceph-key&gt;\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ceph-rbd-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  rbd:\n    monitors:\n      - 10.0.0.1:6789\n    pool: kube\n    image: rbd-vol\n    user: admin\n    secretRef:\n      name: ceph-secret\n    fsType: ext4\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ceph-rbd-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#8-cephfs-file-storage","title":"\ud83d\udcc2 8. CephFS (File Storage)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: cephfs-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  cephfs:\n    monitors:\n      - 10.0.0.1:6789\n    path: /\n    user: admin\n    secretRef:\n      name: ceph-secret\n    readOnly: false\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#9-glusterfs","title":"\ud83d\udce1 9. GlusterFS","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: gluster-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  glusterfs:\n    endpoints: gluster-cluster\n    path: myvol\n    readOnly: false\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: gluster-cluster\nsubsets:\n  - addresses:\n      - ip: 10.0.0.2\n    ports:\n      - port: 1\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gluster-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Common%20Kubernetes%20storage%20types/#10-iscsi","title":"\ud83d\udcbf 10. iSCSI","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: iscsi-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  iscsi:\n    targetPortal: 10.0.0.10:3260\n    iqn: iqn.2023-04.com.example:storage.disk1\n    lun: 0\n    fsType: ext4\n    readOnly: false\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: iscsi-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/","title":"Hands-On Demo: Deploying Longhorn on Kubernetes","text":"<p>In this demo, we\u2019ll: \u2705 Install Longhorn on a Kubernetes cluster \u2705 Create a Persistent Volume Claim (PVC) \u2705 Deploy a MySQL database using Longhorn storage \u2705 Test fault tolerance by killing a pod \u2705 Take a snapshot &amp; restore </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, K3s, EKS, AKS, etc.)  </li> <li><code>kubectl</code> configured  </li> <li>Helm (for installation)  </li> </ul>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-1-install-longhorn","title":"Step 1: Install Longhorn","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#option-a-using-helm-recommended","title":"Option A: Using Helm (Recommended)","text":"<pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\nhelm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#option-b-using-kubectl","title":"Option B: Using Kubectl","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/deploy/longhorn.yaml\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#verify-installation","title":"Verify Installation","text":"<p><pre><code>kubectl get pods -n longhorn-system\n</code></pre> \u2705 Expected Output: All pods should be <code>Running</code>.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-2-access-longhorn-dashboard","title":"Step 2: Access Longhorn Dashboard","text":"<p>Port-forward the UI: <pre><code>kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80\n</code></pre> Now, open http://localhost:8080 in your browser.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-3-create-a-persistentvolumeclaim-pvc","title":"Step 3: Create a PersistentVolumeClaim (PVC)","text":"<p>Create <code>mysql-pvc.yaml</code>: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> Apply it: <pre><code>kubectl apply -f mysql-pvc.yaml\n</code></pre> Check in Longhorn UI \u2192 Volume tab.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-4-deploy-mysql-with-longhorn-storage","title":"Step 4: Deploy MySQL with Longhorn Storage","text":"<p>Create <code>mysql-deployment.yaml</code>: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        ports:\n        - containerPort: 3306\n        volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n</code></pre> Deploy it: <pre><code>kubectl apply -f mysql-deployment.yaml\n</code></pre> Verify: <pre><code>kubectl get pods\nkubectl logs &lt;mysql-pod-name&gt;\n</code></pre></p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-5-test-fault-tolerance","title":"Step 5: Test Fault Tolerance","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#simulate-a-nodepod-failure","title":"Simulate a Node/Pod Failure","text":"<p><pre><code>kubectl delete pod &lt;mysql-pod-name&gt;\n</code></pre> \u2705 Observe: - A new pod starts automatically. - Data persists because Longhorn ensures replicated storage.  </p>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-6-take-a-snapshot-restore","title":"Step 6: Take a Snapshot &amp; Restore","text":""},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#manual-snapshot-via-longhorn-ui","title":"Manual Snapshot via Longhorn UI","text":"<ol> <li>Go to Volumes \u2192 Select your MySQL volume.  </li> <li>Click \"Take Snapshot\".  </li> <li>(Optional) Backup to S3 (if configured).  </li> </ol>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#restore-from-snapshot","title":"Restore from Snapshot","text":"<ol> <li>In the UI, select the snapshot \u2192 \"Create Standby Volume\".  </li> <li>Update the MySQL deployment to use the new volume.  </li> </ol>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#step-7-clean-up","title":"Step 7: Clean Up","text":"<pre><code>kubectl delete deployment mysql\nkubectl delete pvc mysql-pvc\nhelm uninstall longhorn -n longhorn-system\n</code></pre>"},{"location":"Longhorn/Hands-On%20-%20Deploying%20Longhorn%20on%20Kubernetes/#conclusion","title":"Conclusion","text":"<p>You\u2019ve successfully: \u2714 Deployed Longhorn on Kubernetes \u2714 Created a MySQL database with persistent storage \u2714 Tested fault tolerance \u2714 Used snapshots &amp; backups </p> <p>\ud83d\ude80 Next Steps: - Try cross-cluster replication for disaster recovery. - Integrate with Rancher for easier management.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/","title":"Longhorn Access Modes & Volume Resizing Guide","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#1-understanding-access-modes-in-longhorn","title":"1. Understanding Access Modes in Longhorn","text":"<p>Kubernetes Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) support different access modes, which define how volumes can be mounted by pods.</p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#supported-access-modes-in-longhorn","title":"Supported Access Modes in Longhorn","text":"Access Mode Description Use Case <code>ReadWriteOnce</code> (RWO) Only one pod (on a single node) can mount the volume in read-write mode. Single-instance databases (MySQL, PostgreSQL) <code>ReadOnlyMany</code> (ROX) Multiple pods (across different nodes) can mount the volume in read-only mode. Shared configuration files (e.g., <code>nginx.conf</code>) <code>ReadWriteMany</code> (RWX) Multiple pods (across different nodes) can mount the volume in read-write mode. Shared file storage (NFS-like, e.g., WordPress uploads)"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#which-access-modes-does-longhorn-support","title":"Which Access Modes Does Longhorn Support?","text":"<p>\u2705 Longhorn supports: - <code>ReadWriteOnce</code> (RWO) \u2013 Default and most stable. - <code>ReadWriteMany</code> (RWX) \u2013 Requires NFSv4 or Samba (experimental).  </p> <p>\u274c Longhorn does NOT natively support <code>ReadOnlyMany</code> (ROX). </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#2-how-to-change-access-modes-in-longhorn","title":"2. How to Change Access Modes in Longhorn","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-a-define-in-pvc-yaml","title":"Option A: Define in PVC YAML","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce  # Change to ReadWriteMany for RWX\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-b-using-rwx-readwritemany","title":"Option B: Using RWX (ReadWriteMany)","text":"<ol> <li>Enable RWX in Longhorn UI: </li> <li>Go to Settings \u2192 Enable NFSv4 (or Samba).  </li> <li>Create an RWX PVC: <pre><code>accessModes:\n  - ReadWriteMany\n</code></pre></li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#3-how-to-increase-or-decrease-volume-size","title":"3. How to Increase or Decrease Volume Size","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-1-expand-a-volume-increase-size","title":"Option 1: Expand a Volume (Increase Size)","text":"<ol> <li> <p>Edit the PVC: <pre><code>kubectl edit pvc &lt;pvc-name&gt;\n</code></pre>    Change <code>storage: 5Gi</code> \u2192 <code>storage: 20Gi</code>.  </p> </li> <li> <p>Verify Expansion in Longhorn UI: </p> </li> <li> <p>Go to Volumes \u2192 Check the new size.  </p> </li> <li> <p>Resize Filesystem (if needed): <pre><code>kubectl exec -it &lt;pod-name&gt; -- df -h /var/lib/mysql  # Check current size\nkubectl exec -it &lt;pod-name&gt; -- resize2fs /dev/longhorn/&lt;volume-name&gt;  # For ext4\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#option-2-shrink-a-volume-decrease-size","title":"Option 2: Shrink a Volume (Decrease Size)","text":"<p>\u26a0 Shrinking is risky and not officially supported! - Workaround:   1. Take a snapshot.   2. Create a new smaller volume and restore data.   3. Update the PVC to point to the new volume.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#4-real-world-scenarios","title":"4. Real-World Scenarios","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#scenario-1-scaling-mysql-storage","title":"Scenario 1: Scaling MySQL Storage","text":"<ul> <li>Problem: MySQL is running out of space.  </li> <li>Solution: <pre><code>kubectl edit pvc mysql-pvc  # Change 10Gi \u2192 50Gi\n</code></pre>   Longhorn automatically expands the volume.  </li> </ul>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#scenario-2-shared-storage-for-wordpress","title":"Scenario 2: Shared Storage for WordPress","text":"<ul> <li>Problem: Multiple WordPress pods need shared uploads.  </li> <li>Solution: <pre><code>accessModes:\n  - ReadWriteMany  # Uses NFS under the hood\n</code></pre></li> </ul>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#5-best-practices","title":"5. Best Practices","text":"<p>\u2714 For databases: Use <code>ReadWriteOnce</code> (RWO). \u2714 For shared files: Use <code>ReadWriteMany</code> (RWX) with NFS. \u2714 Always take snapshots before resizing. \u274c Avoid shrinking volumes (data loss risk).  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#conclusion","title":"Conclusion","text":"<ul> <li>Access Modes: </li> <li><code>RWO</code> = Single pod (default).  </li> <li><code>RWX</code> = Multi-pod (requires NFS).  </li> <li>Resizing: </li> <li>Increase size \u2192 Edit PVC.  </li> <li>Decrease size \u2192 Backup &amp; restore to a new volume.  </li> </ul> <p>\ud83d\ude80 Next Steps: - Try dynamic volume provisioning with <code>storageClassName: longhorn</code>. - Explore backup &amp; restore to S3.  </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-by-step-demo-resizing-longhorn-volumes-in-kubernetes","title":"Step-by-Step Demo: Resizing Longhorn Volumes in Kubernetes","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#what-well-cover","title":"\ud83d\udd25 What We'll Cover","text":"<ol> <li>Creating a PVC (5Gi \u2192 20Gi expansion)  </li> <li>Attaching it to a MySQL Pod </li> <li>Expanding the Volume (Live resize)  </li> <li>Shrinking Safely (Via backup &amp; restore)  </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-1-deploy-a-test-mysql-pod-with-longhorn-pvc","title":"Step 1: Deploy a Test MySQL Pod with Longhorn PVC","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#11-create-a-pvc-mysql-pvcyaml","title":"1.1 Create a PVC (<code>mysql-pvc.yaml</code>)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi  # We'll expand this later\n</code></pre> <p>Apply it: <pre><code>kubectl apply -f mysql-pvc.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#12-deploy-mysql-mysql-deploymentyaml","title":"1.2 Deploy MySQL (<code>mysql-deployment.yaml</code>)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        volumeMounts:\n        - name: mysql-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n</code></pre> <p>Deploy: <pre><code>kubectl apply -f mysql-deployment.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#13-verify-pvc-pod","title":"1.3 Verify PVC &amp; Pod","text":"<pre><code>kubectl get pvc  # Should show 5Gi\nkubectl get pods  # Wait for MySQL to run\n</code></pre>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-2-expand-volume-from-5gi-20gi","title":"Step 2: Expand Volume from 5Gi \u2192 20Gi","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#21-edit-pvc-live","title":"2.1 Edit PVC Live","text":"<p><pre><code>kubectl edit pvc mysql-pvc\n</code></pre> Change: <pre><code>resources:\n  requests:\n    storage: 20Gi  # Update from 5Gi \u2192 20Gi\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#22-verify-expansion-in-longhorn-ui","title":"2.2 Verify Expansion in Longhorn UI","text":"<ol> <li> <p>Open Longhorn Dashboard:    <pre><code>kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80\n</code></pre>    Visit http://localhost:8080 \u2192 Check volume size.</p> </li> <li> <p>Confirm in Kubernetes:    <pre><code>kubectl get pvc mysql-pvc  # Should show 20Gi\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#23-resize-filesystem-if-needed","title":"2.3 Resize Filesystem (If Needed)","text":"<p>If MySQL doesn\u2019t see the new space: <pre><code># Enter the MySQL pod\nkubectl exec -it &lt;mysql-pod-name&gt; -- bash\n\n# Check current disk space\ndf -h /var/lib/mysql  # Likely still shows 5Gi\n\n# Resize ext4 filesystem (for Longhorn volumes)\nresize2fs /dev/longhorn/&lt;volume-name&gt;  # Autocomplete with `ls /dev/longhorn/`\n\n# Verify\ndf -h /var/lib/mysql  # Now shows 20Gi\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#step-3-safely-shrink-volume-20gi-10gi","title":"Step 3: Safely Shrink Volume (20Gi \u2192 10Gi)","text":""},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#31-take-a-snapshot-longhorn-ui","title":"3.1 Take a Snapshot (Longhorn UI)","text":"<ol> <li>Go to Volumes \u2192 Select <code>mysql-pvc</code>.  </li> <li>Click \"Take Snapshot\".  </li> <li>(Optional) Backup to S3 for disaster recovery.  </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#32-create-a-new-smaller-pvc","title":"3.2 Create a New Smaller PVC","text":"<pre><code># new-mysql-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc-small\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi  # Smaller size\n</code></pre> <p>Apply: <pre><code>kubectl apply -f new-mysql-pvc.yaml\n</code></pre></p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#33-restore-data-to-new-pvc","title":"3.3 Restore Data to New PVC","text":"<ol> <li>In Longhorn UI:  </li> <li>Go to Snapshot \u2192 \"Create Volume\" from snapshot.  </li> <li> <p>Attach it to <code>mysql-pvc-small</code>.  </p> </li> <li> <p>Update MySQL Deployment:    <pre><code>kubectl edit deployment mysql\n</code></pre>    Change:    <pre><code>volumes:\n- name: mysql-storage\n  persistentVolumeClaim:\n    claimName: mysql-pvc-small  # Point to new PVC\n</code></pre></p> </li> <li> <p>Verify:    <pre><code>kubectl get pvc  # Should show mysql-pvc-small (10Gi)\nkubectl exec -it &lt;mysql-pod&gt; -- df -h /var/lib/mysql  # Should show 10Gi\n</code></pre></p> </li> </ol>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Expanding is easy: Just edit PVC \u2192 Longhorn handles the rest. \u26a0 Shrinking is risky: Requires backup \u2192 restore to a new volume. \ud83d\udd27 Always test resizing in staging first! </p>"},{"location":"Longhorn/Longhorn%20Access%20Modes%20%26%20Volume%20Resizing%20Guide/#next-steps","title":"Next Steps","text":"<ul> <li>Automate snapshots with Longhorn\u2019s Recurring Jobs.  </li> <li>Try RWX volumes for shared storage (e.g., WordPress).  </li> </ul> <p>Need help with specific workloads (PostgreSQL, MongoDB)? Ask away! \ud83d\ude80</p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/","title":"Longhorn in Kubernetes","text":""},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#1-what-is-longhorn","title":"1. What is Longhorn?","text":"<p>Longhorn is a cloud-native, distributed storage system for Kubernetes that provides persistent block storage using dynamically provisioned volumes. It is lightweight, easy to deploy, and highly resilient, making it ideal for stateful applications in Kubernetes.  </p> <p>Real-Time Scenario: A company running a MySQL database on Kubernetes needs persistent storage that survives pod restarts. Longhorn provides replicated volumes, ensuring data remains available even if a node fails.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#2-why-use-longhorn-in-kubernetes","title":"2. Why Use Longhorn in Kubernetes?","text":"<ul> <li>Decentralized Storage: No single point of failure.  </li> <li>Snapshot &amp; Backup: Supports point-in-time snapshots and backups to S3-compatible storage.  </li> <li>Thin Provisioning: Efficient disk space usage.  </li> <li>Cross-Cluster Replication: Enables disaster recovery.  </li> </ul> <p>Real-Time Scenario: An e-commerce platform running MongoDB needs automated backups. Longhorn takes snapshots every hour and backs them up to AWS S3, ensuring data recovery in case of corruption.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#3-how-does-longhorn-work","title":"3. How Does Longhorn Work?","text":"<ul> <li>Volume Replication: Each volume is replicated across multiple nodes (default: 3 replicas).  </li> <li>Scheduling: Longhorn dynamically schedules replicas based on node availability.  </li> <li>Recovery: If a node fails, Longhorn rebuilds replicas automatically.  </li> </ul> <p>Real-Time Scenario: A Node failure occurs in a Kubernetes cluster running a PostgreSQL database. Longhorn detects the failure and rebuilds the lost replica on another healthy node, ensuring high availability.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#4-where-is-longhorn-used","title":"4. Where is Longhorn Used?","text":"<ul> <li>On-Premises Kubernetes Clusters (e.g., Rancher, K3s)  </li> <li>Hybrid &amp; Multi-Cloud Deployments (AWS EKS, GCP GKE, Azure AKS)  </li> <li>Edge Computing &amp; IoT (Lightweight storage for distributed apps)  </li> </ul> <p>Real-Time Scenario: A financial services firm uses Longhorn in multi-cloud Kubernetes (EKS + on-prem) to ensure consistent storage for Fraud Detection Microservices, avoiding vendor lock-in.  </p>"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#key-benefits-challenges","title":"Key Benefits &amp; Challenges","text":"Benefits Challenges \u2705 Easy to deploy &amp; manage \u274c Not suitable for high-throughput workloads (e.g., big data) \u2705 Self-healing &amp; fault-tolerant \u274c Requires proper resource planning (CPU/memory for replicas) \u2705 Works across multiple clouds \u274c Backup performance depends on network bandwidth"},{"location":"Longhorn/Longhorn%20in%20Kubernetes/#conclusion","title":"Conclusion","text":"<p>Longhorn is an ideal choice for Kubernetes users needing reliable, distributed block storage with easy backup &amp; recovery. It shines in stateful applications (databases, message queues) but may not be optimal for high-performance storage needs.  </p> <p>Would you like a hands-on demo of setting up Longhorn in a real Kubernetes cluster? \ud83d\ude80</p>"},{"location":"Namespace/Delete%20all%20resources%20from%20namespace/","title":"Delete all resources from namespace","text":"<p>Run the following command to delete all resources in a namespace:</p> <pre><code>kubectl delete all --all -n demo\n</code></pre> <p>To also delete configmaps, secrets, PVCs, etc., run:</p> <pre><code>kubectl delete all,cm,secret,pvc,ingress,role,rolebinding,serviceaccount --all -n demo\n</code></pre> <p>Or to delete the entire namespace (which deletes everything inside):</p> <pre><code>kubectl delete namespace demo\n</code></pre>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/","title":"Create a Docker image registry in Nexus","text":"<p>Here are the steps to create a Docker image registry in Nexus (using Nexus OSS) and allow clients to push/pull using username/password authentication.</p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#1-install-nexus-repository-oss","title":"\u2705 1. Install Nexus Repository OSS","text":"<p>On a server (e.g., <code>10.14.14.17</code>):</p> <pre><code>docker run -d -p 8081:8081 --name nexus \\\n  -v nexus-data:/nexus-data \\\n  sonatype/nexus3\n</code></pre> <p>Access via: <code>http://10.14.14.17:8081</code></p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#2-login-setup","title":"\u2705 2. Login &amp; Setup","text":"<ul> <li> <p>Default credentials:</p> <ul> <li>User: <code>admin</code></li> <li> <p>Password: check with:</p> <pre><code>docker exec nexus cat /nexus-data/admin.password\n</code></pre> </li> </ul> </li> <li> <p>Login \u2192 Change password</p> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#3-create-a-docker-hosted-registry","title":"\u2705 3. Create a Docker Hosted Registry","text":"<ul> <li>Go to: \"Repositories\" \u2192 \"Create repository\"</li> <li> <p>Choose: docker (hosted)</p> </li> <li> <p>Fill:</p> <ul> <li>Name: <code>docker-hosted</code></li> <li>HTTP port: <code>5007</code></li> <li>Blob store: (default)</li> <li>Enable Allow anonymous access (optional)</li> </ul> </li> <li> <p>Click Create</p> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#4-create-a-user","title":"\u2705 4. Create a User","text":"<ul> <li>Go to: \"Security\" \u2192 \"Users\"</li> <li>Create a new user (e.g., <code>dockeruser</code>)</li> <li>Set roles:<ul> <li><code>nx-repository-view-docker-docker-hosted-*</code> (read + write)</li> </ul> </li> </ul>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#5-allow-docker-client-access-insecure-or-secure","title":"\u2705 5. Allow Docker Client Access (Insecure or Secure)","text":""},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#option-a-insecure-registry","title":"Option A: Insecure Registry","text":"<p>Edit <code>/etc/containers/registries.conf</code> (Podman) or <code>/etc/docker/daemon.json</code> (Docker):</p> <pre><code>{\n  \"insecure-registries\": [\"10.14.14.17:5007\"]\n}\n</code></pre> <p>Then restart:</p> <pre><code>sudo systemctl restart docker\n# or\nsudo systemctl restart podman\n</code></pre>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#6-login-from-client-machine","title":"\u2705 6. Login from Client Machine","text":"<pre><code>podman login 10.14.14.17:5007\n# OR\ndocker login 10.14.14.17:5007\n</code></pre> <p>Enter: - Username: <code>dockeruser</code> - Password: (your created password)</p>"},{"location":"Nexus/Create%20a%20Docker%20image%20registry%20in%20Nexus/#7-tag-and-push-image","title":"\u2705 7. Tag and Push Image","text":"<pre><code>podman tag localhost/ubi-unzip:latest 10.14.14.17:5007/cron-image/busybox:latest\npodman push 10.14.14.17:5007/cron-image/busybox:latest\n</code></pre> <p>Let me know if you want to add TLS/SSL or set up proxy/hosted group registries.</p>"},{"location":"OpenTofu/About%20OpenTofu/","title":"About OpenTofu","text":"<p>No, OpenTofu (a fork of Terraform) is not only for cloud infrastructure.</p> <p>https://opentofu.org/docs/intro/core-workflow/</p> What Open-source infrastructure-as-code (IaC) tool (Terraform fork) to manage infrastructure using <code>.tf</code> files. Why Automates provisioning, scaling, and managing infrastructure consistently and repeatably. Avoids manual errors. Where Can be used for cloud (AWS, Azure, GCP) and on-premises (VMware, OpenStack, bare metal, K8s, etc.) environments. When Use when you need to version, manage, or provision infrastructure automatically with code and apply the same config anywhere. How Write <code>.tf</code> configs \u2192 run <code>opentofu init</code> \u2192 <code>opentofu plan</code> \u2192 <code>opentofu apply</code>. Uses providers for target platforms. <p>\u2705 It supports cloud + hybrid + on-prem setups, depending on provider availability.</p> <p>Here\u2019s a categorized list of what you can do with OpenTofu:</p> Category Use Cases Cloud Infrastructure Provision VMs, networks, load balancers on AWS, Azure, GCP Kubernetes Deploy EKS, AKS, GKE, manage K8s resources via <code>kubernetes</code> provider On-Prem Infra Manage VMware, OpenStack, bare-metal servers Containers &amp; Clusters Create Docker containers, Podman setups, K3s/K8s clusters (via scripts) CI/CD Setup Provision Jenkins, GitLab Runners, Agents on cloud or VMs DNS Management Create/update DNS zones and records (Cloudflare, Route53, etc.) Load Balancers &amp; Firewalls Manage LB rules, WAFs, security groups IAM &amp; Security Create users, roles, policies in AWS, Azure, GCP Secrets Management Integrate with Vault, AWS Secrets Manager, SOPS Monitoring Tools Deploy Prometheus, Grafana, Loki stacks Database Setup Provision RDS, CloudSQL, MySQL/Postgres containers Serverless Manage Lambda, Azure Functions, GCP Cloud Functions Storage Create S3 buckets, disks, volumes, NFS mounts Networking VPCs, subnets, routes, VPN, NAT gateways Edge/IoT Provision edge nodes, IoT device config via custom providers or scripts Multi-Cloud Management Use single <code>.tf</code> to deploy infra across multiple clouds Automation Scripts Run shell scripts, Ansible roles, local-exec commands Version Control &amp; GitOps Manage infra via VCS triggers (GitHub Actions, GitLab CI, etc.) Infrastructure Testing Validate infra using Terratest, Checkov, or pre/post-hooks # Terraform vs OpenTofu Aspect Terraform OpenTofu License BSL (Business Source License) \u2014 restricted for large-scale use MPL (Mozilla Public License) \u2014 truly open-source Community Huge, well-established Growing fast with open-source supporters (e.g., Gruntwork, Spacelift) Enterprise Support Official support via HashiCorp No official enterprise from a single vendor (yet) State Management Terraform Cloud + CLI state backend options No Terraform Cloud, but supports same backends (S3, GCS, etc.) Feature Parity Leading with new features Follows Terraform v1.6+ features (frozen after the fork) Modules/Providers Vast ecosystem, official registry Uses same ecosystem; registry is mirrored and interchangeable Community Governance Controlled by HashiCorp Community-driven (by Linux Foundation, OpenTofu community) Tooling Support Supported by many tools: Atlantis, Spacelift, Env0, Terraform Cloud Most Terraform tools already compatible or adapting (e.g., Spacelift) Future Risk May restrict more under BSL, limited by HashiCorp's roadmap Open roadmap, safe from vendor lock-in Backward Compatibility Maintains it but subject to license terms Fully backward compatible with Terraform \u22641.6"},{"location":"OpenTofu/About%20OpenTofu/#tldr","title":"TL;DR","text":"<ul> <li> <p>\u2705 Use OpenTofu if you want open-source freedom, no vendor lock-in, and alignment with community-led governance.</p> </li> <li> <p>\u2705 Use Terraform if you need official enterprise support, and are OK with BSL and using Terraform Cloud.</p> </li> </ul> <p>Let me know if you want a command compatibility table too.</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/","title":"PV's and PVC's in Kubernetes","text":""},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#what-are-persistentvolumes-pvs-and-persistentvolumeclaims-pvcs","title":"What are PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs)?","text":"<p>PersistentVolumes (PVs): - Cluster-wide storage resources provisioned by administrators - Represent physical storage (NFS, iSCSI, cloud storage, etc.) in the cluster - Have a lifecycle independent of any individual Pod - Can be statically provisioned (pre-created) or dynamically provisioned (on-demand)</p> <p>PersistentVolumeClaims (PVCs): - Requests for storage by users/application Pods - Bind to available PVs that match their requirements - Act as an abstraction layer between Pods and physical storage - Are namespaced resources (unlike PVs which are cluster-scoped)</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#why-use-pvs-and-pvcs","title":"Why use PVs and PVCs?","text":"<ol> <li>Data persistence: Survive Pod restarts and crashes</li> <li>Storage abstraction: Decouple storage configuration from Pod specs</li> <li>Dynamic provisioning: Automatically create storage when needed</li> <li>Resource management: Control storage allocation and access</li> <li>Portability: Same manifest can work across different storage backends</li> <li>Lifecycle management: Handle storage reclaim policies (retain, delete, recycle)</li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#when-to-use-pvs-and-pvcs","title":"When to use PVs and PVCs?","text":"<p>Use PV/PVC when: - Your application needs to persist data beyond a Pod's lifecycle - Multiple Pods need to share the same storage - You need different access modes (ReadWriteOnce, ReadOnlyMany, ReadWriteMany) - You want to manage storage separately from application deployments - You need storage with specific performance characteristics (SSD vs HDD) - You're running stateful applications (databases, message queues, etc.)</p>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#how-to-use-pvs-and-pvcs","title":"How to use PVs and PVCs?","text":""},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#basic-usage","title":"Basic Usage","text":"<ol> <li>Static Provisioning (Admin creates PV first)</li> </ol> <pre><code># PersistentVolume (cluster-admin creates)\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: manual\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre> <pre><code># PersistentVolumeClaim (user creates)\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n</code></pre> <ol> <li>Dynamic Provisioning (Automatic PV creation)</li> </ol> <pre><code># StorageClass (cluster-admin creates)\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp3\n</code></pre> <pre><code># PVC that triggers dynamic provisioning\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: dynamic-pvc\nspec:\n  storageClassName: fast\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n</code></pre> <ol> <li>Using PVC in a Pod</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: mycontainer\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/usr/share/nginx/html\"\n        name: my-storage\n  volumes:\n    - name: my-storage\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#advanced-usage","title":"Advanced Usage","text":"<ol> <li> <p>Volume Snapshots: <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: my-snapshot\nspec:\n  volumeSnapshotClassName: csi-aws-vsc\n  source:\n    persistentVolumeClaimName: my-pvc\n</code></pre></p> </li> <li> <p>Raw Block Volumes: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre></p> </li> <li> <p>Volume Expansion (requires StorageClass to allow it): <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: expandable\nprovisioner: kubernetes.io/aws-ebs\nallowVolumeExpansion: true\n</code></pre></p> </li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Use Dynamic Provisioning for most cases (simpler management)</li> <li>Set appropriate reclaim policies:</li> <li><code>Retain</code>: Keep data after PVC deletion (manual cleanup)</li> <li><code>Delete</code>: Automatically delete storage (be careful with production data)</li> <li>Use StorageClasses to abstract storage backend details</li> <li>Monitor storage usage to avoid unexpected costs</li> <li>Consider volume snapshots for backup strategies</li> <li>Use volume modes appropriately:</li> <li><code>Filesystem</code> (default) for most applications</li> <li><code>Block</code> for databases or performance-sensitive apps</li> <li>Set resource requests/limits for StatefulSets using PVCs</li> </ol>"},{"location":"PV%20n%20PVCs/PV%27s%20and%20PVC%27s%20in%20Kubernetes/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>PVC stuck in \"Pending\" state:</li> <li>Check if storage class exists</li> <li>Verify sufficient capacity is available</li> <li> <p>Check provisioner logs</p> </li> <li> <p>Mount errors:</p> </li> <li>Verify access modes match between PV and PVC</li> <li> <p>Check if multiple Pods are trying to use ReadWriteOnce</p> </li> <li> <p>Common commands:    <pre><code># Check PVs and PVCs\nkubectl get pv\nkubectl get pvc --all-namespaces\n\n# Describe resources for details\nkubectl describe pv my-pv\nkubectl describe pvc my-pvc\n\n# Check storage classes\nkubectl get storageclass\n\n# Check events for issues\nkubectl get events --sort-by=.metadata.creationTimestamp\n</code></pre></p> </li> <li> <p>Access modes:</p> </li> <li><code>ReadWriteOnce</code> (RWO): Read-write by one node</li> <li><code>ReadOnlyMany</code> (ROX): Read-only by many nodes</li> <li><code>ReadWriteMany</code> (RWX): Read-write by many nodes</li> </ol>"},{"location":"Pods/Debug%20pod%20-%20create%20for%20test%20and%20delete%20on%20post%20test/","title":"Debug pod   create for test and delete on post test","text":"<p>Yes, you can access the pods, but not via external IP because it's a headless service (<code>ClusterIP: None</code>) \u2014 it only helps with pod discovery, not load balancing or external access.</p> <pre><code>gouse@gouse:~/DevOps/k3d$ kubectl run curlpod --rm -it --image=curlimages/curl -- /bin/sh\nIf you don't see a command prompt, try pressing enter.\n~ $ curl nginx-0.nginx-headless.default.svc.cluster.local\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n~ $ exit\nSession ended, resume using 'kubectl attach curlpod -c curlpod -i -t' command when the pod is running\npod \"curlpod\" deleted\n</code></pre>"},{"location":"Pods/Debug%20pod%20-%20create%20for%20test%20and%20delete%20on%20post%20test/#ways-to-access","title":"\u2705 Ways to Access","text":""},{"location":"Pods/Debug%20pod%20-%20create%20for%20test%20and%20delete%20on%20post%20test/#1-from-inside-the-cluster-eg-debug-pod","title":"1. From inside the cluster (e.g., debug pod):","text":"<pre><code>kubectl run curlpod --rm -it --image=curlimages/curl -- /bin/sh\n</code></pre> <p>Then try:</p> <pre><code>curl nginx-0.nginx-headless.default.svc.cluster.local\n</code></pre> <p>Or test all pods:</p> <pre><code>for i in 0 1 2; do curl nginx-$i.nginx-headless.default.svc.cluster.local; done\n</code></pre>"},{"location":"Pods/Debug%20pod%20-%20create%20for%20test%20and%20delete%20on%20post%20test/#2-if-you-want-external-access-expose-with-a-regular-service","title":"2. If you want external access, expose with a regular service:","text":"<pre><code>kubectl expose statefulset nginx --name=nginx-service --port=80 --target-port=80 --type=NodePort\n</code></pre> <p>Then get:</p> <pre><code>kubectl get svc nginx-service\n</code></pre> <p>Access via:</p> <pre><code>http://&lt;NodeIP&gt;:&lt;NodePort&gt;\n</code></pre> <p>You can also do and access this <code>statefulset</code> with <code>headless applicaiton</code> to access, if you want an <code>Ingress</code> setup or <code>LoadBalancer</code> type instead.</p>"},{"location":"Pods/List%20all%20pods%20with%20their%20init%20containers%20and%20sidecar%20containers%20info/","title":"List all pods with their init containers and sidecar containers info","text":"<pre><code># List all pods with their init containers and sidecar containers info\n\nkubectl get pods --all-namespaces -o json | jq -r '\n  .items[] | \n  {\n    namespace: .metadata.namespace,\n    pod: .metadata.name,\n    initContainers: (.spec.initContainers // [] | map(.name)),\n    containers: (.spec.containers // [] | map(.name))\n  } | \n  \"Namespace: \\(.namespace)\\nPod: \\(.pod)\\nInit Containers: \\(.initContainers | join(\", \"))\\nContainers: \\(.containers | join(\", \"))\\n\"\n'\n</code></pre> <p>If you don't have <code>jq</code>, you can do a simpler approach:</p> <pre><code>kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,POD:.metadata.name,INIT_CONTAINERS:.spec.initContainers[*].name,CONTAINERS:.spec.containers[*].name\n</code></pre> <p>This shows init containers and sidecar (all containers except main) by pod. Sidecars are normally just additional containers listed alongside main containers.</p>"},{"location":"Pods/Types%20of%20Containers%20in%20Kubernetes/","title":"Types of Containers in Kubernetes","text":"Type Description Use Cases Example Tools/Workloads Standard Containers OCI-compliant containers (Docker, containerd, CRI-O). Microservices, web apps. <code>nginx</code>, <code>redis</code>, custom apps. Windows Containers Containers running Windows OS (requires Windows nodes). .NET apps, legacy Windows services. IIS, SQL Server on Windows. Init Containers Run before main app containers in a pod for setup tasks. DB migrations, config preloading. <code>alpine</code> (for setup scripts). Sidecar Containers Auxiliary containers running alongside the main app in the same pod. Logging, monitoring, proxying. <code>Fluentd</code>, <code>Envoy</code>, <code>Prometheus exporter</code>. Ephemeral Containers Temporary containers for debugging (attached to running pods). Troubleshooting without restarting pods. <code>busybox</code>, <code>alpine</code> (for <code>kubectl debug</code>). Job/CronJob Runs a container to completion (Job) or on a schedule (CronJob). Batch processing, backups, reports. Database backups, data processing scripts. DaemonSet Runs one instance per node (cluster-wide). Log collectors, node monitoring. <code>kube-proxy</code>, <code>Node Exporter</code>. StatefulSet Manages stateful apps with stable network IDs and storage. Databases, distributed storage. <code>MySQL</code>, <code>MongoDB</code>, <code>Elasticsearch</code>. GPU-Accelerated Containers requiring GPUs/TPUs (needs node support). AI/ML workloads, video encoding. <code>TensorFlow</code>, <code>PyTorch</code>. Security-Hardened Containers with extra isolation (sandboxing/VMs). Multi-tenant clusters, untrusted workloads. <code>gVisor</code>, <code>Kata Containers</code>. Multi-Container Pods Pods with multiple containers sharing network/storage. Co-located helper services (e.g., log shipper + app). <code>app + Fluentd</code> sidecar. Serverless (FaaS) Containers deployed as serverless functions (scales to zero). Event-driven workloads. <code>Knative</code>, <code>OpenFaaS</code>, <code>AWS Lambda on EKS</code>. Wasm Containers WebAssembly-based workloads (experimental). Edge computing, lightweight plugins. <code>wasmtime</code>, <code>wasmedge</code>. ### Key Notes: 1. OCI Runtimes: Kubernetes now defaults to <code>containerd</code> or <code>CRI-O</code> (Docker is deprecated but still usable via shims). 2. Windows Support: Requires Windows nodes and compatible images. 3. Security: Use <code>gVisor</code>/<code>Kata</code> for sandboxing, <code>PodSecurityPolicy</code> (or Kyverno/OPA) for restrictions. 4. Debugging: Ephemeral containers allow live troubleshooting without pod restarts."},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/","title":"Deep Dive into securing secrets in specific envs","text":"<p>Fantastic! Let\u2019s dive deeper into securing secrets in specific environments, with real-world examples, tools, and actionable best practices.  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#1-kubernetes-secrets-management","title":"1. Kubernetes Secrets Management","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-teslas-kubernetes-crypto-mining-breach","title":"\u26a0\ufe0f Real-World Incident: Tesla\u2019s Kubernetes Crypto Mining Breach","text":"<ul> <li>What Happened? Tesla\u2019s Kubernetes dashboard was left unpassworded, allowing attackers to deploy crypto-mining pods.  </li> <li>Root Cause: Misconfigured RBAC + exposed secrets in environment variables.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-kubernetes-secrets","title":"\ud83d\udd12 How to Secure Kubernetes Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-avoid-default-secrets-base64-isnt-encryption","title":"A. Avoid Default Secrets (Base64 Isn\u2019t Encryption!)","text":"<p>\u274c Bad: Storing secrets in plaintext YAML (even if Base64-encoded). \u2705 Fix: Use: - Sealed Secrets (Mozilla SOPS, Kubeseal) - External Secrets Operator (syncs with AWS/Azure/HashiCorp Vault)  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-restrict-access","title":"B. Restrict Access","text":"<ul> <li>RBAC: Limit <code>get</code>/<code>list</code> access to secrets.  </li> <li>Pod Security Policies: Block pods from mounting host secrets.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-dynamic-secrets","title":"C. Dynamic Secrets","text":"<ul> <li>HashiCorp Vault + Kubernetes Auth: Automatically generates short-lived DB credentials.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose Kubeseal Encrypts secrets for GitOps Vault Agent Injects secrets into pods securely Kyverno Policies to block plaintext secrets"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#2-cicd-pipeline-secrets","title":"2. CI/CD Pipeline Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-circleci-breach-2023","title":"\u26a0\ufe0f Real-World Incident: CircleCI Breach (2023)","text":"<ul> <li>What Happened? Attackers stole CI/CD session tokens, granting access to customer code + secrets.  </li> <li>Root Cause: Engineers stored secrets in plaintext env vars.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-cicd-secrets","title":"\ud83d\udd12 How to Secure CI/CD Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-store-secrets-in-plaintext","title":"A. Never Store Secrets in Plaintext","text":"<p>\u274c Bad: <pre><code># GitHub Actions (UNSAFE)  \nenv:  \n  AWS_KEY: \"AKIA...\"  \n</code></pre> \u2705 Fix: - Use encrypted secrets (GitHub Secrets, GitLab CI Variables). - For temporary credentials, use OIDC (e.g., AWS IAM Roles for GitHub Actions).  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-limit-secret-access","title":"B. Limit Secret Access","text":"<ul> <li>Scoped Secrets: Restrict by branch/environment (e.g., prod vs. staging).  </li> <li>Audit Logs: Monitor who accesses secrets (e.g., GitLab Audit Events).  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-ephemeral-secrets","title":"C. Ephemeral Secrets","text":"<ul> <li>Vault Dynamic Secrets: Generate a new DB password per pipeline run.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_1","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose GitHub Actions OIDC Temporary AWS creds via IAM Roles HashiCorp Vault Dynamic secrets for CI/CD Argo Workflows Secure secret injection in pipelines"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#3-cloud-secrets-awsazuregcp","title":"3. Cloud Secrets (AWS/Azure/GCP)","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-ubers-aws-key-leak-2022","title":"\u26a0\ufe0f Real-World Incident: Uber\u2019s AWS Key Leak (2022)","text":"<ul> <li>What Happened? An engineer committed an AWS key to a private repo; attackers found it and breached Uber\u2019s internal systems.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-cloud-secrets","title":"\ud83d\udd12 How to Secure Cloud Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-use-long-term-keys","title":"A. Never Use Long-Term Keys","text":"<p>\u274c Bad: Hardcoded <code>~/.aws/credentials</code> files. \u2705 Fix: - IAM Roles (for EC2, Lambda, ECS). - AWS Session Manager (SSH-less server access).  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-rotate-secrets-automatically","title":"B. Rotate Secrets Automatically","text":"<ul> <li>AWS Secrets Manager: Auto-rotates RDS passwords.  </li> <li>Vault Dynamic AWS Creds: 1-hour expiry for CLI access.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-monitor-for-leaks","title":"C. Monitor for Leaks","text":"<ul> <li>AWS GuardDuty: Alerts on anomalous key usage.  </li> <li>GitGuardian: Scans GitHub/GitLab for leaked keys.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_2","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose AWS Secrets Manager Auto-rotation + audit logging GCP Secret Manager Versioned secrets for GCP Azure Key Vault Integrates with AAD for RBAC"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#4-developer-workstations-local-secrets","title":"4. Developer Workstations (Local Secrets)","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#real-world-incident-codecov-hack-2021","title":"\u26a0\ufe0f Real-World Incident: Codecov Hack (2021)","text":"<ul> <li>What Happened? Attackers modified Codecov\u2019s Bash script to exfiltrate env vars (including secrets) from CI/CD systems.  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#how-to-secure-local-secrets","title":"\ud83d\udd12 How to Secure Local Secrets","text":""},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#a-never-store-in-plaintext","title":"A. Never Store in Plaintext","text":"<p>\u274c Bad: <pre><code># ~/.bashrc (UNSAFE)  \nexport DB_PASSWORD=\"123456\"  \n</code></pre> \u2705 Fix: - Use 1Password CLI or macOS Keychain. - For dev environments, use dotenv + .gitignore.  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#b-hardware-backed-secrets","title":"B. Hardware-Backed Secrets","text":"<ul> <li>YubiKey: Store GPG/SSH keys securely.  </li> <li>Temporary SSH Certificates (e.g., Teleport).  </li> </ul>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#c-scan-for-accidental-leaks","title":"C. Scan for Accidental Leaks","text":"<pre><code># Install trufflehog &amp; scan your repo  \npip install trufflehog  \ntrufflehog git https://github.com/your/repo --fail  \n</code></pre>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#tools_3","title":"\ud83d\udee0\ufe0f Tools:","text":"Tool Purpose 1Password CLI Secure local secret storage TruffleHog Detect secrets in Git history gopass Team password management"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#pro-tip-secret-zero-problem","title":"\ud83d\udd25 Pro Tip: Secret Zero Problem","text":"<p>Even with perfect secrets management, how do you bootstrap the first secret? (e.g., the password to unlock Vault?) - Solutions:   - Shamir\u2019s Secret Sharing: Split the root key among team leads.   - Cloud KMS: AWS KMS/GCP Cloud HSM for auto-unsealing Vault.  </p>"},{"location":"Secrets/Deep%20Dive%20into%20securing%20secrets%20in%20specific%20envs/#checklist-immediate-actions","title":"\ud83d\ude80 Checklist: Immediate Actions","text":"<ol> <li>Scan your repos: <code>trufflehog git https://github.com/your/repo</code> </li> <li>Rotate long-lived keys: AWS/GitHub/DB passwords.  </li> <li>Enable MFA for all secrets managers (Vault, AWS, etc.).  </li> <li>Audit who has access: Principle of least privilege.  </li> </ol>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/","title":"Secrets management for sensitive information","text":""},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#1-what","title":"1. WHAT","text":"<ul> <li>Definition: Secrets are sensitive pieces of information that grant access to systems, data, or resources (e.g., passwords, API keys, encryption keys, tokens, SSH keys).  </li> <li>Examples:  </li> <li>Database credentials  </li> <li>Cloud service access keys  </li> <li>OAuth tokens  </li> <li>Private certificates  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#2-why-why-are-secrets-important","title":"2. WHY (Why are secrets important?)","text":"<ul> <li>Security Risks:  </li> <li>Unauthorized access if leaked  </li> <li>Data breaches, financial loss, reputational damage  </li> <li>Compliance: Many regulations (GDPR, HIPAA, PCI-DSS) require proper secrets management.  </li> <li>Operational Integrity: Prevents service disruptions due to compromised credentials.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#3-who-who-handles-secrets","title":"3. WHO (Who handles secrets?)","text":"<ul> <li>Developers: Need access for application integration.  </li> <li>DevOps/SREs: Manage secrets in CI/CD pipelines and infrastructure.  </li> <li>Security Teams: Enforce policies, monitor leaks, and rotate keys.  </li> <li>Attackers: Actively seek secrets via code leaks, phishing, or misconfigurations.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#4-how-how-to-manage-secrets-securely","title":"4. HOW (How to manage secrets securely?)","text":"<ul> <li>Best Practices:  </li> <li>Avoid Hardcoding: Never store secrets in plaintext in code/config files.  </li> <li>Use Secrets Managers: Tools like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault.  </li> <li>Environment Variables: Temporary storage (but not foolproof).  </li> <li>Access Controls: Principle of least privilege (restrict who can access secrets).  </li> <li>Rotation: Regularly update secrets to limit exposure.  </li> <li>Audit &amp; Monitoring: Track access and detect anomalies.  </li> <li>Encryption: Ensure secrets are encrypted at rest and in transit.  </li> </ul>"},{"location":"Secrets/Secrets%20management%20for%20sensitive%20information/#additional-considerations","title":"Additional Considerations","text":"<ul> <li>Zero Trust: Assume breaches can happen; verify every access request.  </li> <li>Secret Scanning: Use tools like GitGuardian, TruffleHog to detect leaks in repositories.  </li> </ul> <p>Would you like a deeper dive into any specific aspect (e.g., tools, implementation)?</p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/","title":"Some real world examples of secrets mismanagement","text":"<p>Here are some real-world examples of secrets mismanagement leading to security incidents, along with best-practice solutions:  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#1-github-api-key-leak-unauthorized-cloud-access","title":"1. \ud83d\udea8 GitHub API Key Leak \u2192 Unauthorized Cloud Access**","text":"<p>What Happened? - A developer accidentally committed a <code>.env</code> file containing AWS credentials to a public GitHub repo. - Attackers scanned GitHub, found the keys, and spun up cryptocurrency mining servers, costing the company $50,000+ in cloud bills.  </p> <p>How to Prevent? \u2705 Use Git Secrets/Pre-commit Hooks: Tools like <code>git-secrets</code> block commits containing sensitive data. \u2705 Automated Scanning: GitHub\u2019s built-in secret scanning or GitGuardian alerts on leaked keys. \u2705 Short-Lived Credentials: Use AWS IAM roles instead of long-term access keys.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#2-hardcoded-database-password-data-breach","title":"2. \ud83d\udd11 Hardcoded Database Password \u2192 Data Breach**","text":"<p>What Happened? - A fintech app had a PostgreSQL password hardcoded in its Dockerfile. - Hackers accessed the DB, stealing 300,000+ user records (PII and payment info).  </p> <p>How to Prevent? \u2705 Secrets Manager: Store DB creds in HashiCorp Vault/Azure Key Vault. \u2705 Dynamic Secrets: Tools like Vault generate short-lived DB credentials per request. \u2705 Infra-as-Code Checks: Use Terraform/Snyk to detect hardcoded secrets in configs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#3-exposed-kubernetes-config-cryptojacking-attack","title":"3. \u2601\ufe0f Exposed Kubernetes Config \u2192 Cryptojacking Attack**","text":"<p>What Happened? - A misconfigured Kubernetes cluster had its <code>kubeconfig</code> file (admin access) exposed on a public S3 bucket. - Attackers deployed malicious pods to mine Monero, disrupting production workloads.  </p> <p>How to Prevent? \u2705 RBAC: Limit <code>kubeconfig</code> access with Kubernetes Role-Based Access Control. \u2705 Sealed Secrets: Encrypt Kubernetes secrets using tools like Mozilla SOPS. \u2705 Cloud CSPM: Tools like Prisma Cloud scan for open S3 buckets/misconfigs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#4-mobile-app-api-key-fake-user-impersonation","title":"4. \ud83d\udcf1 Mobile App API Key \u2192 Fake User Impersonation**","text":"<p>What Happened? - A food delivery app embedded its Firebase API key in the client-side code. - Attackers extracted the key, bypassed auth, and placed fake orders charged to legitimate users.  </p> <p>How to Prevent? \u2705 Backend Proxying: Never expose API keys in frontend/mobile apps; route calls via a backend service. \u2705 Restrict Key Usage: Firebase allows domain/IP restrictions for API keys. \u2705 Key Rotation: Automatically rotate keys monthly.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#5-stolen-ssh-key-ransomware-attack","title":"5. \ud83d\udcbe Stolen SSH Key \u2192 Ransomware Attack**","text":"<p>What Happened? - An employee\u2019s SSH private key (without a passphrase) was stolen from their laptop. - Attackers accessed servers, deployed ransomware, and demanded $1M to restore data.  </p> <p>How to Prevent? \u2705 Passphrase-Protect Keys: Always encrypt SSH keys with a password. \u2705 Ephemeral SSH: Use temporary certificates (e.g., Teleport, AWS Session Manager). \u2705 Network Segmentation: Limit SSH access to jump hosts/VPNs.  </p>"},{"location":"Secrets/Some%20real-world%20examples%20of%20secrets%20mismanagement/#best-practices-summary","title":"Best Practices Summary","text":"Mistake Solution Tool Example Hardcoded secrets Use secrets managers HashiCorp Vault, AWS Secrets Manager Git leaks Pre-commit hooks + scanning GitGuardian, TruffleHog Long-lived keys Auto-rotation + short-lived creds AWS IAM, Vault dynamic secrets Misconfigured cloud CSPM + infrastructure checks Prisma Cloud, Checkov Exposed API keys Backend proxying + restrictions Firebase, API Gateway <p>Pro Tip: Run <code>trufflehog git https://github.com/your/repo</code> to scan your repos right now for leaked secrets!  </p> <p>Want a deep dive on securing secrets in a specific environment (e.g., Kubernetes, CI/CD)? Let me know! \ud83d\udd12</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/","title":"ServiceAccounts in Kubernetes","text":""},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#what-are-serviceaccounts-in-kubernetes","title":"What are ServiceAccounts in Kubernetes?","text":"<p>ServiceAccounts are Kubernetes objects that provide an identity for processes running in Pods. They: - Are namespaced resources (exist within a specific namespace) - Are used to authenticate Pods with the Kubernetes API server - Are automatically mounted into Pods at <code>/var/run/secrets/kubernetes.io/serviceaccount</code> - Are different from UserAccounts which are for human users</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#why-use-serviceaccounts","title":"Why use ServiceAccounts?","text":"<p>ServiceAccounts are essential because: 1. Security: They enable least-privilege access control for Pods 2. Authentication: They allow Kubernetes to identify which Pod is making API requests 3. Authorization: They work with RBAC (Role-Based Access Control) to define what actions Pods can perform 4. Automation: They enable secure communication between Pods and the API server without manual credential management</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#when-to-use-serviceaccounts","title":"When to use ServiceAccounts?","text":"<p>You should use ServiceAccounts when: - A Pod needs to interact with the Kubernetes API - Different Pods need different levels of access to cluster resources - You need to restrict what certain workloads can do in your cluster - You're implementing CI/CD pipelines that interact with Kubernetes - You're running custom controllers or operators</p>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#how-to-use-serviceaccounts","title":"How to use ServiceAccounts?","text":""},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#basic-usage","title":"Basic Usage","text":"<ol> <li> <p>Default ServiceAccount: Every namespace has a default ServiceAccount automatically used by Pods    <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mycontainer\n    image: myimage\n  # Automatically uses the 'default' ServiceAccount\n</code></pre></p> </li> <li> <p>Creating a custom ServiceAccount <pre><code>kubectl create serviceaccount my-sa\n</code></pre></p> </li> <li> <p>Assigning to a Pod <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  serviceAccountName: my-sa  # custom ServiceAccount\n  containers:\n  - name: mycontainer\n    image: myimage\n</code></pre></p> </li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#advanced-usage","title":"Advanced Usage","text":"<ol> <li> <p>Creating RBAC bindings (ClusterRole and RoleBinding)    <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> </li> <li> <p>Using in Deployments <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 3\n  template:\n    spec:\n      serviceAccountName: my-sa\n      containers:\n      - name: mycontainer\n        image: myimage\n</code></pre></p> </li> <li> <p>Inspecting the token (from inside a Pod)    <pre><code># Inside a Pod:\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre></p> </li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Avoid using the default ServiceAccount for production workloads</li> <li>Create dedicated ServiceAccounts for different types of workloads</li> <li>Follow the principle of least privilege when assigning permissions</li> <li>Regularly audit ServiceAccount permissions</li> <li>Consider using <code>automountServiceAccountToken: false</code> for Pods that don't need API access</li> </ol>"},{"location":"ServiceAccounts/ServiceAccounts%20in%20Kubernetes/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If a Pod can't access the API, check:</li> <li>Does the ServiceAccount exist?</li> <li>Are the RBAC permissions correct?</li> <li>Is the token properly mounted?</li> <li>Use <code>kubectl auth can-i</code> commands to verify permissions</li> </ul>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/","title":"Headless Services in Kubernetes: A Comprehensive Guide","text":""},{"location":"Services/Headless%20Services%20in%20Kubernetes/#what-is-a-headless-service","title":"What is a Headless Service?","text":"<p>A headless service is a Kubernetes service that ==doesn't provide load balancing or a single IP address==. Instead, it returns the IP addresses of all pods matching the selector, allowing direct connections to individual pods.</p> <p><code>Note</code>: ==Not necessarily. While StatefulSets and Headless Services are commonly used together, they aren't strictly dependent on each other. Let me clarify when and why you might use a Headless Service with different types of workloads.==</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#why-use-a-headless-service","title":"Why Use a Headless Service?","text":"<p>You'd use a headless service when: - You need direct pod-to-pod communication - Your application handles load balancing itself - You're deploying stateful applications where each pod has unique network identity - You need DNS records for all pods (e.g., for databases with replication)</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#when-to-use-a-headless-service","title":"When to Use a Headless Service?","text":"<p>Common use cases: - StatefulSets (like databases: MongoDB, PostgreSQL, etc.) - Peer-to-peer applications - When pods need to discover and communicate with each other directly - Applications that require stable network identities</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#how-to-use-a-headless-service-with-example","title":"How to Use a Headless Service (with Example)","text":""},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-mongodb-replica-set","title":"Example: MongoDB Replica Set","text":"<p>Here's how to create a headless service for a MongoDB replica set:</p> <ol> <li>Create the Headless Service:</li> </ol> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\nspec:\n  clusterIP: None  # This makes it headless\n  ports:\n  - port: 27017\n    name: mongodb\n  selector:\n    app: mongodb\n</code></pre> <ol> <li>Create the StatefulSet:</li> </ol> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongodb\nspec:\n  serviceName: mongodb  # Matches the headless service name\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:4.4\n        ports:\n        - containerPort: 27017\n        command: [\"mongod\", \"--replSet\", \"rs0\", \"--bind_ip_all\"]\n</code></pre>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#how-it-works","title":"How It Works","text":"<ol> <li>When you create this setup:</li> <li>Kubernetes creates 3 MongoDB pods with names: mongodb-0, mongodb-1, mongodb-2</li> <li> <p>The headless service creates DNS entries for each pod:</p> <ul> <li><code>mongodb-0.mongodb.default.svc.cluster.local</code></li> <li><code>mongodb-1.mongodb.default.svc.cluster.local</code></li> <li><code>mongodb-2.mongodb.default.svc.cluster.local</code></li> </ul> </li> <li> <p>Inside the MongoDB configuration, you can initialize the replica set using these DNS names because they're stable (won't change even if pods restart).</p> </li> <li> <p>Applications can:</p> </li> <li>Connect to individual pods directly using their DNS names</li> <li>Discover all pods by querying the DNS (which returns all pod IPs)</li> </ol>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#verifying-the-dns","title":"Verifying the DNS","text":"<p>You can check the DNS records from another pod:</p> <pre><code># Run this in another pod in the same namespace\nnslookup mongodb\n</code></pre> <p>This will return all pod IPs rather than a single service IP.</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#key-differences-from-regular-services","title":"Key Differences from Regular Services","text":"Feature Regular Service Headless Service ClusterIP Has an IP None DNS Resolution Returns service IP Returns all pod IPs Load Balancing Yes No Use Case Stateless applications Stateful applications <p>Headless services give you more control over networking when you need direct access to pods rather than going through a service proxy.</p> <p>==Not necessarily. While StatefulSets and Headless Services are commonly used together, they aren't strictly dependent on each other. Let me clarify when and why you might use a Headless Service with different types of workloads.==</p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#when-to-use-headless-services-with-statefulsets-vs-deployments","title":"When to Use Headless Services with StatefulSets vs Deployments","text":"Scenario Recommended Workload Why? Stateful applications (e.g., databases like MongoDB, Cassandra, PostgreSQL) StatefulSet + Headless Service StatefulSets provide stable pod names (<code>pod-0</code>, <code>pod-1</code>, etc.) and persistent storage. Headless Services allow direct pod-to-pod communication with stable DNS (<code>pod-0.svc.cluster.local</code>). Peer-to-peer or clustered apps (e.g., Kafka, Elasticsearch, RabbitMQ clusters) StatefulSet + Headless Service Each pod needs to discover others via DNS. StatefulSets ensure ordered scaling and stable identities. Direct pod access without load balancing (e.g., custom service discovery, gRPC with client-side LB) Deployment + Headless Service If you don\u2019t need stable pod identities but still want DNS records for all pods, a Deployment with a Headless Service works. Stateless apps needing pod DNS (e.g., monitoring agents that scrape all pods) Deployment + Headless Service A Headless Service lets you get all pod IPs via DNS, even if the app itself is stateless."},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-1-headless-service-with-statefulset-mongodb","title":"Example 1: Headless Service with StatefulSet (MongoDB)","text":"<p>This is the most common pairing: <pre><code># Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\nspec:\n  clusterIP: None  # Headless\n  ports:\n  - port: 27017\n  selector:\n    app: mongodb\n---\n# StatefulSet\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongodb\nspec:\n  serviceName: mongodb  # Matches the Headless Service\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo\n        command: [\"mongod\", \"--replSet\", \"rs0\"]\n</code></pre> Why? - Each MongoDB replica (<code>mongodb-0</code>, <code>mongodb-1</code>, etc.) gets a stable DNS name (<code>mongodb-0.mongodb.ns.svc.cluster.local</code>). - The replica set can be initialized using these DNS names.  </p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#example-2-headless-service-with-deployment-stateless-grpc-service","title":"Example 2: Headless Service with Deployment (Stateless gRPC Service)","text":"<p>You can use a Headless Service even with a Deployment if you need direct pod access: <pre><code># Headless Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: grpc-service\nspec:\n  clusterIP: None  # Headless\n  ports:\n  - port: 50051\n  selector:\n    app: grpc-server\n---\n# Deployment (stateless)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grpc-server\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: grpc-server\n    spec:\n      containers:\n      - name: grpc-server\n        image: my-grpc-server\n</code></pre> Why? - Clients can discover all pods via DNS (<code>grpc-service.ns.svc.cluster.local</code> returns all pod IPs). - Useful for client-side load balancing (e.g., gRPC uses round-robin DNS).  </p>"},{"location":"Services/Headless%20Services%20in%20Kubernetes/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>StatefulSets + Headless Services are best for:</li> <li>Stateful apps (databases, queues).</li> <li>Pods that need stable DNS names (<code>pod-0.svc</code>, <code>pod-1.svc</code>).  </li> <li> <p>Ordered scaling (e.g., <code>pod-0</code> must start before <code>pod-1</code>).  </p> </li> <li> <p>Deployments + Headless Services work when:</p> </li> <li>You don\u2019t need stable pod identities but still want direct pod DNS resolution.  </li> <li> <p>Your app handles discovery/load balancing itself (e.g., gRPC, custom service mesh).  </p> </li> <li> <p>Avoid Headless Services if:</p> </li> <li>You just need a simple load-balanced service (use a normal <code>ClusterIP</code> instead).  </li> <li>Your app doesn\u2019t care about individual pod IPs.  </li> </ol> <p>Would you like a deeper dive into any specific scenario?</p>"},{"location":"Services/Types%20of%20Kubernetes%20Services/","title":"Types of Kubernetes Services","text":"<p>Kubernetes offers several service types to expose your applications in different ways. Here's a comprehensive breakdown:</p>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#1-clusterip-default-service-type","title":"1. ClusterIP (Default Service Type)","text":"<ul> <li>Purpose: Exposes the service on an internal IP within the cluster</li> <li>Use Case: Communication between microservices within the cluster</li> <li>Characteristics:</li> <li>Only accessible from inside the cluster</li> <li>Provides load balancing across pods</li> <li>Default service type if none specified</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-internal-service\nspec:\n  type: ClusterIP\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#2-nodeport","title":"2. NodePort","text":"<ul> <li>Purpose: Exposes the service on each Node's IP at a static port</li> <li>Use Case: When you need external access to your service</li> <li>Characteristics:</li> <li>Makes service accessible from outside the cluster</li> <li>Opens a specific port on all nodes</li> <li>Automatically creates a ClusterIP service to route to it</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 9376\n      nodePort: 30007  # Optional (default range: 30000-32767)\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#3-loadbalancer","title":"3. LoadBalancer","text":"<ul> <li>Purpose: Exposes the service externally using a cloud provider's load balancer</li> <li>Use Case: Production applications needing external access</li> <li>Characteristics:</li> <li>Creates an external load balancer</li> <li>Automatically creates NodePort and ClusterIP services</li> <li>Cloud provider specific (AWS ALB, GCP LB, Azure LB)</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#4-externalname","title":"4. ExternalName","text":"<ul> <li>Purpose: Maps a service to a DNS name</li> <li>Use Case: Accessing external services from within the cluster</li> <li>Characteristics:</li> <li>No selectors or endpoints</li> <li>Returns CNAME record of external service</li> <li>Useful for services outside Kubernetes</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#5-headless-service-special-case","title":"5. Headless Service (Special Case)","text":"<ul> <li>Purpose: For direct pod-to-pod communication without load balancing</li> <li>Use Case: Stateful applications, databases, peer discovery</li> <li>Characteristics:</li> <li>clusterIP: None</li> <li>Returns DNS A records for all matching pods</li> <li>No single service IP/proxy</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-headless-service\nspec:\n  clusterIP: None\n  selector:\n    app: my-stateful-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#comparison-table","title":"Comparison Table","text":"Type Cluster IP External Access Load Balancing DNS Resolution Use Case ClusterIP Yes No Yes Single IP Internal services NodePort Yes Via Node IP Yes Single IP Development/testing LoadBalancer Yes Yes Yes Single IP Production external access ExternalName No N/A No CNAME External services Headless None Depends No All Pod IPs Stateful apps, direct pod access"},{"location":"Services/Types%20of%20Kubernetes%20Services/#advanced-service-types-cloud-specific","title":"Advanced Service Types (Cloud-Specific)","text":"<p>Some cloud providers offer additional service types:</p> <ol> <li>Ingress: Not technically a service type but works with services to provide HTTP routing</li> <li>External IPs: Additional external IPs for services</li> <li>AWS NLB: AWS-specific Network Load Balancer</li> <li>GCP Internal LB: For internal load balancing in GCP</li> </ol>"},{"location":"Services/Types%20of%20Kubernetes%20Services/#choosing-the-right-service-type","title":"Choosing the Right Service Type","text":"<ol> <li>Internal cluster communication: ClusterIP</li> <li>Development/testing external access: NodePort</li> <li>Production external access: LoadBalancer</li> <li>Stateful applications: Headless Service</li> <li>External services: ExternalName</li> </ol> <p>Would you like me to elaborate on any specific service type or provide more detailed examples?</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/","title":"Updating a MongoDB StatefulSet in Kubernetes","text":"<p>Updating a MongoDB StatefulSet in Kubernetes requires careful handling since StatefulSets manage stateful applications with persistent data. Below is a step-by-step guide on how to safely apply changes (e.g., configuration, image version, or resource limits) without causing downtime or data corruption.</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#steps-to-update-a-mongodb-statefulset","title":"\ud83d\udd27 Steps to Update a MongoDB StatefulSet","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#1-identify-the-change-needed","title":"1\ufe0f\u20e3 Identify the Change Needed","text":"<p>Possible changes: - Configuration (e.g., <code>mongod.conf</code> settings) - Docker Image Version (e.g., <code>mongo:4.4</code> \u2192 <code>mongo:5.0</code>) - Resource Limits (CPU/Memory) - Replica Count (Scaling up/down)</p>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#2-update-strategy-options","title":"2\ufe0f\u20e3 Update Strategy Options","text":"<p>StatefulSets support two update strategies:</p> Strategy Behavior Best For RollingUpdate (Default) Updates Pods one by one (in reverse order: <code>mongo-2</code> \u2192 <code>mongo-1</code> \u2192 <code>mongo-0</code>) Minor changes (e.g., configs, non-breaking image updates) OnDelete Requires manual deletion of Pods for updates Major changes (e.g., MongoDB major version upgrade)"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#3-example-updating-mongodb-configuration","title":"3\ufe0f\u20e3 Example: Updating MongoDB Configuration","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#step-1-modify-the-statefulset-yaml","title":"Step 1: Modify the StatefulSet YAML","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  updateStrategy:\n    type: RollingUpdate  # Default (Pod-by-Pod replacement)\n  template:\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:5.0  # Updated image\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          value: admin\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          value: password\n        volumeMounts:\n        - name: mongo-config\n          mountPath: /etc/mongod.conf  # Custom config file\n  volumeClaimTemplates: [...]\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#step-2-apply-the-changes","title":"Step 2: Apply the Changes","text":"<pre><code>kubectl apply -f mongo-statefulset.yaml\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#what-happens","title":"What Happens?","text":"<ul> <li>Kubernetes replaces Pods one by one (starting from the highest ordinal, <code>mongo-2</code> \u2192 <code>mongo-1</code> \u2192 <code>mongo-0</code>).</li> <li>Each Pod retains its PVC (data is preserved).</li> <li>The MongoDB replica set auto-reconfigures (if properly set up).</li> </ul>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#4-example-major-version-upgrade-mongodb-44-50","title":"4\ufe0f\u20e3 Example: Major Version Upgrade (MongoDB 4.4 \u2192 5.0)","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#warning-requires-manual-intervention","title":"\u26a0\ufe0f Warning: Requires Manual Intervention","text":"<ol> <li>Set <code>updateStrategy: OnDelete</code> <pre><code>spec:\n  updateStrategy:\n    type: OnDelete  # Prevents automatic updates\n</code></pre></li> <li>Apply the new YAML <pre><code>kubectl apply -f mongo-statefulset.yaml\n</code></pre></li> <li>Manually Delete Pods (One at a Time) <pre><code>kubectl delete pod mongo-2  # Starts with the highest ordinal\n</code></pre></li> <li>Wait for <code>mongo-2</code> to rejoin the replica set before deleting the next Pod.</li> </ol>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#5-verifying-the-update","title":"5\ufe0f\u20e3 Verifying the Update","text":""},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-pod-status","title":"Check Pod Status","text":"<pre><code>kubectl get pods -l app=mongo\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-mongodb-version","title":"Check MongoDB Version","text":"<pre><code>kubectl exec mongo-0 -- mongosh --eval \"db.version()\"\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#check-replica-set-health","title":"Check Replica Set Health","text":"<pre><code>kubectl exec mongo-0 -- mongosh --eval \"rs.status()\"\n</code></pre>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#best-practices-for-updating-statefulsets","title":"\u2705 Best Practices for Updating StatefulSets","text":"<ol> <li>Backup Data Before Major Changes <pre><code>kubectl exec mongo-0 -- mongodump --out /backup\n</code></pre></li> <li>Test Changes in Staging First </li> <li>Use <code>RollingUpdate</code> for Minor Changes </li> <li>Use <code>OnDelete</code> for Major Upgrades </li> <li>Monitor Sync Status After Updates </li> </ol>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<ul> <li>\u26a0\ufe0f Breaking Replication: Changing auth credentials without updating all Pods can break sync.</li> <li>\u26a0\ufe0f Downtime Risk: If <code>mongo-0</code> (primary) restarts, a new leader election happens.</li> <li>\u26a0\ufe0f Storage Issues: Changing <code>volumeClaimTemplates</code> does not modify existing PVCs.</li> </ul>"},{"location":"Statefulsets/Updating%20a%20MongoDB%20StatefulSet%20in%20Kubernetes/#final-answer","title":"Final Answer","text":"<ul> <li>For minor changes (configs, patches): Use <code>RollingUpdate</code>.  </li> <li>For major upgrades (MongoDB 4 \u2192 5): Use <code>OnDelete</code> + manual Pod deletion.  </li> <li>Always verify replica set health after updates.  </li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/","title":"W3H Understanding StatefulSets in Kubernetes","text":""},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#what-are-statefulsets","title":"What are StatefulSets?","text":"<p>StatefulSets are a Kubernetes workload API object used to manage stateful applications. They provide guarantees about the ordering and uniqueness of Pods, which is essential for applications that require:</p> <ul> <li>Stable, unique network identifiers</li> <li>Stable, persistent storage</li> <li>Ordered, graceful deployment and scaling</li> <li>Ordered, automated rolling updates</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#why-use-statefulsets","title":"Why Use StatefulSets?","text":"<p>You'd use StatefulSets when you need to run stateful applications in Kubernetes that require:</p> <ol> <li>Stable identities: Each Pod gets a persistent hostname that stays the same even after rescheduling</li> <li>Persistent storage: Volumes are created for each Pod and persist across rescheduling</li> <li>Ordered operations: Pods are created, updated, and deleted in a predictable order (ordinal index)</li> </ol>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#when-to-use-statefulsets","title":"When to Use StatefulSets?","text":"<p>Use StatefulSets for applications like:</p> <ul> <li>Databases (MongoDB, PostgreSQL, MySQL clusters)</li> <li>Message queues (Kafka, RabbitMQ)</li> <li>Any application that requires stable network identity or persistent storage</li> <li>Distributed systems that require ordered deployment/scaling</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#how-statefulsets-work-key-concepts","title":"How StatefulSets Work: Key Concepts","text":""},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#1-pod-identity","title":"1. Pod Identity","text":"<ul> <li>Each Pod gets a stable name: <code>&lt;statefulset-name&gt;-&lt;ordinal-index&gt;</code></li> <li>Example: <code>web-0</code>, <code>web-1</code>, <code>web-2</code> for a StatefulSet named \"web\"</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#2-persistent-storage","title":"2. Persistent Storage","text":"<ul> <li>Uses PersistentVolumeClaims (PVCs) that follow the Pod</li> <li>Each Pod gets its own PVC(s) bound to its ordinal index</li> <li>Storage persists even if the Pod is rescheduled</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#3-ordered-operations","title":"3. Ordered Operations","text":"<ul> <li>Scaling up: Pods are created sequentially (0, 1, 2...)</li> <li>Scaling down: Pods are terminated in reverse order (...2, 1, 0)</li> <li>Updates follow the same ordering</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#how-to-create-a-statefulset","title":"How to Create a StatefulSet","text":"<p>Here's a basic example for a MongoDB StatefulSet:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  serviceName: \"mongo\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4.2\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-persistent-storage\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-persistent-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#key-components","title":"Key Components:","text":"<ul> <li><code>serviceName</code>: Headless Service that controls the network domain</li> <li><code>volumeClaimTemplates</code>: Defines PVCs that will be created for each Pod</li> <li>Ordered creation of Pods with stable names</li> </ul>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#statefulset-vs-deployment","title":"StatefulSet vs Deployment","text":"Feature Deployment StatefulSet Pod names Random hash Ordered (web-0, web-1) Storage Shared volumes Dedicated per Pod Scaling Any order Ordered Network identity Not stable Stable DNS name Use cases Stateless apps Stateful apps"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#best-practices","title":"Best Practices","text":"<ol> <li>Always create a headless Service for your StatefulSet</li> <li>Design your application to handle the ordered operations</li> <li>Consider using Pod Management Policies (<code>parallel</code> or <code>ordered</code>) based on your needs</li> <li>Implement proper backup strategies for your persistent data</li> <li>Use readiness probes to ensure proper startup ordering</li> </ol> <p>StatefulSets bring the benefits of Kubernetes to stateful applications while providing the stability and predictability these applications require.</p>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#1-statefulsets-vs-deployments","title":"1. StatefulSets vs Deployments","text":"Feature Deployment (Stateless) StatefulSet (Stateful) Pod Naming Random (<code>web-7dfd6c8f5</code>) Ordered (<code>web-0</code>, <code>web-1</code>) Storage Shared volumes (if any) Dedicated PVC per Pod Scaling Order Parallel (no order) Sequential (0 \u2192 1 \u2192 2) Deletion Order Random Reverse order (2 \u2192 1 \u2192 0) Network Identity Unstable (changes on restart) Stable DNS (<code>web-0.mongo.default.svc.cluster.local</code>) Use Cases Stateless apps (frontend, APIs) Databases (MongoDB, MySQL), Message Queues (Kafka) Service Required Regular ClusterIP/LoadBalancer Headless Service (for stable DNS)"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#2-when-to-use-statefulsets","title":"2. When to Use StatefulSets?","text":"Scenario Reason Databases (MySQL, MongoDB, PostgreSQL) Needs persistent storage &amp; stable hostnames Message Brokers (Kafka, RabbitMQ) Requires ordered scaling &amp; stable identities Distributed Systems with Leader Election Pods need to recognize each other via DNS Applications with Persistent Data Data must survive Pod restarts"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#3-key-features-of-statefulsets","title":"3. Key Features of StatefulSets","text":"Feature Description Stable Pod Names Pods are named <code>&lt;statefulset&gt;-&lt;ordinal&gt;</code> (e.g., <code>mongo-0</code>, <code>mongo-1</code>) Persistent Volumes (PVCs) Each Pod gets its own storage (<code>volumeClaimTemplates</code>) Ordered Operations Pods start/stop in sequence (<code>0 \u2192 1 \u2192 2</code>) Stable Network IDs Each Pod gets a DNS record (<code>&lt;pod&gt;.&lt;svc&gt;.&lt;namespace&gt;.svc.cluster.local</code>) Graceful Scaling Scaling down removes the highest ordinal first"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#4-example-statefulset-yaml-breakdown","title":"4. Example StatefulSet YAML Breakdown","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  serviceName: \"mongo\"  # Requires a Headless Service\n  replicas: 3           # Creates mongo-0, mongo-1, mongo-2\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4.2\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-storage\n          mountPath: /data/db\n  volumeClaimTemplates:  # Creates PVCs for each Pod\n  - metadata:\n      name: mongo-storage\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Statefulsets/W3H-Understanding%20StatefulSets%20in%20Kubernetes/#summary-table-statefulset-key-points","title":"Summary Table: StatefulSet Key Points","text":"Aspect StatefulSet Behavior Pod Identity Fixed hostnames (<code>&lt;name&gt;-0</code>, <code>&lt;name&gt;-1</code>) Storage Each Pod gets its own PVC Scaling Up Sequential (<code>0 \u2192 1 \u2192 2</code>) Scaling Down Reverse order (<code>2 \u2192 1 \u2192 0</code>) Updates Ordered rolling updates DNS Records Stable (<code>&lt;pod&gt;.&lt;service&gt;.ns.svc.cluster.local</code>) <p>Would you like a deeper dive into any specific part? \ud83d\ude80</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/","title":"Understanding Tolerations in Kubernetes","text":""},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#what-are-tolerations","title":"What are Tolerations?","text":"<p>Tolerations are Kubernetes mechanisms that allow pods to be scheduled onto nodes with matching taints. They work in conjunction with taints to enable advanced scheduling scenarios where certain nodes are \"reserved\" for specific workloads.</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#why-use-tolerations","title":"Why Use Tolerations?","text":"<p>Tolerations are used when you need to: - Schedule pods on specialized nodes (like GPU nodes) - Allow critical workloads to run on reserved nodes - Handle node maintenance gracefully - Implement multi-tenant clusters with dedicated resources</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#when-to-use-tolerations","title":"When to Use Tolerations?","text":"<p>Common use cases: - Nodes with specialized hardware - Nodes reserved for specific teams/applications - Nodes that might be problematic for some workloads (e.g., spot instances) - Nodes undergoing maintenance</p>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#how-to-use-tolerations","title":"How to Use Tolerations?","text":""},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#basic-example-gpu-workload","title":"Basic Example: GPU Workload","text":"<ol> <li> <p>Taint the GPU node: <pre><code>kubectl taint nodes gpu-node-1 hardware=gpu:NoSchedule\n</code></pre></p> </li> <li> <p>Create a pod with matching toleration:</p> </li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  containers:\n  - name: cuda-container\n    image: nvidia/cuda:11.0-base\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n  tolerations:\n  - key: \"hardware\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#advanced-example-spot-instance-with-prefernoschedule","title":"Advanced Example: Spot Instance with PreferNoSchedule","text":"<ol> <li> <p>Taint the spot instance nodes: <pre><code>kubectl taint nodes spot-node-1 instance-type=spot:PreferNoSchedule\n</code></pre></p> </li> <li> <p>Pod with toleration and node selector:</p> </li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cost-optimized-app\nspec:\n  containers:\n  - name: app\n    image: my-app:latest\n  tolerations:\n  - key: \"instance-type\"\n    operator: \"Equal\"\n    value: \"spot\"\n    effect: \"PreferNoSchedule\"\n  nodeSelector:\n    instance-type: spot\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#maintenance-example-allow-pod-during-node-drain","title":"Maintenance Example: Allow pod during node drain","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-app\nspec:\n  containers:\n  - name: app\n    image: critical-app:latest\n  tolerations:\n  - key: \"node.kubernetes.io/unschedulable\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  - key: \"node.kubernetes.io/unreachable\"\n    operator: \"Exists\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 6000  # Keep running for 100 minutes if node becomes unreachable\n</code></pre>"},{"location":"Taint-Tolerations/Understanding%20Tolerations%20in%20Kubernetes/#key-points-to-remember","title":"Key Points to Remember","text":"<ol> <li>Tolerations don't guarantee pod scheduling - they just allow it if other conditions are met</li> <li>Use <code>operator: \"Exists\"</code> when you don't care about the value</li> <li><code>tolerationSeconds</code> controls how long a pod stays bound to an unreachable node</li> <li>Combine tolerations with nodeSelectors/affinity for precise scheduling</li> </ol> <p>Tolerations are powerful tools for advanced scheduling scenarios in Kubernetes, especially when working with heterogeneous node pools or specialized hardware.</p>"},{"location":"Taint-Tolerations/What%20is%20taint%20_/","title":"What is taint","text":"<p>In Kubernetes, a taint is like a \"warning label\" you put on a node to say:  </p> <p>\"Don\u2019t schedule pods here unless they explicitly allow it!\" </p>"},{"location":"Taint-Tolerations/What%20is%20taint%20_/#simple-meaning","title":"Simple Meaning:","text":"<p>A taint marks a node as restricted\u2014pods will avoid it unless they have a matching toleration (special permission to run there).  </p>"},{"location":"Taint-Tolerations/What%20is%20taint%20_/#real-world-example","title":"Real-World Example:","text":"<p>Imagine a \"Employees Only\" sign on a door: - Taint = The sign (\"Employees Only\") - Toleration = An employee badge (allows entry) - Pods without toleration = Regular people (can\u2019t enter)  </p>"},{"location":"Taint-Tolerations/What%20is%20taint%20_/#why-use-it","title":"Why Use It?","text":"<ul> <li>Reserve nodes for special workloads (e.g., GPU nodes).  </li> <li>Prevent random pods from scheduling on sensitive nodes.  </li> <li>Handle maintenance or unreliable nodes (e.g., spot instances).  </li> </ul>"},{"location":"Taint-Tolerations/What%20is%20taint%20_/#command-example","title":"Command Example:","text":"<p><pre><code>kubectl taint nodes my-node special=true:NoSchedule  \n</code></pre> This means: \"Only pods that tolerate <code>special=true</code> can run here.\" </p> <p>In short: Taints block pods, tolerations bypass the block. \ud83d\udea6</p>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/","title":"Securing Traefik Dashboard with Keycloak (OIDC/OAuth2)","text":"<p>Yes, you can definitely use Keycloak instead of basic auth to secure your Traefik dashboard. This is a more secure and modern approach using OIDC/OAuth2. Here's how to set it up:</p>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running Keycloak instance</li> <li>Traefik deployed (preferably via Helm)</li> </ul>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-1-configure-keycloak","title":"Step 1: Configure Keycloak","text":"<ol> <li>Create a new client in Keycloak:</li> <li>Client ID: <code>traefik-dashboard</code></li> <li>Client Protocol: <code>openid-connect</code></li> <li>Root URL: <code>https://traefik.yourdomain.com</code></li> <li>Valid Redirect URIs: <code>https://traefik.yourdomain.com/oauth2/callback</code></li> <li>Web Origins: <code>+</code></li> <li>Access Type: <code>confidential</code></li> <li> <p>Standard Flow Enabled: ON</p> </li> <li> <p>Create roles (e.g., <code>traefik-admin</code>) and assign them to users</p> </li> <li> <p>Note down these values from the client's \"Credentials\" tab:</p> </li> <li>Client ID</li> <li>Client Secret</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-2-configure-traefik-with-keycloak-integration","title":"Step 2: Configure Traefik with Keycloak Integration","text":""},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#using-helm-values","title":"Using Helm Values","text":"<p>Update your <code>values.yaml</code>:</p> <pre><code>additionalArguments:\n  - --api.dashboard=true\n  - --providers.kubernetesingress\n  - --providers.kubernetescrd\n  - --entrypoints.web.address=:80\n  - --entrypoints.websecure.address=:443\n  - --certificatesresolvers.default.acme.tlschallenge=true\n  - --certificatesresolvers.default.acme.email=your@email.com\n  - --certificatesresolvers.default.acme.storage=/data/acme.json\n\ningressRoute:\n  dashboard:\n    enabled: true\n    matchRule: Host(`traefik.yourdomain.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\n    entryPoints: [\"websecure\"]\n    middlewares:\n      - name: keycloak-auth\n    tls:\n      certResolver: default\n\nmiddlewares:\n  keycloak-auth:\n    enabled: true\n    forwardAuth:\n      address: \"https://keycloak.yourdomain.com/auth/realms/your-realm/protocol/openid-connect/auth\"\n      trustForwardHeader: true\n      authResponseHeaders:\n        - \"X-Forwarded-User\"\n        - \"Authorization\"\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#create-forwardauth-middleware-alternative","title":"Create ForwardAuth Middleware (Alternative)","text":"<p>For more control, create a separate ForwardAuth middleware:</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: keycloak-oauth\nspec:\n  forwardAuth:\n    address: \"https://keycloak.yourdomain.com/auth/realms/your-realm/protocol/openid-connect/auth\"\n    authResponseHeaders:\n      - \"X-Forwarded-User\"\n      - \"Authorization\"\n    tls:\n      insecureSkipVerify: false # Set to true if using self-signed certs\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-3-configure-keycloak-authentication-flow","title":"Step 3: Configure Keycloak Authentication Flow","text":"<ol> <li>In Keycloak, go to your client's settings</li> <li>Add these valid redirect URIs:</li> <li><code>https://traefik.yourdomain.com/oauth2/callback</code></li> <li><code>https://traefik.yourdomain.com/dashboard/*</code></li> <li>Configure roles/scopes as needed</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#step-4-deploy-the-configuration","title":"Step 4: Deploy the Configuration","text":"<pre><code>helm upgrade --install traefik traefik/traefik -f values.yaml -n traefik\n</code></pre>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#verification","title":"Verification","text":"<ol> <li>Access <code>https://traefik.yourdomain.com/dashboard/</code></li> <li>You should be redirected to Keycloak for authentication</li> <li>After successful login, you'll be redirected back to the Traefik dashboard</li> </ol>"},{"location":"Traefik/Securing%20Traefik%20Dashboard%20with%20Keycloak%20%28OIDC%20or%20OAuth2%29/#additional-security-recommendations","title":"Additional Security Recommendations","text":"<ol> <li> <p>Add role-based access:    <pre><code># In your middleware configuration\nlabels:\n  traefik.forwardauth.authResponseHeadersRegex: \"^X-Forwarded-User$\"\n  traefik.forwardauth.authResponseHeaders: \"X-Forwarded-User, Authorization\"\n</code></pre></p> </li> <li> <p>Enable HTTPS everywhere:    <pre><code>additionalArguments:\n  - --entrypoints.web.http.redirections.entryPoint.to=websecure\n  - --entrypoints.web.http.redirections.entryPoint.scheme=https\n</code></pre></p> </li> <li> <p>Configure session timeout in Keycloak client settings</p> </li> </ol> <p>This setup provides much better security than basic auth, with features like: - Single Sign-On (SSO) - Multi-factor authentication - Session management - Fine-grained access control - Token expiration</p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/","title":"Traefik Dashboard only for Visibility not for Administration","text":""},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#traefik-dashboard-visibility-vs-administration","title":"Traefik Dashboard: Visibility vs. Administration","text":"<p>Yes, by default, the Traefik Dashboard is primarily for visibility and monitoring\u2014it displays routing configurations, services, middlewares, and HTTP traffic metrics. However, it does not allow direct administration (e.g., modifying routes, adding TLS, or changing load-balancing rules) unless explicitly configured for write access (which is not recommended for security reasons).</p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#key-points-about-traefik-dashboard","title":"Key Points About Traefik Dashboard","text":"Aspect Details Default Mode Read-only (just visualization) Admin Access Disabled by default (must explicitly enable API write access) Security Risk Enabling admin control exposes Traefik to potential misconfigurations or attacks Alternative Use Kubernetes <code>IngressRoute</code>/<code>Ingress</code> CRDs or Traefik's CLI/API for changes"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#how-to-expose-the-dashboard-safely","title":"How to Expose the Dashboard (Safely)","text":""},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#1-read-only-dashboard-recommended","title":"1. Read-Only Dashboard (Recommended)","text":"<pre><code># traefik-dashboard.yaml (IngressRoute for Kubernetes)\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`traefik.example.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\n      kind: Rule\n      services:\n        - name: api@internal  # Built-in Traefik service\n          kind: TraefikService\n</code></pre>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#2-enable-api-for-admin-access-use-with-caution","title":"2. Enable API (For Admin Access - Use with Caution!)","text":"<pre><code># traefik-config.yaml (Dangerous!)\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-auth\nspec:\n  basicAuth:\n    secret: traefik-auth-secret  # Requires a Kubernetes Secret\n\n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard-admin\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`traefik-admin.example.com`) &amp;&amp; PathPrefix(`/api`)\n      kind: Rule\n      middlewares:\n        - name: traefik-auth\n      services:\n        - name: api@internal\n          kind: TraefikService\n  tls:\n    secretName: traefik-tls\n</code></pre> <p>\u26a0\ufe0f Warning: - The API allows full control (create/delete routes, TLS, middlewares). - Never expose <code>/api</code> without authentication (use <code>basicAuth</code> or IP whitelisting). - Prefer Kubernetes CRDs (<code>IngressRoute</code>) for safer, GitOps-friendly management.  </p>"},{"location":"Traefik/Traefik%20Dashboard%20only%20for%20Visibility%20not%20for%20Administration/#best-practices","title":"Best Practices","text":"<ol> <li>Keep it read-only (only expose <code>/dashboard</code>).  </li> <li>Use auth (BasicAuth, OAuth, or network policies).  </li> <li>Avoid exposing <code>/api</code> unless absolutely necessary.  </li> <li>Prefer CLI/kubectl for changes instead of the dashboard.  </li> </ol> <p>Conclusion: The dashboard is great for debugging and monitoring, but not for administration unless explicitly (and securely) configured. Always manage Traefik via Kubernetes manifests or API with strict access controls. \ud83d\udd12</p>"},{"location":"Ubuntu/list%20of%20good%20tools%20to%20extract%20rar%20part%20files/","title":"List of good tools to extract rar part files","text":"<p>Here's a list of good tools to extract <code>.part*.rar</code> files on Ubuntu:</p> Tool Command to Install Supports Multi-Part RAR Notes unrar <code>sudo apt install unrar</code> \u2705 Yes Official but non-free binary unar <code>sudo apt install unar</code> \u2705 Yes Free and easier syntax 7z (p7zip-full) <code>sudo apt install p7zip-full</code> \u2705 Yes Supports many formats, flexible rar <code>sudo apt install rar</code> \u2705 Yes Non-free, but official RAR tool <p>\ud83d\udd27 Recommended: Use <code>unar</code> for ease or <code>unrar</code> if you're okay with non-free software.</p> <p>Yes \u2014 just run <code>7z x filename.part1.rar</code>, and it will:</p> <ul> <li> <p>\u2705 Automatically detect and extract all parts (<code>filename.part2.rar</code>, <code>part3.rar</code>, etc.)</p> </li> <li> <p>\u2757 All parts must be in the same directory</p> </li> <li> <p>\ud83d\udeab Do not run it on <code>part2.rar</code> or later \u2014 only <code>part1.rar</code> contains the header</p> </li> </ul>"},{"location":"Ubuntu/list%20of%20good%20tools%20to%20extract%20rar%20part%20files/#example","title":"Example:","text":"<pre><code>7z x myarchive.part1.rar\n</code></pre> <p>Will extract the full content from:</p> <pre><code>myarchive.part1.rar\nmyarchive.part2.rar\nmyarchive.part3.rar\n...\n</code></pre> <p>Let me know if it throws any error.</p>"},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/","title":"Install etcdctl tool with supported versions","text":"<p>Learn etcd : \"https://etcd.io/docs/\" <pre><code>gouse@gouse:~$ k3d version\nk3d version v5.8.3\nk3s version v1.31.5-k3s1 (default)\n\ngouse@gouse:~$ kubectl version\nClient Version: v1.30.4+k3s1\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.31.5+k3s1\n</code></pre></p> <p>Since your k3s version is <code>v1.31.5-k3s1</code>, it uses etcd v3.5.x internally. \u2705 So, you should install <code>etcdctl v3.5.13</code>, which is fully compatible.</p>"},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/#install-etcdctl-v3513-on-host","title":"\ud83d\udce6 Install <code>etcdctl v3.5.13</code> on Host","text":"<pre><code>ETCD_VER=v3.5.13\n\nwget https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz\ntar xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz\nsudo mv etcd-${ETCD_VER}-linux-amd64/etcdctl /usr/local/bin/\nrm -rf etcd-${ETCD_VER}-linux-amd64*\n</code></pre> <p>Then confirm:</p> <pre><code>etcdctl version\n</code></pre>"},{"location":"etcdctl/Install%20etcdctl%20tool%20with%20supported%20versions/#etcd-compatibility-versions-list","title":"etcd compatibility versions list","text":"Kubernetes Version etcd Version Used (Bundled) v1.31.x v3.5.9+ v1.30.x v3.5.9 v1.29.x v3.5.9 v1.28.x v3.5.7 v1.27.x v3.5.6 v1.26.x v3.5.6 v1.25.x v3.5.4 v1.24.x v3.5.3 v1.23.x v3.5.0 v1.22.x v3.4.13 v1.21.x v3.4.13 v1.20.x v3.4.13"},{"location":"etcdctl/check%20etcd%20is%20running/","title":"Check etcd is running","text":"<p><pre><code>gouse@gouse:~$ etcdctl put foo \"bar\" {\"level\":\"warn\",\"ts\":\"2025-06-13T18:12:52.857946+0300\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.13/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0005a2000/127.0.0.1:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\\\"\"} Error: context deadline exceeded\n</code></pre> The error means <code>etcdctl</code> cannot connect to your etcd server at <code>127.0.0.1:2379</code>. The connection is refused because etcd is either not running or not listening on that address/port.</p> <p>Check these:</p> <pre><code># Check if etcd is running\nps aux | grep etcd\n\n# Or if using systemd\nsudo systemctl status etcd\n\n# Check if port 2379 is open\nss -tlnp | grep 2379\n</code></pre> <p>If etcd is not running, start it:</p> <pre><code>sudo systemctl start etcd\n</code></pre> <p>If etcd is running but on a different address/port, set the endpoint explicitly:</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=&lt;your-etcd-ip&gt;:&lt;port&gt; put foo \"bar\"\n</code></pre>"},{"location":"etcdctl/check%20etcd%20is%20running/#for-a-k3d-cluster","title":"For a k3d cluster","text":"<p>For a k3d cluster, etcd runs inside the k3s server container and is not directly accessible on <code>127.0.0.1:2379</code> by default.</p> <p>To interact with etcd in k3d/k3s, you usually:</p> <ol> <li> <p>Exec into the k3s server container where etcd runs.</p> </li> <li> <p>Use etcdctl inside that container with the correct environment variables.</p> </li> </ol> <p>Example commands:</p> <pre><code># List k3d containers\ndocker ps | grep k3d\n\n# Exec into the k3s server container (replace container name)\ndocker exec -it &lt;k3d-server-container-name&gt; sh\n\n# Inside container, set etcdctl env variables and run command\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/client.key\n\netcdctl put foo \"bar\"\n</code></pre> <p>If you want, I can give you the exact command to get your k3d server container name and run etcdctl inside it.</p> <p>If you really need etcdctl, consider:</p> <ul> <li> <p>Running an etcdctl container inside k3d with mounted certs.</p> </li> <li> <p>Or use the k3s builtin ==<code>3s etcd-snapshot</code>== commands if your version supports it.</p> </li> </ul>"},{"location":"etcdctl/etcdctl%20commands%20for%20get%2C%20put%2C%20snapshot%20save%20and%20restore/","title":"Etcdctl commands for get, put, snapshot save and restore","text":"<pre><code># Set a key\netcdctl put foo \"bar\"\n\n# Get a key\netcdctl get foo\n\n# Save a snapshot\netcdctl snapshot save /backup/etcd-snapshot.db\n\n# Restore a snapshot\netcdctl snapshot restore /backup/etcd-snapshot.db \\\n  --data-dir=/var/lib/etcd-restored\n</code></pre>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/","title":"Fluentd DaemonSet deployment YAML for Kubernetes","text":"<p>Here\u2019s a basic Fluentd DaemonSet deployment YAML for Kubernetes (output to stdout by default \u2014 you can change to Elasticsearch, Loki, etc.):</p> <p>Actually \u2014 \u2705 yes, we did deploy Fluentd in that YAML.</p>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#heres-how","title":"Here's how:","text":"<ul> <li> <p>ConfigMap: Defines Fluentd's configuration (<code>fluent.conf</code>)</p> </li> <li> <p>DaemonSet: Deploys Fluentd on every node</p> <ul> <li> <p>Pulls <code>fluent/fluentd:v1.14-1</code> image</p> </li> <li> <p>Mounts the logs from host (<code>/var/log/containers/</code>)</p> </li> <li> <p>Runs using the config from the ConfigMap</p> </li> </ul> </li> <li> <p>ServiceAccount: For access control (if needed)</p> </li> </ul> <p>\ud83d\udd04 <code>DaemonSet</code> ensures Fluentd runs on each node, reading logs and processing them as defined.</p> <p>You can verify deployment with:</p> <pre><code>kubectl get pods -n logging -l name=fluentd\n</code></pre> <p>Let me know if you want a Loki/Elasticsearch version or a Helm chart method.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: logging\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\n  namespace: logging\ndata:\n  fluent.conf: |\n    &lt;source&gt;\n      @type tail\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kube.*\n      format json\n    &lt;/source&gt;\n\n    &lt;match **&gt;\n      @type stdout\n    &lt;/match&gt;\n\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: logging\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      serviceAccountName: fluentd\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          operator: Exists\n          effect: NoSchedule\n      containers:\n        - name: fluentd\n          image: fluent/fluentd:v1.14-1\n          env:\n            - name: FLUENTD_CONF\n              value: fluent.conf\n          volumeMounts:\n            - name: varlog\n              mountPath: /var/log\n            - name: config-volume\n              mountPath: /fluentd/etc/fluent.conf\n              subPath: fluent.conf\n      volumes:\n        - name: varlog\n          hostPath:\n            path: /var/log\n        - name: config-volume\n          configMap:\n            name: fluentd-config\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: logging\n</code></pre> <p>We can change the output destination (e.g., Loki, Elasticsearch, S3, etc.), so modify the <code>ConfigMap</code> section accordingly.</p> <p>Fluentd itself does not have a built-in dashboard for logs or metrics.</p>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#common-practice","title":"Common practice:","text":"<ul> <li> <p>Fluentd collects and forwards logs to logging backends that do provide dashboards, such as:</p> <ul> <li> <p>Elasticsearch + Kibana (Kibana is the dashboard UI)</p> </li> <li> <p>Grafana Loki + Grafana (Grafana is the dashboard)</p> </li> <li> <p>Splunk, Datadog, Sumo Logic, etc.</p> </li> </ul> </li> </ul>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#fluentd-monitoring","title":"Fluentd monitoring:","text":"<ul> <li> <p>You can enable Fluentd metrics (via plugins) and export them to Prometheus.</p> </li> <li> <p>Then visualize Fluentd metrics in Grafana dashboards (for Fluentd health, throughput, errors).</p> </li> </ul>"},{"location":"fluentd/Fluentd%20DaemonSet%20deployment%20YAML%20for%20Kubernetes/#tldr","title":"TL;DR:","text":"Tool Role Dashboard Fluentd Log collector and forwarder No built-in UI Elasticsearch Log storage and search Kibana Loki Log storage Grafana Prometheus Metrics collection Grafana <p>If you want, you can set up Fluentd with Elasticsearch + Kibana or Loki + Grafana for dashboards.</p>"},{"location":"fluentd/Fluentd%20can%20run%20on%20standalone%20VMs/","title":"Fluentd can run on standalone VMs","text":"<p>Yes, Fluentd can run on standalone VMs (including those running Siebel or any apps) to collect logs and forward them.</p>"},{"location":"fluentd/Fluentd%20can%20run%20on%20standalone%20VMs/#how-it-works-on-standalone-vms","title":"How it works on standalone VMs:","text":"<ul> <li> <p>Install Fluentd agent on the VM.</p> </li> <li> <p>Configure Fluentd to tail application log files (e.g., Siebel logs).</p> </li> <li> <p>Fluentd forwards logs to your desired backend (Elasticsearch, Loki, remote syslog, etc.).</p> </li> </ul> Setup Fluentd Role Use Case Kubernetes Cluster DaemonSet log collector Collect container logs Standalone VM Fluentd agent daemon/service Collect app logs from files <p>Fluentd is very flexible and works well outside Kubernetes for log collection and forwarding.</p>"},{"location":"fluentd/Fluentd%20in%20Kubernetes/","title":"Fluentd in Kubernetes","text":"W3H Details What Fluentd is a log collector and forwarder that unifies logging across systems. Why To collect, process, filter, and send logs from containers to destinations like Elasticsearch, Loki, S3, etc. Where Runs as a DaemonSet on each Kubernetes node to access container logs via <code>/var/log/containers/</code> or CRI socket. How Deploy Fluentd as a DaemonSet + ConfigMap for parsing + output plugin for destination. Example: Fluentd \u2192 Elasticsearch or Fluentd \u2192 Loki."},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/","title":"Fluentd install and config steps for a Linux VM","text":"<p>Here are the steps to install and configure Fluentd on a standalone Linux VM to collect and forward logs (e.g., Siebel logs):</p>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#1-install-fluentd","title":"1. Install Fluentd","text":"<pre><code># Install td-agent (Fluentd stable distribution) on Ubuntu/Debian:\ncurl -fsSL https://packages.treasuredata.com/GPG-KEY-td-agent | sudo apt-key add -\necho \"deb https://packages.treasuredata.com/4/ubuntu/$(lsb_release -cs)/ $(lsb_release -cs) contrib\" | sudo tee /etc/apt/sources.list.d/treasure-data.list\nsudo apt-get update\nsudo apt-get install -y td-agent\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#2-configure-fluentd","title":"2. Configure Fluentd","text":"<p>Edit the main config file <code>/etc/td-agent/td-agent.conf</code>: Example to tail Siebel logs (adjust <code>/path/to/siebel/logs/*.log</code>):</p> <pre><code>&lt;source&gt;\n  @type tail\n  path /path/to/siebel/logs/*.log\n  pos_file /var/log/td-agent/siebel.log.pos\n  tag siebel.logs\n  format none\n&lt;/source&gt;\n\n&lt;match siebel.logs&gt;\n  @type stdout\n  # For forwarding to Elasticsearch, replace above with:\n  # @type elasticsearch\n  # host localhost\n  # port 9200\n&lt;/match&gt;\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#3-restart-fluentd","title":"3. Restart Fluentd","text":"<pre><code>sudo systemctl restart td-agent\nsudo systemctl enable td-agent\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#4-verify-fluentd-is-running","title":"4. Verify Fluentd is running","text":"<pre><code>sudo systemctl status td-agent\ntail -f /var/log/td-agent/td-agent.log\n</code></pre>"},{"location":"fluentd/Fluentd%20install%20and%20config%20steps%20for%20a%20Linux%20VM/#optional-forward-logs-to-elasticsearch-or-other-destinations","title":"Optional: Forward logs to Elasticsearch or other destinations","text":"<p>Replace the <code>&lt;match&gt;</code> block with appropriate output plugin config, for example:</p> <pre><code>&lt;match siebel.logs&gt;\n  @type elasticsearch\n  host your-elasticsearch-host\n  port 9200\n  logstash_format true\n  index_name siebel-logs\n&lt;/match&gt;\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/","title":"Steps to deploy MailHog with Ingress","text":"<p>Here are the full steps to deploy MailHog with Ingress via Traefik on a k3d cluster using port 80.</p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-1-create-k3d-cluster-with-port-80-mapped","title":"\u2705 Step 1: Create <code>k3d</code> cluster with port 80 mapped","text":"<pre><code># file: k3d-mailhog.yaml\napiVersion: k3d.io/v1alpha4\nkind: Simple\nname: mailhog-cluster\nports:\n  - port: 80:80      # maps host port 80 to container port 80\n    nodeFilters:\n      - loadbalancer\n</code></pre> <pre><code>k3d cluster create --config k3d-mailhog.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-2-deploy-traefik-if-not-already","title":"\u2705 Step 2: Deploy Traefik (if not already)","text":"<p>Traefik is installed by default with <code>k3d</code>. Check with:</p> <pre><code>kubectl get pods -n kube-system | grep traefik\n</code></pre> <p>If not, install manually via Helm or manifest.</p>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-3-create-mailhog-deployment-and-service","title":"\u2705 Step 3: Create MailHog Deployment and Service","text":"<pre><code># file: mailhog-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mailhog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mailhog\n  template:\n    metadata:\n      labels:\n        app: mailhog\n    spec:\n      containers:\n      - name: mailhog\n        image: mailhog/mailhog\n        ports:\n        - containerPort: 1025\n        - containerPort: 8025 \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mailhog\nspec:\n  selector:\n    app: mailhog\n  ports:\n  - name: smtp\n    port: 1025\n    targetPort: 1025\n  - name: http\n    port: 8025  # i used 80, becuase i used 80 in k3d config\n    targetPort: 8025\n</code></pre> <pre><code>kubectl apply -f mailhog-deployment.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-4-create-ingress-for-traefik","title":"\u2705 Step 4: Create Ingress for Traefik","text":"<pre><code># file: mailhog-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mailhog\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  rules:\n  - host: mailhog.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: mailhog\n            port:\n              number: 8025  # i used 80, becuase i used 80 in k3d config\n</code></pre> <pre><code>kubectl apply -f mailhog-ingress.yaml\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-5-add-local-dns-for-mailhoglocal","title":"\u2705 Step 5: Add local DNS for <code>mailhog.local</code>","text":"<p>Edit <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 mailhog.local\n</code></pre>"},{"location":"mailhog/Steps%20to%20deploy%20MailHog%20with%20Ingress/#step-6-access-mailhog","title":"\u2705 Step 6: Access MailHog","text":"<p>Open in browser: <code>http://mailhog.local</code> \u2192 Web UI SMTP will be available at <code>mailhog.local:1025</code> (for apps)</p> <p>Let me know if you want the same with TLS (HTTPS) using Traefik + cert-manager.</p>"},{"location":"mkdocs/mkdocs%20to%20github%20projects/","title":"Mkdocs to github projects","text":"<pre><code>$ python3 -m venv handsondevops\n$ source ./handsondevops/bin/activate\n$ pip install mkdocs\n</code></pre>"},{"location":"python/mkdocs%20to%20github%20pages/","title":"Mkdocs to github pages","text":""},{"location":"python/mkdocs%20to%20github%20pages/#solution-initialize-a-git-repository","title":"Solution: Initialize a Git Repository","text":"<ol> <li>Check if you\u2019re in a Git repo: <pre><code>git status\n</code></pre></li> <li> <p>If you see <code>fatal: not a git repository...</code>, proceed to initialize Git.</p> </li> <li> <p>Initialize a new Git repository: <pre><code>git init\n</code></pre></p> </li> <li> <p>Connect to your GitHub repo (if not already linked): <pre><code>git remote add origin https://github.com/&lt;username&gt;/&lt;repo-name&gt;.git\n</code></pre></p> </li> <li> <p>Replace <code>&lt;username&gt;</code> and <code>&lt;repo-name&gt;</code> with your GitHub details.</p> </li> <li> <p>Stage and commit your MkDocs files: <pre><code>git add .\ngit commit -m \"Initial commit\"\n</code></pre></p> </li> <li> <p>Push to GitHub (if needed): <pre><code>git push -u origin main  # or `master` for older repos\n</code></pre></p> </li> <li> <p>Now try <code>mkdocs gh-deploy</code> again: <pre><code>mkdocs gh-deploy\n</code></pre></p> </li> <li>This should now work since Git is properly initialized.</li> </ol>"},{"location":"python/mkdocs%20to%20github%20pages/#alternative-fix-run-gh-deploy-from-the-correct-directory","title":"Alternative Fix: Run <code>gh-deploy</code> from the Correct Directory","text":"<ul> <li>Ensure you\u2019re running the command in the root of your MkDocs project (where <code>mkdocs.yml</code> is located).</li> <li>If you cloned a repo, <code>cd</code> into it first:   <pre><code>cd your-repo-name/\n</code></pre></li> </ul>"},{"location":"python/mkdocs%20to%20github%20pages/#still-getting-the-error","title":"Still Getting the Error?","text":"<ul> <li>If you\u2019re in a subdirectory (e.g., <code>docs/</code>), move back to the root:   <pre><code>cd ..\n</code></pre></li> <li>Verify the <code>.git</code> folder exists:   <pre><code>ls -la .git\n</code></pre></li> <li>If missing, reinitialize Git (<code>git init</code>).</li> </ul>"},{"location":"python/mkdocs%20to%20github%20pages/#final-check","title":"Final Check","text":"<p>After fixing, run: <pre><code>git add .\ngit commit -am \"new changes\"\npython .\\gen_index.py\nmkdocs gh-deploy --clean --remote-name origin\nmkdocs build\ngit status  # Should show tracked files\nmkdocs serve\nmkdocs gh-deploy  # Should now deploy to GitHub Pages\n\n\n\n(.venv) PS D:\\githubRepos\\homelab&gt; mkdocs gh-deploy\nINFO    -  Cleaning site directory\nINFO    -  Building documentation to directory: D:\\githubRepos\\homelab\\site\nINFO    -  Documentation built in 1.33 seconds\nINFO    -  Copying 'D:\\githubRepos\\homelab\\site' to 'gh-pages' branch and pushing to GitHub.\nEnumerating objects: 1, done.\nCounting objects: 100% (1/1), done.\nWriting objects: 100% (1/1), 216 bytes | 216.00 KiB/s, done.\nTotal 1 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nTo https://github.com/gowseshaik/homelab.git\n   5ba2b58..23f810a  gh-pages -&gt; gh-pages\nINFO    -  Your documentation should shortly be available at: https://gowseshaik.github.io/homelab/\n</code></pre></p> <p>wait for 5 mins, it will take some time to upload and generate the site </p> <p>Wala!, Your site should deploy successfully! \ud83d\ude80  </p>"},{"location":"python/python%20pyautogui/","title":"Python pyautogui","text":"<pre><code>You're hitting the **PEP 668** protection \u2014 Ubuntu is blocking system-wide `pip install` to avoid breaking Python.\n</code></pre> <pre><code>python3 -m venv ~/pyauto-env\nsource ~/pyauto-env/bin/activate\npip install pyautogui\n\npython3 -m venv pyauto-env\nsource ./pyauto-env/bin/activate\npip install pyautogui\n</code></pre> <pre><code>import pyautogui\nimport time\n\n# Give time to switch to desired app\ntime.sleep(3)\n\n# Move to specific screen location and click (adjust x, y)\npyautogui.moveTo(500, 400)\npyautogui.click()\n\n# Wait a bit\ntime.sleep(1)\n\n# Close window (Alt+F4 for Windows/Linux)\npyautogui.hotkey('alt', 'f4')\n\n\npython your_script.py\n</code></pre> <p>To run your PyAutoGUI script as a Windows background package, follow these steps:</p>"},{"location":"python/python%20pyautogui/#1-freeze-the-script-as-a-standalone-exe","title":"\u2705 1. Freeze the script as a standalone <code>.exe</code>","text":"<p>Install <code>pyinstaller</code>:</p> <pre><code>pip install pyinstaller\n</code></pre> <p>Create the <code>.exe</code>:</p> <pre><code>pyinstaller --noconsole --onefile demo.py\n</code></pre> <ul> <li> <p><code>--noconsole</code>: hides terminal window (runs silently in background)</p> </li> <li> <p><code>--onefile</code>: packs everything into a single <code>.exe</code></p> </li> <li> <p>Output will be in <code>dist/demo.exe</code></p> </li> </ul>"},{"location":"python/python%20pyautogui/#2-auto-run-in-background-optional","title":"\u2705 2. Auto-run in background (optional)","text":"<p>To run at startup silently:</p> <ul> <li> <p>Copy <code>demo.exe</code> to:</p> <pre><code>C:\\Users\\&lt;YourUsername&gt;\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\n</code></pre> </li> </ul>"},{"location":"python/python%20pyautogui/#3-run-minimized-or-background-safe-avoid-popups","title":"\u2705 3. Run minimized or background-safe (avoid popups)","text":"<p>Update your <code>demo.py</code> to avoid mouse jumps while idle:</p> <pre><code>import pyautogui\nimport time\n\npyautogui.FAILSAFE = False  # prevent crash on screen corner\ntime.sleep(2)  # time to move mouse away if needed\n\npyautogui.moveTo(500, 500)\npyautogui.click()\npyautogui.hotkey('alt', 'f4')\n</code></pre> <p>Let me know if you want to:</p> <ul> <li> <p>Add a tray icon to stop it</p> </li> <li> <p>Auto-close specific app by name</p> </li> <li> <p>Make a config file for coordinates</p> </li> </ul>"}]}