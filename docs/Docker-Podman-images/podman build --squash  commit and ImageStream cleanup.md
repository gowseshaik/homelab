| Option                  | Recommendation                                         |
| ----------------------- | ------------------------------------------------------ |
| `podman build --squash` | ‚úÖ Native support                                       |
| `docker build --squash` | ‚ö†Ô∏è Supported, but only with experimental + no BuildKit |
| Multi-stage build       | ‚úÖ Best practice for both Docker and Podman             |

| Feature                       | `docker commit`                     | `podman commit`                   |
| ----------------------------- | ----------------------------------- | --------------------------------- |
| ‚úÖ Create image from container | ‚úÖ Yes                               | ‚úÖ Yes                             |
| üß± Squash layers              | ‚ùå No (not supported)                | ‚úÖ `--squash` supported            |
| üîê Rootless support           | ‚ùå No (needs root unless configured) | ‚úÖ Fully rootless                  |
| üßæ Add metadata               | ‚úÖ `--author`, `--message`           | ‚úÖ Same                            |
| üß™ OCI-compliant              | ‚úÖ (with extra steps)                | ‚úÖ Native                          |
| üß∞ Set image format           | ‚ùå                                   | ‚úÖ `--format=oci/docker`           |
| üöÄ Systemd-aware              | ‚ùå                                   | ‚úÖ Better integration with systemd |
| üîç Default history tracking   | ‚úÖ via container diff                | ‚úÖ via container diff              |
| üßº Clean image from temp      | ‚ùå manual                            | ‚úÖ better layered FS support       |

| Scenario               | Use `docker commit` | Use `podman commit` |
| ---------------------- | ------------------- | ------------------- |
| One-time image capture | ‚úÖ Yes               | ‚úÖ Yes               |
| Reproducible CI builds | ‚ùå No                | ‚ùå No                |
| Secure release image   | ‚ùå No                | ‚úÖ With `--squash`   |
| Rootless environments  | ‚ùå Limited           | ‚úÖ Best option       |

### üîß **Dockerfile** (very basic):

```Dockerfile
# file: Dockerfile
FROM alpine:latest
RUN apk add --no-cache curl
RUN echo "hello world" > /hello.txt
```
### üõ†Ô∏è **Build Command**:

```bash
podman build --squash -t gowse/alpine-curl:v1 .
```

### üß± **Explanation **:

|Part|Meaning|
|---|---|
|`podman build`|Create an image from a Dockerfile|
|`--squash`|Combines all image layers into **one layer** (final image is smaller & cleaner)|
|`-t gowse/alpine-curl:v1`|Tags the image with a name and version|
|`.`|Build context is current directory (should contain Dockerfile)|

### ‚úÖ Result:

- Final image has a **single layer** (no intermediate `RUN` layers).
- Smaller and cleaner image, better for production.

You're hitting a **real-world architectural pain point** that many large organizations face when dealing with **incremental container builds + integration-heavy runtimes** like **IBM App Connect Enterprise (ACE)**.

## üéØ Problem Recap:

- You're continuously building and deploying **IBM App Connect containers**.
- Each **Integration Server (IS)** image has many **.bar workflows**.
- Over time, you **incrementally build** on top of previous Image Streams (possibly using S2I or `COPY`).
- Eventually, you're hitting this:

```text
error: max depth exceeded / too many layers
```

## üß† Deep Technical Cause (Layer Limit)

Each build adds **a new image layer**, especially if:

- You do **not squash**
- You **rebuild on top** of prior image streams (`FROM imageStreamName`)
- You add `.bar` files via separate `COPY`/`RUN` instructions.

You hit the **OverlayFS (OCI) limit**:

- Maximum = **125 layers** per image on `overlay2` (used by Docker, Podman, OpenShift)
- Many base images (e.g., ACE + Ubuntu) already have 10‚Äì30 layers
- Every new `COPY`, `RUN`, `.bar` deployment ‚Üí new layer

‚ö†Ô∏è App Connect doesn‚Äôt do "one-bar-only" builds. So these accumulate fast.
## üîç Real Symptoms:

|Symptom|Root Cause|
|---|---|
|"Max layer/depth exceeded"|OCI spec violation due to too many `diff_ids` (layers)|
|App runs slow or crashes|Layer lookups degrade performance|
|Image push fails in CI/CD|Registry often rejects too deep manifest trees|
## ‚úÖ `--squash`: Benefits for Your Case

|Benefit|Why It Matters for App Connect|
|---|---|
|üí• Removes all intermediate `.bar` layer diffs|You don't retain N history of bar files in FS layers|
|‚¨áÔ∏è Greatly reduces image layer count|You can continuously rebuild without hitting max layer|
|üîê Hides sensitive integration flow stages (if any)|Good for regulatory reasons|
|üí® Smaller images = faster deploys across DEV/QA/PROD|Especially with remote registries|
|üßπ Prevents accumulation of dead bar versions|Since each squash is clean slate|
## ‚ùå Potential Downsides

|Risk|Mitigation|
|---|---|
|‚ùå Caching loss|Squashed images can't be layered on top of easily.|
|‚ùå Longer build times|Because everything builds from scratch|
|‚ùå Debuggability|Can‚Äôt `podman history` to inspect past layers|
|‚ùå Squash not supported in OpenShift S2I by default|You may need custom build scripts or `Containerfile` use|
## üîÑ Long-Term Strategy

### üîß Option 1: Use `--squash` for production deploys only

- **DEV:** Use normal layered builds for fast iteration
- **STAGE/PROD:** Use squashed images to avoid layer bloat
### üîß Option 2: Split Workflows by Domains

Instead of dumping 50 `.bar` files into one IS image:

- Break into **domain-specific Integration Servers**
    - `payments.is`, `orders.is`, `crm.is`
- Each one gets 5‚Äì10 bars max
- Now your `Dockerfile` looks like:

```Dockerfile
FROM ibm-appconnect:latest
COPY payment-flow1.bar /home/ace/ace-server/bars/
COPY payment-flow2.bar /home/ace/ace-server/bars/
```

- Less `.bar` layers = less layer growth over time

### üîß Option 3: Flatten image with custom script

Use a **build stage** that unpacks `.bar` files, copies them directly, then squashes.

```Dockerfile
FROM ibm-appconnect as builder
COPY *.bar /tmp/bars/
RUN for f in /tmp/bars/*.bar; do unpack-and-verify.sh "$f"; done

FROM scratch
COPY --from=builder /home/ace /home/ace
```
Then build with:
```bash
podman build --squash -t appconnect-integrations:v1 .
```

## üìä Bonus: Audit Current Image Streams

```bash
oc get istag -n your-namespace
for tag in $(oc get istag -n your-namespace -o name); do
  echo "$tag: $(oc get "$tag" -o jsonpath='{.image.metadata.name}' | wc -l) layers"
done
```
Helps track which image stream has grown unacceptably large.
## üéØ Final Recommendation

|Environment|Use `--squash`?|Notes|
|---|---|---|
|DEV|‚ùå|Keep fast caching for rebuilds|
|QA / UAT|‚úÖ|Use squash to test realistic image|
|PROD|‚úÖ|Always squash to avoid image growth over time|

#### periodically clean up OpenShift ImageStreams to avoid bloating and hitting the layer limit:

## ‚úÖ Goal:
- Clean up **old image tags** (ImageStreamTags)
- Prune **unused image layers** in registry
- Avoid **OCI layer depth limit**

## üßº 1. Clean Up Old Tags in ImageStream (AppConnect IS)

### üîÅ Script to Keep Only Last N Tags:

```bash
#!/bin/bash
# keep last 5 tags in image stream
N=5
IMAGESTREAM=my-is-name
NAMESPACE=my-project

TAGS=$(oc get istag -n $NAMESPACE --sort-by=.metadata.creationTimestamp -o jsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}" | grep "^$IMAGESTREAM:")

COUNT=$(echo "$TAGS" | wc -l)

if (( COUNT > N )); then
  DELETE_TAGS=$(echo "$TAGS" | head -n $((COUNT - N)))
  for tag in $DELETE_TAGS; do
    echo "Deleting tag: $tag"
    oc delete istag -n $NAMESPACE "$tag"
  done
fi
```

> üí° Run this via **cronjob or Jenkins**.

## üöÆ 2. Prune Unused Images from Internal Registry (Admin Only)

Run as cluster admin:
```bash
oc adm prune images \
  --keep-tag-revisions=3 \
  --keep-younger-than=72h \
  --confirm
```

|Option|Description|
|---|---|
|`--keep-tag-revisions=3`|Keep latest 3 image revisions per tag|
|`--keep-younger-than=72h`|Keep recent images (last 72 hours)|
|`--confirm`|Actually delete (omit for dry-run)|
> ‚ö†Ô∏è Only affects **internal OpenShift registry**.

## üîç 3. Monitor Image Layer Depth per ImageStream

```bash
oc get istag -n your-namespace -o json | jq '.items[] | {name: .metadata.name, layers: .image.dockerImageLayers | length}'
```
## üõ°Ô∏è 4. Add CI/CD Rule to Prevent Deploying Images with Too Many Layers

In your Jenkins/CI:
```bash
LAYER_COUNT=$(podman history your-image | wc -l)

if (( LAYER_COUNT > 100 )); then
  echo "ERROR: Image has too many layers ($LAYER_COUNT). Deployment blocked."
  exit 1
fi
```
## üóëÔ∏è 5. Optionally Remove ImageStream Manually

If certain ImageStreams are outdated:
```bash
oc delete is appconnect-is-old -n your-namespace
```

## üí° Tips

|Tip|Benefit|
|---|---|
|Use `--squash` in QA/PROD images|Prevents future bloat|
|Automate `oc delete istag` by age/tag count|Keeps env clean|
|Use separate ImageStreams for DEV and PROD|Avoids dirty reuse|
|Run `oc adm prune images` weekly|Frees storage, resets layer chains|
#### production-ready OpenShift CronJob YAML to periodically **clean up old ImageStreamTags**, keeping only the latest **5 tags** in a given ImageStream.

## ‚úÖ OpenShift CronJob: Cleanup Old Tags

### üìÅ `imagestream-cleanup-cronjob.yaml`
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: imagestream-cleanup
  namespace: your-project
spec:
  schedule: "0 2 * * *"  # Every day at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: registry.redhat.io/openshift4/ose-cli  # OpenShift CLI image
            command:
            - /bin/bash
            - -c
            - |
              N=5
              IMAGESTREAM="your-image-stream-name"
              NAMESPACE="your-project"

              TAGS=$(oc get istag -n $NAMESPACE --sort-by=.metadata.creationTimestamp \
                -o jsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}" | grep "^$IMAGESTREAM:")

              COUNT=$(echo "$TAGS" | wc -l)

              if (( COUNT > N )); then
                DELETE_TAGS=$(echo "$TAGS" | head -n $((COUNT - N)))
                for tag in $DELETE_TAGS; do
                  echo "Deleting $tag"
                  oc delete istag -n $NAMESPACE "$tag"
                done
              else
                echo "Nothing to delete. Total tags: $COUNT"
              fi
          restartPolicy: OnFailure
          serviceAccountName: imagestream-cleanup-sa
```

## üîê Service Account & Permissions

### üìÅ `imagestream-cleanup-role.yaml`
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: imagestream-cleanup-sa
  namespace: your-project
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: imagestream-cleanup-role
  namespace: your-project
rules:
- apiGroups: ["image.openshift.io"]
  resources: ["imagestreamtags"]
  verbs: ["get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: imagestream-cleanup-binding
  namespace: your-project
subjects:
- kind: ServiceAccount
  name: imagestream-cleanup-sa
roleRef:
  kind: Role
  name: imagestream-cleanup-role
  apiGroup: rbac.authorization.k8s.io
```

## üöÄ Deploy in OpenShift
```bash
oc apply -f imagestream-cleanup-role.yaml
oc apply -f imagestream-cleanup-cronjob.yaml
```

## üìå Notes

|Field|Description|
|---|---|
|`N=5`|Keeps only the latest 5 ImageStreamTags|
|`IMAGESTREAM`|Replace with your ACE Integration Server image stream name|
|`schedule`|Change to fit your cleanup window (e.g., `"0 */6 * * *"` for every 6h)|
|`ose-cli`|Lightweight Red Hat image with `oc` CLI for scripting|
